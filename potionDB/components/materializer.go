package components

import (
	"container/heap"
	fmt "fmt"
	"math/rand"
	"time"

	"potionDB/crdt/clocksi"
	"potionDB/crdt/crdt"
	"potionDB/crdt/proto"
	"potionDB/potionDB/utilities"
	"potionDB/shared/shared"

	hashFunc "github.com/twmb/murmur3"
)

//TODO: A good way to "softcheck" things would be to:
//Step1: do a TPC-H test
//Step2: check some time after the updates finished if every hold/prepare is empty
//Step3: check the max sizes of those (i.e., keep track with counters)
//The idea is that, with a test as big as TPC-H, it should cover a lot of situations.

//TODO: When updating TM/Replicator, search for "Pre-condition"
//TODO: Possible improvement: storing the pending updates associated to each object, instead of grouped per txn.
//Advantage is that the access is instant, disadvantage is a lot more maps
//TODO: Group reads of the same partition?

// /////*****************TYPE DEFINITIONS***********************/////
//////////************Requests**************************//////////

type MaterializerRequest struct {
	MatRequestArgs
}

type MatRequestArgs interface {
	getRequestType() (requestType MatRequestType)
	getChannel() (channelId uint64)
}

type MatStaticReadArgs struct {
	MatReadCommonArgs
}

type MatReadArgs struct {
	MatReadCommonArgs
	TransactionId
}

// Args for update request. Note that unlike with MatReadArgs, a MatUpdateArgs represents multiple updates, but all for the same partition
type MatUpdateArgs struct {
	Updates []crdt.UpdateObjectParams
	TransactionId
}

// These arguments are common to StaticReads and normal Reads
type MatReadCommonArgs struct {
	crdt.ReadObjectParams
	clocksi.Timestamp
	ReplyChan chan crdt.State
	//HashKey   *uint64 //Filled up automatically when getChannel() is called.
}

type MatStaticUpdateArgs struct {
	Updates []crdt.UpdateObjectParams
	TransactionId
	ReplyChan chan clocksi.Timestamp
}

type MatCommitArgs struct {
	TransactionId   TransactionId
	CommitTimestamp clocksi.Timestamp
}

type MatAbortArgs struct {
	TransactionId TransactionId
}

type MatPrepareArgs struct {
	TransactionId TransactionId
	ReplyChan     chan clocksi.Timestamp
}

// Used when multiple remote txns may be commited in a row.
type MatRemoteGroupTxnArgs struct {
	Txns      []MatRemoteTxn
	FinalClk  clocksi.Timestamp //minimum clock that is known to be safe after applying all remote txns
	ReplyChan chan []crdt.UpdateObjectParams
}

type MatRemoteTxnArgs struct {
	MatRemoteTxn
	ReplyChan chan []crdt.UpdateObjectParams
}

type MatRemoteTxn struct {
	ReplicaID int16
	clocksi.Timestamp
	Upds []crdt.UpdateObjectParams
}

type MatClkPosUpdArgs struct {
	ReplicaID int16
	StableTs  int64
	ReplyChan chan bool
}

// Used for preparing operations generated by applying downstream remotes
type MatPrepareForRemoteArgs struct {
	Updates []crdt.UpdateObjectParams
	TransactionId
	ReplyChan chan clocksi.Timestamp
}

// Used by the logging layer (to then pass to the replication layer) to know the most recent clock that is "safe", i.e., for which there will be no commit for sure.
type MatSafeClkArgs struct {
	//ReplyChan chan clocksi.Timestamp
}

// Used by the replicator's join to know the latest commited timestamp
type MatCommitedClkArgs struct {
	ReplyChan chan TimestampPartIdPair
}

type MatResetArgs struct {
	ReplyChan chan bool
}

// Used to get a snapshot of CRDTs, which is needed by the joining mechanism
type MatGetSnapshotArgs struct {
	clocksi.Timestamp
	Buckets   map[string]struct{}
	ReplyChan chan []*proto.ProtoCRDT
}

// Used to apply a snapshot of CRDTs send by existing replicas in the system
type MatApplySnapshotArgs struct {
	clocksi.Timestamp
	ProtoCRDTs []*proto.ProtoCRDT
}

// Processed separately. This is used to know when the partition can start and the initial clock be generated.
type MatWaitForReplicasArgs struct{}

type MatGCArgs struct {
	SafeClk clocksi.Timestamp //Can clean anything "before" (<) this clock.
}

// Request
type MatBCUpdPermissions struct {
	ReplyChan chan MatBCUpdPermissionsReply
}

type MatBCUpdPermissionsReply struct {
	ReqsMap map[int16]map[crdt.KeyParams]int32
	PartID  int16
}

// Applying txn for permissions updates
type MatBCTxnPermissions struct {
	Values    map[crdt.KeyParams]int32
	ReplicaID int16
}

// ///DEBUG/Testing methods. May be unsafe for concurrent usage
type MatGetCRDTArgs struct {
	crdt.KeyParams
	Pos       int
	ReplyChan chan CRDTPosPair
}

type CRDTPosPair struct {
	CRDT crdt.CRDT
	Pos  int
}

type MatRequestType byte

type Materializer struct {
	channels []chan MaterializerRequest
}

type partition struct {
	id int64 //partitionID
	partitionClock
	log              Logger            //logs the operations of txns that were commited in this partition
	commitChan       chan TMCommitInfo //channel used to inform the TM of a commit ending.
	db               map[uint64]VersionManager
	pendingOps       map[TransactionId][]crdt.UpdateObjectParams //prepared transactions waiting for a commit to be issued
	pendingOpsRemote map[TransactionId][]crdt.UpdateObjectParams //prepared NuCRDT transactions for other replicas (not to be applied locally)
	commitsOnHold    commitHeap                                  //commited transactions that couldn't be applied yet due to other prepared (not commited) clocks
	rng              rand.Source
	bcInfo           //Keeps track of all BCounters in this partition
	debugPart
	nCommits int //TODO: Remove, debug. Counts number of commits
	mat      *Materializer
}

type bcInfo struct {
	bcKeys          []crdt.KeyParams
	nKeys           int
	replicaIDs      []int16
	partID          int16
	opsForTxn       map[TransactionId]map[crdt.KeyParams]int32
	permsForReplica map[TransactionId]int16
}

// Manages the multiple clocks of the partition
type partitionClock struct {
	replicaID  int16
	lastUsedTs int64             //Last value generated by this partition for this replica with time.Now().UnixNano(). It helps ensure there are no two prepares with same clock.
	stableClk  clocksi.Timestamp //The latest clock for all the objects. Reads can be applied on this version or any prior.
	prepClks   prepClockHeap     //Sorted (smallest to highest) list of prepared clocks.
}

type TxnIdClkPair struct {
	clk   clocksi.Timestamp
	txnId TransactionId
}

type TimestampPartIdPair struct {
	clocksi.Timestamp
	partID int64
}

type prepClockHeap struct {
	entries  []TxnIdClkPair
	nEntries *int
	innerMap map[TransactionId]int //Helper map; for efficiency keeps a mapping of txnId -> pos in heap
}

// Pop() must give the lowest clock.
type commitHeap struct {
	entries  []MatCommitArgs
	nEntries *int
}

// Holds data used in debug mode
type debugPart struct {
	hashToParams map[uint64]crdt.KeyParams //Keeps a track of all hash -> KeyParams seen by this partition
}

/////*****************TYPE METHODS***********************/////

/*func (args MatStaticReadArgs) GetHashKey() uint64 {
	if *args.HashKey == 0 {
		*args.HashKey = hashFunc.StringSum64(args.Bucket + args.CrdtType.String() + args.Key)
	}
	return *args.HashKey
}*/

//Data structures
//PrepClockHeap

func (c prepClockHeap) Len() int { return *c.nEntries }

// We want the heap's Pop() to return the lowest element, so we use < on "less". The smallest element is on h[0].
func (c prepClockHeap) Less(i, j int) bool {
	return c.entries[j].clk.IsHigher(c.entries[i].clk)
}

func (c prepClockHeap) Swap(i, j int) {
	c.entries[i], c.entries[j] = c.entries[j], c.entries[i]
	c.innerMap[c.entries[i].txnId], c.innerMap[c.entries[j].txnId] = i, j
}

func (c *prepClockHeap) Push(value interface{}) {
	convValue := value.(TxnIdClkPair)
	c.innerMap[convValue.txnId] = *c.nEntries
	if *c.nEntries == cap(c.entries) {
		c.entries = append(c.entries, convValue)
		c.entries = c.entries[:cap(c.entries)] //Extending to capacity
		*c.nEntries += 1
	} else {
		c.entries[*c.nEntries], *c.nEntries = convValue, *c.nEntries+1
	}

}

func (c prepClockHeap) Pop() interface{} {
	if *c.nEntries == 0 {
		return nil
	}
	old := c.entries[*c.nEntries-1]
	c.entries[*c.nEntries-1] = TxnIdClkPair{}
	*c.nEntries--
	delete(c.innerMap, old.txnId)
	return old
}

// Returns the lowest value, but does not remove it.
func (c prepClockHeap) PeekMin() TxnIdClkPair {
	if *c.nEntries == 0 {
		return TxnIdClkPair{}
	}
	return c.entries[0]
}

// Returns the highest value, but does not remove it.
func (c prepClockHeap) PeekMax() TxnIdClkPair {
	if *c.nEntries == 0 {
		return TxnIdClkPair{}
	}
	return c.entries[*c.nEntries-1]
}

//CommitHeap

func (c commitHeap) Len() int { return *c.nEntries }

// We want the heap's Pop() to return the lowest element, so we use < on "less". The smallest element is on h[0].
func (c commitHeap) Less(i, j int) bool {
	return c.entries[j].CommitTimestamp.IsHigher(c.entries[i].CommitTimestamp)
}

func (c commitHeap) Swap(i, j int) {
	c.entries[i], c.entries[j] = c.entries[j], c.entries[i]
	//c.entries[i].pos, c.entries[j].pos = i, j
}

func (c *commitHeap) Push(value interface{}) {
	convValue := value.(MatCommitArgs)
	//convValue.pos = *c.nEntries
	if *c.nEntries == cap(c.entries) {
		c.entries = append(c.entries, convValue)
		c.entries = c.entries[:cap(c.entries)] //Extending to capacity
		*c.nEntries += 1
	} else {
		c.entries[*c.nEntries], *c.nEntries = convValue, *c.nEntries+1
	}

}

func (c commitHeap) Pop() interface{} {
	if *c.nEntries == 0 {
		return nil
	}
	old := c.entries[*c.nEntries-1]
	//Cleaning the clock for GC purposes
	c.entries[*c.nEntries-1].CommitTimestamp = nil
	*c.nEntries--
	return old
}

// Returns the lowest value, but does not remove it.
func (c commitHeap) PeekMin() MatCommitArgs {
	if *c.nEntries == 0 {
		return MatCommitArgs{}
	}
	return c.entries[0]
}

// Returns the highest value, but does not remove it.
func (c commitHeap) PeekMax() MatCommitArgs {
	if *c.nEntries == 0 {
		return MatCommitArgs{}
	}
	return c.entries[*c.nEntries-1]
}

//

func (args MatStaticReadArgs) getRequestType() (requestType MatRequestType) {
	return readStaticMatRequest
}

func (args MatStaticReadArgs) getChannel() (channelId uint64) {
	//return GetChannelKeyRead(args.MatReadCommonArgs)
	return GetChannelKey(args.KeyParams)
}

func (args MatStaticUpdateArgs) getRequestType() (requestType MatRequestType) {
	return writeStaticMatRequest
}
func (args MatStaticUpdateArgs) getChannel() (channelId uint64) {
	return GetChannelKey(args.Updates[0].KeyParams)
}

func (args MatReadArgs) getRequestType() (requestType MatRequestType) {
	return readMatRequest
}
func (args MatReadArgs) getChannel() (channelId uint64) {
	//return GetChannelKeyRead(args.MatReadCommonArgs)
	return GetChannelKey(args.KeyParams)
}

func (args MatUpdateArgs) getRequestType() (requestType MatRequestType) {
	return writeMatRequest
}
func (args MatUpdateArgs) getChannel() (channelId uint64) {
	return GetChannelKey(args.Updates[0].KeyParams)
}

// /*
// func (args MatVersionArgs) getRequestType() (requestType MatRequestType) {
// 	return versionMatRequest
// }
// func (args MatVersionArgs) getChannel() (channelId uint64) {
// 	return args.ChannelId
// }*/

func (args MatCommitArgs) getRequestType() (requestType MatRequestType) {
	return commitMatRequest
}

func (args MatCommitArgs) getChannel() (channelId uint64) {
	return 0 //When sending the commit the TM already knows the channel to send the request
}

func (args MatAbortArgs) getRequestType() (requestType MatRequestType) {
	return abortMatRequest
}

func (args MatAbortArgs) getChannel() (channelId uint64) {
	return 0 //When sending an abort the TM already knows the channel to send the request
}

func (args MatPrepareArgs) getRequestType() (requestType MatRequestType) {
	return prepareMatRequest
}

func (args MatPrepareArgs) getChannel() (channelId uint64) {
	return 0 //When sending a prepare the TM already knows the channel to send the request
}

func (args MatRemoteTxnArgs) getRequestType() (requestType MatRequestType) {
	return remoteTxnMatRequest
}

func (args MatRemoteTxnArgs) getChannel() (channelId uint64) {
	return 0 //When sending a remoteTxn request the TM sends directly to the correct materializer
}

func (args MatRemoteGroupTxnArgs) getRequestType() (requestType MatRequestType) {
	return remoteGroupTxnMatRequest
}

func (args MatRemoteGroupTxnArgs) getChannel() (channelId uint64) {
	return 0 //When sending a remoteTxn request the TM sends directly to the correct materializer
}

func (args MatClkPosUpdArgs) getRequestType() (requestType MatRequestType) {
	return clkPosUpdMatRequest
}

func (args MatClkPosUpdArgs) getChannel() (channelId uint64) {
	return 0 //matClkPosUpdRequest is always sent to all partitions
}

func (args MatSafeClkArgs) getRequestType() (requestType MatRequestType) {
	return safeClkMatRequest
}

func (args MatSafeClkArgs) getChannel() (channelId uint64) {
	return 0 //When sending a safeClk request the logger sends directly to the correct materializer
}

func (args MatPrepareForRemoteArgs) getRequestType() (requestType MatRequestType) {
	return prepareForRemoteRequest
}

func (args MatPrepareForRemoteArgs) getChannel() (channelId uint64) {
	return GetChannelKey(args.Updates[0].KeyParams)
}

func (args MatCommitedClkArgs) getRequestType() (requestType MatRequestType) {
	return commitedClkMatRequest
}

func (args MatCommitedClkArgs) getChannel() (channelId uint64) {
	return 0 //commitedClkMatRequest is always sent to all partitions
}

func (args MatGetSnapshotArgs) getRequestType() (requestType MatRequestType) {
	return getSnapshotMatRequest
}

func (args MatGetSnapshotArgs) getChannel() (channelId uint64) {
	return 0 //getSnapshotArgs is always sent to all partitions
}

func (args MatApplySnapshotArgs) getRequestType() (requestType MatRequestType) {
	return applySnapshotMatRequest
}

func (args MatApplySnapshotArgs) getChannel() (channelId uint64) {
	return 0 //ApplySnapshotArgs is sent directly to the right partition
}

func (args MatResetArgs) getRequestType() (requestType MatRequestType) {
	return resetMatRequest
}

func (args MatResetArgs) getChannel() (channelId uint64) {
	return 0 //MatReset is always sent to all partitions
}

func (args MatWaitForReplicasArgs) getRequestType() (requestType MatRequestType) {
	return waitForReplicasRequest
}

func (args MatWaitForReplicasArgs) getChannel() (channelId uint64) {
	return 0 //MatWaitForReplicas is always sent to all partitions
}

func (args MatGCArgs) getRequestType() (requestType MatRequestType) {
	return gcMatRequest
}

func (args MatGCArgs) getChannel() (channelId uint64) {
	return 0 //MatGCArgs is always sent to all partitions
}

func (args MatBCUpdPermissions) getRequestType() (requestType MatRequestType) {
	return bcUpdPermRequest
}

func (args MatBCUpdPermissions) getChannel() (channelId uint64) {
	return 0 //MatBCUpdPermissions is always sent to all partitions
}

func (args MatBCTxnPermissions) getRequestType() (requestType MatRequestType) {
	return bcTxnPermRequest
}

func (args MatBCTxnPermissions) getChannel() (channelId uint64) {
	return 0 //MatBCTxnPermissions is always sent to all partitions
}

func (args MatGetCRDTArgs) getRequestType() (requestType MatRequestType) {
	return getCrdtRequest
}

func (args MatGetCRDTArgs) getChannel() (channelId uint64) {
	return GetChannelKey(args.KeyParams)
}

// /////*****************CONSTANTS AND VARIABLES***********************/////

const (
	//Types of requests
	readStaticMatRequest     MatRequestType = 0
	writeStaticMatRequest    MatRequestType = 1
	readMatRequest           MatRequestType = 2
	writeMatRequest          MatRequestType = 3
	commitMatRequest         MatRequestType = 4
	abortMatRequest          MatRequestType = 5
	prepareMatRequest        MatRequestType = 6
	safeClkMatRequest        MatRequestType = 7
	remoteTxnMatRequest      MatRequestType = 8
	clkPosUpdMatRequest      MatRequestType = 9
	prepareForRemoteRequest  MatRequestType = 10
	commitedClkMatRequest    MatRequestType = 11
	getSnapshotMatRequest    MatRequestType = 12
	applySnapshotMatRequest  MatRequestType = 13
	resetMatRequest          MatRequestType = 14
	waitForReplicasRequest   MatRequestType = 15
	remoteGroupTxnMatRequest MatRequestType = 16
	gcMatRequest             MatRequestType = 17
	bcUpdPermRequest         MatRequestType = 18
	bcTxnPermRequest         MatRequestType = 19
	getCrdtRequest           MatRequestType = 20
	versionMatRequest        MatRequestType = 255

	initialPrepSize = 100 //Initial size for the prepClocks heap
)

var (
	nGoRoutines               uint64 //Number of partitions of the materializer. Each partition has one goroutine
	requestQueueSize          int    //Size for the channel that handles requests
	heapSize                  = 50   //Initial size for the prepare and commit buffer heaps
	expectedNewDownstreamSize int    //Initial size for new downstream upds generated by non-uniform CRDTs when applying remote ops
	keyRangeSize              uint64 //Number of keys that each partition is responsible, except for the last one which might have a bit more.

	debugMode = false //In debug mode, extra variables are kept track of and sanity checks will be done periodically.
)

func InitializeMaterializer(replicaID int16, commitCh chan TMCommitInfo) (mat *Materializer,
	loggers []Logger) {

	requestQueueSize = 1000
	mat = &Materializer{channels: make([]chan MaterializerRequest, nGoRoutines)}
	loggers = make([]Logger, nGoRoutines)
	var i uint64
	for i = 0; i < nGoRoutines; i++ {
		loggers[i] = &MemLogger{}
		loggers[i].Initialize(mat, i)
		//Channel to receive requests from TM. Has a buffer since requests may be received from multiple sources.
		mat.channels[i] = make(chan MaterializerRequest, requestQueueSize)
		go listenToTM(int64(i), loggers[i], replicaID, commitCh, mat)
	}

	/*lineKey1 := crdt.KeyParams{Key: "lineitem", Bucket: "R1", CrdtType: proto.CRDTType_RRMAP}
	orderKey1 := crdt.KeyParams{Key: "orders", Bucket: "R1", CrdtType: proto.CRDTType_RRMAP}
	lineKey2 := crdt.KeyParams{Key: "lineitem", Bucket: "R2", CrdtType: proto.CRDTType_RRMAP}
	orderKey2 := crdt.KeyParams{Key: "orders", Bucket: "R2", CrdtType: proto.CRDTType_RRMAP}
	lineKey3 := crdt.KeyParams{Key: "lineitem", Bucket: "R3", CrdtType: proto.CRDTType_RRMAP}
	orderKey3 := crdt.KeyParams{Key: "orders", Bucket: "R3", CrdtType: proto.CRDTType_RRMAP}
	lineKey4 := crdt.KeyParams{Key: "lineitem", Bucket: "R4", CrdtType: proto.CRDTType_RRMAP}
	orderKey4 := crdt.KeyParams{Key: "orders", Bucket: "R4", CrdtType: proto.CRDTType_RRMAP}
	lineKey5 := crdt.KeyParams{Key: "lineitem", Bucket: "R5", CrdtType: proto.CRDTType_RRMAP}
	orderKey5 := crdt.KeyParams{Key: "orders", Bucket: "R5", CrdtType: proto.CRDTType_RRMAP}
	//GetChannelKey
	fmt.Printf("Partition for R1 lineitems and orders: %d, %d.\n", GetChannelKey(lineKey1), GetChannelKey(orderKey1))
	fmt.Printf("Partition for R2 lineitems and orders: %d, %d.\n", GetChannelKey(lineKey2), GetChannelKey(orderKey2))
	fmt.Printf("Partition for R3 lineitems and orders: %d, %d.\n", GetChannelKey(lineKey3), GetChannelKey(orderKey3))
	fmt.Printf("Partition for R4 lineitems and orders: %d, %d.\n", GetChannelKey(lineKey4), GetChannelKey(orderKey4))
	fmt.Printf("Partition for R5 lineitems and orders: %d, %d.\n", GetChannelKey(lineKey5), GetChannelKey(orderKey5))*/

	return
}

func listenToTM(id int64, logger Logger, replicaID int16, commitCh chan TMCommitInfo, mat *Materializer) {
	//Each goroutine is responsible for the range of keys [keyRangeSize * id, keyRangeSize * (id + 1)[
	//Where keyRangeSize = math.MaxUint64 / number of goroutines

	part := &partition{
		id:               id,
		log:              logger,
		commitChan:       commitCh,
		db:               make(map[uint64]VersionManager),
		pendingOps:       make(map[TransactionId][]crdt.UpdateObjectParams),
		pendingOpsRemote: make(map[TransactionId][]crdt.UpdateObjectParams),
		commitsOnHold:    commitHeap{entries: make([]MatCommitArgs, heapSize), nEntries: new(int)},
		rng:              rand.NewSource(time.Now().Unix() + id),
		mat:              mat,
	}
	channel := mat.channels[id]

	waitForReplicas(channel)
	part.initializePartitionClock(replicaID)
	part.initializeBCInfo(int16(id))

	//if debugMode {
	part.hashToParams = make(map[uint64]crdt.KeyParams)
	//go part.sanityCheck()
	//}

	for {
		part.handleRequest(<-channel)
	}
	/*var request MaterializerRequest
	//wasFull, wasEmpty := false, false
	nFullProc := 0
	var startFull time.Time
	region := dp.Region
	//TODO: Maybe just implement a "heartbeat": when timeout, print saying "I am alive" and how many requests processed since last timeout.
	for {
		if part.partID == getCriticalPartId(region) && (startFull == time.Time{}) && len(channel) == cap(channel) {
			startFull = time.Now()
			fmt.Printf("[MAT%d]Channel is full, starting at %v.\n", part.partID, startFull.Format("2006-01-02 15:04:05.000"))
		}
		if part.partID == getCriticalPartId(region) && len(channel) == cap(channel) {
			nFullProc++
		}
		timer := time.NewTimer(3 * time.Second)
		select {
		case request = <-channel:
			part.handleRequest(request)
			if (startFull != time.Time{}) && part.partID == getCriticalPartId(region) && (len(channel)+10) < cap(channel) {
				fmt.Printf("[MAT%d]Channel was full but is not anymore. Processed %d reqs. Curr time: %v.\n", part.partID, nFullProc, time.Now().Format("2006-01-02 15:04:05.000"))
				startFull, nFullProc = time.Time{}, 0
			}
		case <-timer.C:
			fmt.Printf("[MAT%d]No request received in the last 3 seconds. Current time: %v\n", part.partID, time.Now().Format("2006-01-02 15:04:05.000"))
		}
		//request = <-channel

	}*/
	/*for {
		if part.partID == 2 && len(channel) == cap(channel) {
			wasFull = true
		}
		request = <-channel
		if part.partID == 2 && wasEmpty {
			fmt.Printf("[MAT%d]Channel was empty but we just received request %d. Starting to process at %v.\n", part.partID, request.getRequestType(), time.Now().Format("2006-01-02 15:04:05.000"))
		}
		wasEmpty = false
		if part.partID == 2 && wasFull {
			fmt.Printf("[MAT%d]Channel full, starting to process request %d at %v.\n", part.partID, request.getRequestType(), time.Now().Format("2006-01-02 15:04:05.000"))
		}
		part.handleRequest(request)
		if wasFull {
			if len(channel) == cap(channel) {
				fmt.Printf("[MAT%d]Channel was (and IS) full, finished processing request %d at %v.\n", part.partID, request.getRequestType(), time.Now().Format("2006-01-02 15:04:05.000"))
			} else {
				fmt.Printf("[MAT%d]Channel was full (but is not anymore), finished processing request %d at %v.\n", part.partID, request.getRequestType(), time.Now().Format("2006-01-02 15:04:05.000"))
			}
		}
		wasFull = false
		if part.partID == 2 && len(channel) == 0 {
			wasEmpty = true
			fmt.Printf("[MAT%d]Channel is empty after processing the last request (%d) at %v!\n", part.partID, request.getRequestType(), time.Now().Format("2006-01-02 15:04:05.000"))
		}
	}*/
}

func getCriticalPartId(region int8) int16 {
	// TODO: Debug method. Returns the partition that gets most overwhelmed with updates (lineitem partition)
	//R1: 2. R2: 22. R3: R4: 1?. R5: ?

	/*
		Partition for R1 lineitems and orders: 2, 16.
		Partition for R2 lineitems and orders: 28, 29.
		Partition for R3 lineitems and orders: 21, 1.
		Partition for R4 lineitems and orders: 1, 23.
		Partition for R5 lineitems and orders: 20, 15.
	*/
	//Note: For R2, it's usually partition 22 that overloads. Odd.
	if region == 0 {
		return 2
	}
	if region == 1 {
		return 22
	}
	if region == 2 {
		return 21
	}
	if region == 3 {
		return 1
	}
	if region == 4 {
		return 20
	}
	return 0
}

// Makes the partition wait for the rest of the system to start before processing requests.
func waitForReplicas(channel chan MaterializerRequest) {
	for {
		msg := <-channel
		if msg.getRequestType() == waitForReplicasRequest {
			break
		}
	}
}

func (part *partition) initializePartitionClock(replicaID int16) {
	part.partitionClock = partitionClock{
		replicaID:  replicaID,
		lastUsedTs: 0,
		stableClk:  clocksi.NewClockSiTimestamp(),
		prepClks:   prepClockHeap{entries: make([]TxnIdClkPair, heapSize), nEntries: new(int), innerMap: make(map[TransactionId]int)},
	}
}

func (part *partition) initializeBCInfo(partID int16) {
	part.bcInfo = bcInfo{
		bcKeys:          make([]crdt.KeyParams, 10),
		nKeys:           0,
		replicaIDs:      clocksi.GetCopyKeys(),
		partID:          int16(part.id),
		opsForTxn:       make(map[TransactionId]map[crdt.KeyParams]int32),
		permsForReplica: make(map[TransactionId]int16),
	}
}

func (part *partition) handleRequest(request MaterializerRequest) {
	switch request.getRequestType() {
	case readStaticMatRequest:
		part.handleMatStaticRead(request.MatRequestArgs.(MatStaticReadArgs))
	case readMatRequest:
		part.handleMatRead(request.MatRequestArgs.(MatReadArgs))
	case writeStaticMatRequest:
		part.handleMatStaticWrite(request.MatRequestArgs.(MatStaticUpdateArgs))
	case writeMatRequest:
		part.handleMatWrite(request.MatRequestArgs.(MatUpdateArgs))
	case commitMatRequest:
		part.handleMatCommit(request.MatRequestArgs.(MatCommitArgs))
	case prepareMatRequest:
		part.handleMatPrepare(request.MatRequestArgs.(MatPrepareArgs))
	case abortMatRequest:
		part.handleMatAbort(request.MatRequestArgs.(MatAbortArgs))
	case safeClkMatRequest:
		part.handleMatSafeClk(request.MatRequestArgs.(MatSafeClkArgs))
	case remoteTxnMatRequest:
		part.handleMatRemoteTxn(request.MatRequestArgs.(MatRemoteTxnArgs))
	case clkPosUpdMatRequest:
		part.handleMatClkPosUpd(request.MatRequestArgs.(MatClkPosUpdArgs))
	case remoteGroupTxnMatRequest:
		part.handleMatRemoteGroupTxn(request.MatRequestArgs.(MatRemoteGroupTxnArgs))
	case prepareForRemoteRequest:
		part.handleMatPrepareRemote(request.MatRequestArgs.(MatPrepareForRemoteArgs))
		//For Replicator's join
	case commitedClkMatRequest:
		part.handleMatCommitedClk(request.MatRequestArgs.(MatCommitedClkArgs))
	case bcUpdPermRequest:
		part.handleMatBCUpdPerm(request.MatRequestArgs.(MatBCUpdPermissions))
	case bcTxnPermRequest:
		part.handleMatPermsTxn(request.MatRequestArgs.(MatBCTxnPermissions))
	case resetMatRequest:
		part.handleMatReset(request.MatRequestArgs.(MatResetArgs))
		//TODO: snapshots, version, etc.
	case gcMatRequest:
		part.handleMatGC(request.MatRequestArgs.(MatGCArgs))
	case getCrdtRequest:
		part.handleMatGetCRDT(request.MatRequestArgs.(MatGetCRDTArgs))
	default:
		fmt.Println("[WARNING][MAT]Unknown request type: ", request.getRequestType())
	}
}

func (part *partition) handleMatRead(args MatReadArgs) {
	readLatest := part.canReadLatest(args.Timestamp)
	obj := part.getObject(args.KeyParams, args.Timestamp)
	//obj := part.getObject(args.KeyParams, *args.HashKey, args.Timestamp)
	objPending := part.getObjPendingOps(args.KeyParams, args.TransactionId)
	args.ReplyChan <- part.getState(readLatest, obj, args.ReadArgs, args.Timestamp, objPending)
}

func (part *partition) handleMatStaticRead(args MatStaticReadArgs) {
	//Static read does not need to check for pending ops
	readLatest := part.canReadLatest(args.Timestamp)
	/*hashKey := getHash(getCombinedKey(args.KeyParams))
	_, hasKey := part.db[hashKey]
	if !hasKey {
		fmt.Printf("[MAT][StaticRead]Does not have Key: %v, Args: %+v (%T)\n", args.KeyParams, args.ReadArgs, args.ReadArgs)
	}*/
	//fmt.Println("[MAT][StaticRead]HashKey: ", *args.HashKey)
	//obj := part.getObject(args.KeyParams, *args.HashKey, args.Timestamp)
	obj := part.getObject(args.KeyParams, args.Timestamp)
	state := part.getState(readLatest, obj, args.ReadArgs, args.Timestamp, nil)
	/*if hasKey {
		//fmt.Printf("[MAT][StaticRead]Key: %v, Args: %+v (%T)\nCRDT: %v\nState: %+v\n", args.KeyParams, args.ReadArgs, args.ReadArgs, obj.GetLatestCRDT(), state)
		fmt.Printf("[MAT][StaticRead]Key: %v, Args: %+v (%T)\n", args.KeyParams, args.ReadArgs, args.ReadArgs)
	}*/
	args.ReplyChan <- state
	//args.ReplyChan <- part.getState(readLatest, obj, args.ReadArgs, args.Timestamp, nil)
}

// Pre-condition: Assumes a read is not for the future - either present or past.
// This implies the TM must ensure a read is not sent to materializer before all partition's clock has caught up
// This should be easy to guarantee by only updating the TM clock after all partitions have confirmed the clock.
// A possible problem may be when applying remote transactions - this may be slow when many remote transactions are received.
func (part *partition) canReadLatest(readTs clocksi.Timestamp) bool {
	if shared.IsReadWaitingDisabled {
		return true
	}
	//If it is lower, a read in the past is issued (i.e., not readLatest)
	return !readTs.IsLower(part.stableClk)
}

func (part *partition) getState(readLatest bool, obj VersionManager, readArgs crdt.ReadArguments,
	clk clocksi.Timestamp, objPending []crdt.UpdateArguments) crdt.State {
	if readLatest {
		//fmt.Println("[MAT]Reading latest version", readTs)
		return obj.ReadLatest(readArgs, objPending)
	} else {
		//fmt.Println("[MAT]Reading old version", readTs)
		return obj.ReadOld(readArgs, clk, objPending)
	}
}

func (part *partition) handleMatPrepare(args MatPrepareArgs) {
	part.prepareClock(args.TransactionId)
	args.ReplyChan <- part.prepClks.PeekMax().clk
}

func (part *partition) handleMatWrite(args MatUpdateArgs) {
	part.pendingOps[args.TransactionId] = append(part.pendingOps[args.TransactionId], args.Updates...)
	part.debugLogUpdParams(args.Updates)
}

func (part *partition) handleMatStaticWrite(args MatStaticUpdateArgs) {
	/*if part.partID == 2 {
		fmt.Printf("[MAT%d][StaticWrite]Starting static write for txn %d.\n", part.id, args.TransactionId)
		part.prepareClock(args.TransactionId)
		fmt.Printf("[MAT%d][StaticWrite]Finished prepareClock. Getting prepClk (part.prepClks.PeekMax().clk)\n", part.id)
		prepClk := part.prepClks.PeekMax().clk
		fmt.Printf("[MAT%d][StaticWrite]Finished prepClk. Adding ops to pendingOps.\n", part.id)
		part.pendingOps[args.TransactionId] = args.Updates
		fmt.Printf("[MAT%d][StaticWrite]Finished adding ops to pendingOps. Setting up timer.\n", part.id)
		timer := time.NewTimer(8 * time.Second)
		select {
		case args.ReplyChan <- prepClk:
			timer.Stop()
		case <-timer.C:
			fmt.Printf("[MAT%d][StaticWrite]Timeout sending reply for txn %d. Timeout of 8s, happened at %s. Size of queue, prepClk, holdCommit, pendingOps: %d, %d, %d, %d. \n",
				part.id, args.TransactionId, time.Now().Format("2006-01-02 15:04:05.000"), len(part.mat.channels[part.partID]), part.prepClks.Len(), part.commitsOnHold.Len(), len(part.pendingOps))
		}

		fmt.Printf("[MAT%d][StaticWrite]Finished sending reply for txn %d. Size of queue, prepClk, holdCommit, pendingOps: %d, %d, %d, %d. \n",
			part.id, args.TransactionId, len(part.mat.channels[part.partID]), part.prepClks.Len(), part.commitsOnHold.Len(), len(part.pendingOps))

	} else {*/
	part.prepareClock(args.TransactionId)
	part.pendingOps[args.TransactionId] = args.Updates
	args.ReplyChan <- part.prepClks.PeekMax().clk
	//}
	part.debugLogUpdParams(args.Updates)
	/*if part.id == 2 {
		fmt.Printf("[MAT%d][StaticWrite]Finished debug log for txn %d.\n", part.id, args.TransactionId)
	}*/
}

func (part *partition) handleMatPermsTxn(args MatBCTxnPermissions) {
	txnID := TransactionId(part.rng.Int63())
	part.prepareClock(txnID)
	part.bcInfo.permsForReplica[txnID] = args.ReplicaID
	part.bcInfo.opsForTxn[txnID] = args.Values
	clk := part.prepClks.PeekMax().clk
	part.commitChan <- TMCommitNPartitions{nPartitions: 1, txnId: txnID, clk: clk.Copy()}
	part.handleMatCommit(MatCommitArgs{TransactionId: txnID, CommitTimestamp: clk})
}

func (part *partition) handleMatAbort(args MatAbortArgs) {
	delete(part.pendingOps, args.TransactionId)
}

func (part *partition) handleMatCommit(args MatCommitArgs) {
	if args.CommitTimestamp == nil {
		fmt.Println("[MAT][ERROR]Received nil timestamp on commit!", args)
	}
	/*if part.partID == 2 {
		fmt.Printf("[MAT%d][Commit]Starting handle commit for txn %d at %s.\n", part.id, args.TransactionId, time.Now().Format("2006-01-02 15:04:05.000"))
		lowestPrep := part.getLowestPrepare()
		fmt.Printf("[MAT%d][Commit]Finished getting lowest prepare for txn %d at %s. Lowest prepare: %v.\n", part.id, args.TransactionId, time.Now().Format("2006-01-02 15:04:05.000"), lowestPrep)
		part.removePrepare(args.TransactionId) //We always remove this prepare, even if the commit is put on hold.
		fmt.Printf("[MAT%d][Commit]Finished removing prepare for txn %d at %s.\n", part.id, args.TransactionId, time.Now().Format("2006-01-02 15:04:05.000"))
		part.applyCommit(args)
		fmt.Printf("[MAT%d][Commit]Finished applying commit for txn %d at %s.\n", part.id, args.TransactionId, time.Now().Format("2006-01-02 15:04:05.000"))
		ignore(lowestPrep) //TODO: UNDO!
	} else {
	lowestPrep := part.getLowestPrepare()
	part.removePrepare(args.TransactionId) //We always remove this prepare, even if the commit is put on hold.
	part.applyCommit(args)
	ignore(lowestPrep) //TODO: UNDO!
	}*/
	lowestPrep := part.getLowestPrepare()
	part.removePrepare(args.TransactionId) //We always remove this prepare, even if the commit is put on hold.
	if part.canCommit(args) {
		//Safe to commit
		//fmt.Println("[MAT]Apply commit")
		part.applyCommit(args)
	} else {
		//fmt.Println("[MAT]Hold commit", args)
		part.holdCommit(args)
	}
	if lowestPrep.txnId == args.TransactionId {
		//Was the lowest prepare, so some commit on hold may be appliable now - even if this transaction's commit was put on hold.
		part.checkHoldCommits()
	}
}

func (part *partition) applyCommit(args MatCommitArgs) {
	//startTs := time.Now().UnixNano()
	//fmt.Println("[MAT]Commit")
	ops, has := part.pendingOps[args.TransactionId]
	var downstreamUpds []crdt.UpdateObjectParams
	if has { //Normal commit
		//fmt.Println("[MAT]Commiting")
		downstreamUpds = part.applyUpdates(ops, args.CommitTimestamp)
	} else if remoteOps, has := part.pendingOpsRemote[args.TransactionId]; has { //Commit only for remote (NuCRDTs)
		//fmt.Println("[MAT]Commiting for remote", args)
		downstreamUpds = remoteOps
	} else {
		downstreamUpds = part.applyBCPermsUpds(args.TransactionId, args.CommitTimestamp)
	}
	part.updatePartitionWithCommit(args, downstreamUpds)
	part.commitChan <- TMPartCommitReply{txnId: args.TransactionId}
	/*timer := time.NewTimer(2 * time.Second)
	select {
	case part.commitChan <- TMPartCommitReply{txnId: args.TransactionId}:

	case <-timer.C:
		fmt.Printf("[MAT%d][Commit]Timeout informing commitchan of commit end for txn %d\n", part.id, args.TransactionId)
	}*/
	/*end := time.Now()
	diff := (end.UnixNano() - startTs) / int64(time.Millisecond)
	fmt.Printf("[MAT%d][Commit]Finished updating at %s. Time taken: %d ms. First key: %s, %s. NUpds: %d.\n",
		part.id, end.Format("2006-01-02 15:04:05.000"), diff, ops[0].Key, ops[0].Bucket, len(ops))*/
}

func (part *partition) holdCommit(args MatCommitArgs) {
	heap.Push(&part.commitsOnHold, args)
	//fmt.Println("[MAT]Heap min after pushing:", part.commitsOnHold.PeekMin(), ". Len:", part.commitsOnHold.Len())
}

// Note: this function also removes args.TxnId from the list of prepares
func (part *partition) canCommit(args MatCommitArgs) (canCommit bool) {
	if part.prepClks.Len() == 0 {
		if part.commitsOnHold.Len() == 0 {
			return true //No prep or commit on hold, can commit for sure.
		}
		//No prep. Need to still check the lowest commit however.
		return part.commitsOnHold.PeekMin().CommitTimestamp.IsHigher(args.CommitTimestamp) //if minCommitHold > args.CommitTimestamp -> safe to commit. Else, can't.
	}
	//If the lowest prepare is lower than args.CommitTimestamp, we can't commit for sure.
	if part.prepClks.PeekMin().clk.IsLower(args.CommitTimestamp) {
		return false
	}
	//The lowest prepare is higher than args.CommitTimestamp. If the lowest commit on hold is also higher (or no commit on hold), then we can commit. Otherwise, can't.
	return part.commitsOnHold.Len() == 0 || part.commitsOnHold.PeekMin().CommitTimestamp.IsHigher(args.CommitTimestamp)
}

// downstreamUpds: key, CRDTType, bucket + downstream arguments of each update. This is what is sent to other replicas for them to apply the txn.
func (part *partition) applyUpdates(updates []crdt.UpdateObjectParams, commitClk clocksi.Timestamp) (downstreamUpds []crdt.UpdateObjectParams) {
	downstreamUpds = make([]crdt.UpdateObjectParams, len(updates))
	i := 0
	var hashKey uint64
	var obj VersionManager

	/*startTime := time.Now()
	startTimeMs := startTime.UnixNano() / 1000000
	fmt.Printf("[MAT][Part%d]Start time for keys %+v: %v\n", part.partID, updates[0].KeyParams, startTime.Format("2006-01-02 15:04:05.000"))*/

	for _, upd := range updates {
		hashKey = getHash(getCombinedKey(upd.KeyParams))
		//fmt.Println("[MAT]ApplyUpdates", upd.KeyParams, upd.UpdateArgs)
		//fmt.Printf("[MAT]ApplyUpdates for key: %v. Are updates nil: %v. Updates type: %T\n", upd.KeyParams, upd.UpdateArgs == nil, upd.UpdateArgs)

		//If it's a reset, delete the CRDT, generate the downstream and continue to the next upd.
		//Note that reads may recreate the CRDT, but with an empty state.
		if (upd.UpdateArgs == crdt.ResetOp{}) {
			downstreamUpds[i] = upd
			i++
			delete(part.db, hashKey)
			//TODO: Remove BCounter
			//fmt.Println("Reset Op, continuing.")
			continue
		}

		obj = part.getObjectForUpd(hashKey, upd.KeyParams, commitClk)
		/*if strings.HasPrefix(upd.KeyParams.Key, "q5nr") {
			fmt.Printf("[MAT%d]ApplyUpdates for Q5. Key: %v. Obj type: %T. Upd: %v\n", part.id, upd.KeyParams, obj.GetLatestCRDT(), upd.UpdateArgs)
		} else if strings.HasPrefix(upd.KeyParams.Key, "q14pp") {
			fmt.Printf("[MAT%d]ApplyUpdates for Q14. Key: %v. Obj type: %T. Upd: %v\n", part.id, upd.KeyParams, obj.GetLatestCRDT(), upd.UpdateArgs)
		} else if strings.HasPrefix(upd.KeyParams.Key, "q") {
			fmt.Printf("[MAT%d]ApplyUpdates with key starting with q. Key: %v. Obj type: %T. Upd: %v\n", part.id, upd.KeyParams, obj.GetLatestCRDT(), upd.UpdateArgs)
		*/
		/*else {
			fmt.Printf("[MAT]ApplyUpdates for key: %v. Obj type: %T.\n", upd.KeyParams, obj.GetLatestCRDT())
		}*/
		//fmt.Printf("[MAT]ApplyUpdates. Obj type: %T\n", obj.GetLatestCRDT())
		//Due to non-uniform CRDTs, the downstream args might be noop (op with no effect/doesn't need to be propagated yet)
		//Some "normal" CRDTs have also be optimized to do the same (e.g., setAWCrdt when removing element that doesn't exist)
		downArgs := obj.Update(upd.UpdateArgs)
		if (downArgs != nil && downArgs != crdt.NoOp{}) {
			//fmt.Printf("[MAT]Applying downstream update to %v with args %T %+v\n", upd.KeyParams, *upd.UpdateArgs, *upd.UpdateArgs)
			obj.Downstream(commitClk, downArgs)
			if downArgs.MustReplicate() {
				downstreamUpds[i] = crdt.UpdateObjectParams{KeyParams: upd.KeyParams, UpdateArgs: downArgs.(crdt.UpdateArguments)}
				i++
			} /*else {
				fmt.Printf("[MAT]ApplyUpdates. Not replicating! Key: %v. Are updates nil: %v. Updates type: %T\n", upd.KeyParams, upd.UpdateArgs == nil, upd.UpdateArgs)
			}*/
		} /*else {
			fmt.Printf("[MAT]ApplyUpdates. DownArgs is nil or NoOp! Key: %v. Are updates nil: %v. Updates: %v. DownUpds: %v. Updates type: %T Down updates type: %T\n",
				upd.KeyParams, upd.UpdateArgs != nil, upd.UpdateArgs, downArgs, upd.UpdateArgs, downArgs)
		}*/
	}

	/*endTime := time.Now()
	fmt.Printf("[MAT][Part%d]Time taken to apply updates: %dms\n. EndTime: %s. Number of updates: %d. Key: %+v\n",
		part.partID, endTime.UnixNano()/1000000-startTimeMs, endTime.Format("2006-01-02 15:04:05.000"), len(updates), updates[0].KeyParams)*/
	//Hides positions that are empty due to non-uniform CRDTs
	/*part.nCommits++
	if part.nCommits%1000 == 0 {
		fmt.Printf("[MAT%d]Finished applying commit number %d at time %s.\n", part.id, part.nCommits, time.Now().Format("2006-01-02 15:04:05.000"))
	}*/
	return downstreamUpds[:i]
}

func (part *partition) updatePartitionWithCommit(args MatCommitArgs, downUpds []crdt.UpdateObjectParams) {
	part.updateClockWithCommit(args.CommitTimestamp)

	/*downUpdsS := "["
	for _, upd := range part.pendingOps[args.TransactionId] {
		downUpdsS += fmt.Sprintf("(%s,%s,%s,%T):%+v; ", upd.KeyParams.Key, upd.KeyParams.Bucket,
			proto.CRDTType_name[int32(upd.KeyParams.CrdtType)], *upd.UpdateArgs, *upd.UpdateArgs)
	}
	downUpdsS += "]"*/
	//No need to log if there's no downstream effects (can happen due to NuCRDTs or, e.g., set remove without element existing)
	if len(downUpds) > 0 {
		/*if part.partID == 2 {
			fmt.Printf("[MAT%d]Sending logger request.\n", part.id)
		}*/
		part.log.SendLoggerRequest(LoggerRequest{
			LogRequestArgs: LogCommitArgs{
				TxnClk: args.CommitTimestamp,
				Upds:   downUpds,
			},
		})
		/*if part.partID == 2 {
			fmt.Printf("[MAT%d]Finished sending logger request.\n", part.id)
		}*/
	} else {
		//fmt.Printf("[MAT]Not sending logger request: no downUpds!!! (original upds: %s)\n", downUpdsS)
	}
	delete(part.pendingOps, args.TransactionId)
	delete(part.pendingOpsRemote, args.TransactionId)
}

// Cyclic. It will keep commiting operations until none are left.
func (part *partition) checkHoldCommits() {
	if part.commitsOnHold.Len() == 0 {
		//fmt.Println("[MAT][CHC]Commit hold empty, returning")
		return
	}
	//fmt.Printf("[MAT%d][CHC]Len of commitHold: %d\n", part.id, part.commitsOnHold.Len())
	if part.prepClks.Len() == 0 {
		//fmt.Println("[MAT][CHC]Prep clocks empty, applying all commits")
		//A commit got applied, no prepare left. Can safely apply all hold commits.
		for part.commitsOnHold.Len() > 0 {
			part.applyCommit(heap.Pop(&part.commitsOnHold).(MatCommitArgs))
		}
		return
	}
	//fmt.Println("[MAT]Checking commits, neither are empty")
	minCommit := part.commitsOnHold.PeekMin()
	minPrep := part.prepClks.PeekMin().clk
	//fmt.Printf("[MAT][CHC]Hold size: %d. MinCommitTs: %s. MinPrepTs: %s\n",
	//part.commitsOnHold.Len(), minCommit.CommitTimestamp.ToSortedString(), minPrep.ToSortedString())
	if part.commitsOnHold.Len() > 0 && minCommit.CommitTimestamp == nil {
		fmt.Println("[MAT][ERROR](before the for) Nil clock for commit on hold!!! Commit:", minCommit, "Len: ", part.commitsOnHold.Len())
		popArgs := heap.Pop(&part.commitsOnHold).(MatCommitArgs)
		fmt.Println("[MAT][ERROR]Args popped:", popArgs)
	}
	if part.commitsOnHold.Len() == 0 {
		fmt.Println("[MAT][ERROR]Empty commitsonHold when it's supposed to still have entries!!!")
	}
	//for minCommit.CommitTimestamp.IsLower(minPrep) {
	for minCommit.CommitTimestamp.IsLowerOrEqual(minPrep) {
		//fmt.Println("[MAT][CHC]Applying commits")
		//Can commit. Can keep commiting as long as < minPrep
		heap.Pop(&part.commitsOnHold)
		part.applyCommit(minCommit)
		if part.commitsOnHold.Len() == 0 {
			//All commits applied, stop
			break
		}
		minCommit = part.commitsOnHold.PeekMin()
		if part.commitsOnHold.Len() > 0 && minCommit.CommitTimestamp == nil {
			fmt.Println("[MAT][ERROR]Nil clock for commit on hold!!! Commit:", minCommit)
		}
	}
	//fmt.Printf("[MAT][CHC]Hold size after attempting to apply commits on hold: %d\n", part.commitsOnHold.Len())
}

func (part *partition) handleMatRemoteTxn(args MatRemoteTxnArgs) {
	//fmt.Println("[MAT]RemoteTxn. Len: ", len(args.Upds))
	//startTs := time.Now().UnixNano()
	newDown := make([]crdt.UpdateObjectParams, 0, utilities.MinInt(expectedNewDownstreamSize, len(args.Upds)))
	newDown = part.applyRemoteUpds(args.MatRemoteTxn, newDown)
	part.updateClockWithCommit(args.Timestamp) //In theory an UpdatePos should be enough?
	args.ReplyChan <- newDown
	/*end := time.Now()
	diff := (end.UnixNano() - startTs) / int64(time.Millisecond)
	fmt.Printf("[MAT%d]Replied from remoteTxn at %s. Took: %dms. Key: %s, %s. NUpds: %d.\n",
		part.id, end.Format("2006-01-02 15:04:05.000"), diff, args.MatRemoteTxn.Upds[0].Key, args.MatRemoteTxn.Upds[0].Bucket, len(args.MatRemoteTxn.Upds))*/
}

func (part *partition) handleMatClkPosUpd(args MatClkPosUpdArgs) {
	part.updateClockWithRemoteTS(args.ReplicaID, args.StableTs)
	args.ReplyChan <- true
}

func (part *partition) handleMatRemoteGroupTxn(args MatRemoteGroupTxnArgs) {
	//fmt.Println("[MAT]RemoteGroupTxn")
	//startTs := time.Now().UnixNano()
	newDown := make([]crdt.UpdateObjectParams, 0, expectedNewDownstreamSize*(1+len(args.Txns)/5))
	for _, txn := range args.Txns {
		newDown = part.applyRemoteUpds(txn, newDown)
	}
	part.updateClockWithCommit(args.FinalClk)
	//fmt.Printf("[MAT]RemoteGroupTxnEnd (nTxns: %d)\n", len(args.Txns))
	args.ReplyChan <- newDown
	/*end := time.Now()
	diff := (end.UnixNano() - startTs) / int64(time.Millisecond)
	fmt.Printf("[MAT%d]Replied from remoteGroupTxn at %s. Took: %dms. Key: %s, %s. NUpds: %d\n",
		part.id, end.Format("2006-01-02 15:04:05.000"), diff, args.Txns[0].Upds[0].Key, args.Txns[0].Upds[0].Bucket, len(args.Txns))*/
}

func (part *partition) applyRemoteUpds(args MatRemoteTxn, newDownstream []crdt.UpdateObjectParams) (newDown []crdt.UpdateObjectParams) {
	//newDownstream = make([]crdt.UpdateObjectParams, 0, tools.MinInt(expectedNewDownstreamSize, len(args.Upds)))
	var hashKey uint64
	var obj VersionManager
	for _, upd := range args.Upds {
		hashKey = getHash(getCombinedKey(upd.KeyParams))
		obj = part.getObjectForUpd(hashKey, upd.KeyParams, args.Timestamp)
		//TODO: Update this in order to avoid forced cast
		var generatedDownstream crdt.UpdateArguments = obj.Downstream(args.Timestamp, (upd.UpdateArgs).(crdt.DownstreamArguments))
		//non-uniform CRDTs may generate downstream updates when applying remote ops. We'll "commit" those upds with the stable clock
		if generatedDownstream != nil {
			//fmt.Printf("[MAT][Part%d]NuCRDT generated new downstream updates afterr applying remote txn! KeyParams: %+v. Clock: %s\n", part.partID,
			//upd.KeyParams, time.Now().Format("2006-01-02 15:04:05.000"))
			newDownstream = append(newDownstream, crdt.UpdateObjectParams{KeyParams: upd.KeyParams, UpdateArgs: generatedDownstream})
		}
	}
	part.debugLogUpdParams(args.Upds)
	return newDownstream
}

func (part *partition) handleMatPrepareRemote(args MatPrepareForRemoteArgs) {
	part.pendingOpsRemote[args.TransactionId] = args.Updates
	part.prepareClock(args.TransactionId)
	args.ReplyChan <- part.prepClks.PeekMax().clk
}

// Tells the partition's logger what is the latest safe clock that can be sent.
func (part *partition) handleMatSafeClk(args MatSafeClkArgs) {
	/*if part.prepClks.Len() == 0 {
		args.ReplyChan <- part.stableClk.NextTimestamp(part.replicaID) //Will send the latest timestamp
	} else {
		safeClk := part.prepClks.PeekMin().clk.Copy()
		safeClk.UpdateForcedPos(part.replicaID, safeClk.GetPos(part.replicaID)-1) //Subtracting 1 for safety: no commit will be < than minimum prepare.
		args.ReplyChan <- safeClk
	}*/
	//fmt.Printf("[MAT%d]Sending MatSafeClk at %v.\n", part.id, time.Now().Format("2006-01-02 15:04:05.000"))
	if part.prepClks.Len() == 0 {
		part.log.SendLoggerRequest(LoggerRequest{LogRequestArgs: LogClkArgs{Clk: part.stableClk.NextTimestamp(part.replicaID)}}) //Will send the latest timestamp
	} else {
		safeClk := part.prepClks.PeekMin().clk.Copy()
		safeClk.UpdateForcedPos(part.replicaID, safeClk.GetPos(part.replicaID)-1) //Subtracting 1 for safety: no commit will be < than minimum prepare.
		part.log.SendLoggerRequest(LoggerRequest{LogRequestArgs: LogClkArgs{Clk: safeClk}})
	}
}

// For replicator's join.
func (part *partition) handleMatCommitedClk(args MatCommitedClkArgs) {
	args.ReplyChan <- TimestampPartIdPair{Timestamp: part.stableClk.Copy(), partID: part.id}
}

func (part *partition) handleMatReset(args MatResetArgs) {
	part.log.Reset()
	part.partitionClock.reset()
	part.db = make(map[uint64]VersionManager)
	part.pendingOps = make(map[TransactionId][]crdt.UpdateObjectParams)
	part.commitsOnHold = commitHeap{entries: make([]MatCommitArgs, heapSize), nEntries: new(int)}
}

func (part *partition) handleMatGC(args MatGCArgs) {
	clkKey := args.SafeClk.GetMapKey()
	for _, obj := range part.db {
		obj.GC(args.SafeClk, clkKey)
	}
}

func (part *partition) handleMatBCUpdPerm(args MatBCUpdPermissions) {
	part.bcInfo.updatePermissions(part.db, args.ReplyChan)
}

func (part *partition) applyBCPermsUpds(txnId TransactionId, ts clocksi.Timestamp) (downstreamUpds []crdt.UpdateObjectParams) {
	ops, replicaID := part.bcInfo.opsForTxn[txnId], part.bcInfo.permsForReplica[txnId]
	downstreamUpds = make([]crdt.UpdateObjectParams, len(ops))
	var obj *crdt.BoundedCounterCrdt
	var hashKey uint64
	i := 0

	for key, value := range ops {
		hashKey = getHash(getCombinedKey(key))
		obj = part.getObjectForUpd(hashKey, key, ts).GetLatestCRDT().(*crdt.BoundedCounterCrdt)
		downArgs := obj.Update(crdt.TransferCounter{ToTransfer: value, FromID: part.replicaID, ToID: replicaID, ToReplicate: new(bool)})
		if (downArgs != nil && downArgs != crdt.NoOp{}) {
			obj.Downstream(ts, downArgs)
			if downArgs.MustReplicate() {
				downstreamUpds[i] = crdt.UpdateObjectParams{KeyParams: key, UpdateArgs: downArgs.(crdt.UpdateArguments)}
				i++
			}
		}
	}
	delete(part.bcInfo.opsForTxn, txnId)
	delete(part.bcInfo.permsForReplica, txnId)

	return downstreamUpds[:i]
}

func (part *partition) handleMatGetCRDT(args MatGetCRDTArgs) {
	crdtVM, has := part.db[getHash(getCombinedKey(args.KeyParams))]
	if !has {
		fmt.Printf("[MAT%d]MatGetCRDT: did not find CRDT with KeyParams %v\n", part.partID, args.KeyParams)
		args.ReplyChan <- CRDTPosPair{CRDT: &crdt.EmptyCrdt{}, Pos: args.Pos}
	} else {
		args.ReplyChan <- CRDTPosPair{CRDT: crdtVM.GetLatestCRDT(), Pos: args.Pos}
	}
}

//Generic helper functions

// func (part *partition) getObject(keyParams crdt.KeyParams, hashKey uint64, clk clocksi.Timestamp) VersionManager {
func (part *partition) getObject(keyParams crdt.KeyParams, clk clocksi.Timestamp) VersionManager {
	hashKey := getHash(getCombinedKey(keyParams))
	obj, hasKey := part.db[hashKey]
	if !hasKey {
		obj = part.initializeVersionManager(keyParams.CrdtType, clk)
	}
	return obj
}

// Also stores the object if non-existent
func (part *partition) getObjectForUpd(hashKey uint64, keyP crdt.KeyParams, clk clocksi.Timestamp) VersionManager {
	obj, hasKey := part.db[hashKey]
	if !hasKey {
		obj = part.initializeVersionManager(keyP.CrdtType, clk)
		part.db[hashKey] = obj
		if keyP.CrdtType == proto.CRDTType_FATCOUNTER {
			part.bcInfo.addKey(keyP)
		}
	}
	return obj
}

// Will be nil if there is no pending ops
func (part *partition) getObjPendingOps(keyParams crdt.KeyParams, txnId TransactionId) (objPending []crdt.UpdateArguments) {
	pendingOps, hasPending := part.pendingOps[txnId]
	if hasPending {
		objPending = make([]crdt.UpdateArguments, 0, len(pendingOps))
		for _, upd := range pendingOps {
			if upd.Key == keyParams.Key && upd.Bucket == keyParams.Bucket && upd.CrdtType == keyParams.CrdtType {
				objPending = append(objPending, upd.UpdateArgs)
			}
		}
	}
	//It is okay to return nil if there is no pending
	return
}

func (part *partition) initializeVersionManager(crdtType proto.CRDTType, clk clocksi.Timestamp) (newVM VersionManager) {
	crdt := part.initializeCrdt(crdtType)
	//Variable defined in versionManager.go
	return BaseVM.Initialize(crdt, clk)
}

func (part *partition) initializeCrdt(crdtType proto.CRDTType) (newCrdt crdt.CRDT) {
	return crdt.InitializeCrdt(crdtType, part.replicaID)
}

/*
Called by the TransactionManager or any other entity that may want to communicate with the Materializer
and doesn't yet know the appropriate channel
*/
func (mat *Materializer) SendRequest(request MaterializerRequest) {
	mat.channels[request.getChannel()] <- request
}

/*
Called by the TransactionManager or any other entity that may want to communicate with the Materializer
when they know the appropriate channel. Avoids computing an extra hash.
*/
func (mat *Materializer) SendRequestToChannel(request MaterializerRequest, channelKey uint64) {
	/*timer := time.NewTimer(7 * time.Second)
	select {
	case mat.channels[channelKey] <- request:
		timer.Stop()
	case <-timer.C:
		fmt.Printf("[MAT]Timeout while sending request of type %v for partition %d. Number of reqs in partition's queue: %d, at %v.\n", request.getRequestType(), channelKey, len(mat.channels[channelKey]), time.Now().Format("2006-01-02 15:04:05.000"))
		time.Sleep(2 * time.Second)
	}*/
	mat.channels[channelKey] <- request
}

func (mat *Materializer) SendRequestToChannels(request MaterializerRequest, channelsToSend ...chan MaterializerRequest) {
	for _, channel := range channelsToSend {
		//Copy the request to avoid concurrent access problems
		newReq := request
		channel <- newReq
	}
}

func (mat *Materializer) SendRequestToAllChannels(request MaterializerRequest) {
	for _, channel := range mat.channels {
		//Copy the request to avoid concurrent access problems
		newReq := request
		channel <- newReq
	}
}

/*func GetChannelKeyRead(args MatReadCommonArgs) (key uint64) {
	*args.HashKey = getHash(getCombinedKey(args.KeyParams))
	//fmt.Println("[MAT]GetChannelKeyRead. Setting hashKey to", *args.HashKey)
	key = *args.HashKey / keyRangeSize
	//Overflow, which might happen due to rounding
	if key == nGoRoutines {
		key -= 1
	}
	return
}*/

func GetChannelKey(keyParams crdt.KeyParams) (key uint64) {
	hashKey := getHash(getCombinedKey(keyParams))
	key = hashKey / keyRangeSize
	//Overflow, which might happen due to rounding
	if key == nGoRoutines {
		key -= 1
	}
	return
}

func getCombinedKey(keyParams crdt.KeyParams) (combKey string) {
	combKey = keyParams.Bucket + keyParams.CrdtType.String() + keyParams.Key
	return
}

func getHash(combKey string) (hash uint64) {
	hash = hashFunc.StringSum64(combKey)
	return
}

func GetNumberPartitions() int {
	return int(nGoRoutines)
}

//PartitionClock functions

func (partClock *partitionClock) prepareClock(txnId TransactionId) {
	//Clocks are monotonically increasing (they are based on real time clock). Can safely use get next timestamp from stableClk
	newTs := partClock.getNextClk(partClock.stableClk)
	//fmt.Println("[MAT]Pushing clock:", TxnIdClkPair{clk: newTs, txnId: txnId})
	heap.Push(&partClock.prepClks, TxnIdClkPair{clk: newTs, txnId: txnId})
	//partClock.prepClks.innerMap[txnId] = partClock.prepClks.Len() - 1 //It always goes to the last position
	//min := partClock.prepClks.PeekMin()
	//fmt.Println("[MAT][PrepareClock]Min clk", min)
	//fmt.Println()
}

func (partClock *partitionClock) getNextClk(baseClk clocksi.Timestamp) (newClk clocksi.Timestamp) {
	newClk = baseClk.NextTimestamp(partClock.replicaID)
	//Ensure the clock is bigger. It may be the same as some previously generated clk if two are generated too quickly
	//Note: the clocksi implementation provides a solution for this, but only works when generating a new TS using as base the last one used.
	newReplicaTS := newClk.GetPos(partClock.replicaID)
	if newReplicaTS <= partClock.lastUsedTs { //<=, in the (extremely rare) occasion three timestamps in a row are generated with same value
		partClock.lastUsedTs++
		newClk.UpdatePos(partClock.replicaID, partClock.lastUsedTs)
	} else {
		partClock.lastUsedTs = newReplicaTS
	}
	return
}

func (partClock *partitionClock) getLowestPrepare() (pair TxnIdClkPair) {
	return partClock.prepClks.PeekMin()
}

func (partClock *partitionClock) removePrepare(txnId TransactionId) {
	pos := partClock.prepClks.innerMap[txnId]
	//fmt.Println("[MAT]Removing prepare at pos:", pos, ", prepClks has length:", partClock.prepClks.Len())
	heap.Remove(&partClock.prepClks, pos)
	//delete(partClock.prepClks.innerMap, txnId)
}

func (partClock *partitionClock) removeLowestPrepare() {
	heap.Pop(&partClock.prepClks)
	//txnId := heap.Pop(&partClock.prepClks).(TxnIdClkPair).txnId
	//delete(partClock.prepClks.innerMap, txnId)
}

func (partClock *partitionClock) updateClockWithCommit(clk clocksi.Timestamp) {
	partClock.stableClk = partClock.stableClk.Merge(clk)
}

func (partClock *partitionClock) updateClockWithRemoteTS(remoteID int16, ts int64) {
	partClock.stableClk.UpdatePos(remoteID, ts)
}

func (partClock *partitionClock) reset() {
	partClock.lastUsedTs = 0
	partClock.stableClk = clocksi.NewClockSiTimestamp()
	partClock.prepClks = prepClockHeap{entries: make([]TxnIdClkPair, heapSize), nEntries: new(int), innerMap: make(map[TransactionId]int)}
}

// BoundedCounter Info methods
func (bc *bcInfo) addKey(keyP crdt.KeyParams) {
	if len(bc.bcKeys) == bc.nKeys {
		bc.bcKeys = append(bc.bcKeys, keyP)
	} else {
		bc.bcKeys[bc.nKeys] = keyP
		bc.nKeys++
	}
}

func (bc *bcInfo) updatePermissions(db map[uint64]VersionManager, replyChan chan MatBCUpdPermissionsReply) {
	reqsMap := make(map[int16]map[crdt.KeyParams]int32) //replicaID -> (keyP -> value)
	for _, id := range bc.replicaIDs {
		reqsMap[id] = make(map[crdt.KeyParams]int32)
	}
	var toAsk int32
	var askID int16
	var keyP crdt.KeyParams
	for i := 0; i < bc.nKeys; i++ {
		keyP = bc.bcKeys[i]
		toAsk, askID = db[getHash(getCombinedKey(keyP))].GetLatestCRDT().(*crdt.BoundedCounterCrdt).RequestTransfer()
		if toAsk != -1 { //-1: doesn't need to ask for permissions
			reqsMap[askID][keyP] = toAsk
		}
	}
	replyChan <- MatBCUpdPermissionsReply{ReqsMap: reqsMap, PartID: bc.partID}
}

//Debug methods

func (part *partition) debugLogUpdParams(upds []crdt.UpdateObjectParams) {
	if debugMode {
		for _, upd := range upds {
			part.hashToParams[getHash(getCombinedKey(upd.KeyParams))] = upd.KeyParams
		}
	}
}

// Checks if all the queues are empty. The output is only relevant if the partition is inactive (i.e., not actively processing requests)
func (part *partition) sanityCheck() {
	previousNCommits, nZero := 0, 0
	for {
		time.Sleep(3 * time.Second)
		if part.nCommits < 10 || previousNCommits != part.nCommits {
			//All OK, still progress being made
			if part.nCommits == 0 {
				nZero++
			}
			if nZero%7 == 0 {
				currTime := time.Now().Format("2006-01-02 15:04:05.000")
				fmt.Printf("[MAT%d]Partition has no commits at %s.\n", part.id, currTime)
				if part.prepClks.Len() > 0 {
					fmt.Printf("[MAT%d]Prep clocks is not empty!!! Time: %s. NCommits: %d. Size: %d. PrepClocks: %v\n.", part.id, currTime, part.nCommits, part.prepClks.Len(), part.prepClks.entries[:part.prepClks.Len()])
				}
				if part.commitsOnHold.Len() > 0 {
					fmt.Printf("[MAT%d]Commits on hold is not empty!!! Time: %s. NCommits: %d. Size: %d. CommitsOnHold: %v\n", part.id, currTime, part.nCommits, part.commitsOnHold.Len(), part.commitsOnHold.entries[:part.commitsOnHold.Len()])
				}
				if len(part.pendingOps) > 0 {
					fmt.Printf("[MAT%d]PendingOps is not empty!!! Time: %s. NCommits: %d. Size: %d.\n", part.id, currTime, part.nCommits, len(part.pendingOps) /*, part.pendingOps*/)
				}
				nZero = 0
			}
			previousNCommits = part.nCommits
			continue
		} else if part.nCommits == previousNCommits {
			currTime := time.Now().Format("2006-01-02 15:04:05.000")
			fmt.Printf("[MAT%d]No new commits (part.nCommits: %d. previousNCommits: %d) at %s.\n", part.id, part.nCommits, previousNCommits, currTime)
			if part.prepClks.Len() > 0 {
				fmt.Printf("[MAT%d]Prep clocks is not empty!!! Time: %s. NCommits: %d. Size: %d. PrepClocks: %v\n", part.id, currTime, part.nCommits, part.prepClks.Len(), part.prepClks.entries[:part.prepClks.Len()])
			}
			if part.commitsOnHold.Len() > 0 {
				fmt.Printf("[MAT%d]Commits on hold is not empty!!! Time: %s. NCommits: %d. Size: %d. CommitsOnHold: %v\n", part.id, currTime, part.nCommits, part.commitsOnHold.Len(), part.commitsOnHold.entries[:part.commitsOnHold.Len()])
			}
			if len(part.pendingOps) > 0 {
				fmt.Printf("[MAT%d]PendingOps is not empty!!! Time: %s. NCommits: %d. Size: %d\n", part.id, currTime, part.nCommits, len(part.pendingOps))
			}
			break
		} else {
			currTime := time.Now().Format("2006-01-02 15:04:05.000")
			fmt.Printf("[MAT%d]Commits are still going on. Previous: %d. Current: %d. Time: %s\n", part.id, previousNCommits, part.nCommits, currTime)
		}
		previousNCommits = part.nCommits
	}
	/*
		time.Sleep(50 * time.Second)
		fmt.Println("[MAT]Clock:", part.stableClk.ToSortedString())
		err := false
		if part.prepClks.Len() > 0 {
			fmt.Printf("[MAT]Prep clocks is not empty!!! PrepClocks: %v\n", part.prepClks.entries)
			err = true
		}
		if part.commitsOnHold.Len() > 0 {
			fmt.Printf("[MAT]Commits on hold is not empty!!! CommitsOnHold: %v\n", part.commitsOnHold.entries)
			err = true
		}
		if len(part.pendingOps) > 0 {
			fmt.Printf("[MAT]PendingOps is not empty!!! PendingOps: %v\n", part.pendingOps)
			err = true
		}
		if err {
			return
		}

		//Check the bucket of "INDEX"
		var sb strings.Builder
		//TODO: Sort keys
		sortedKeys := make([]uint64, len(part.hashToParams))
		i := 0
		for hash, keyParams := range part.hashToParams {
			if keyParams.Bucket == "INDEX" {
				sortedKeys[i] = hash
				i++
			}
		}
		sortedKeys = sortedKeys[:i]
		sort.Slice(sortedKeys, func(i, j int) bool { return sortedKeys[i] < sortedKeys[j] })
		for _, hash := range sortedKeys {
			obj, keyParams := part.db[hash], part.hashToParams[hash]
			fmt.Fprintf(&sb, "[%s,%s,%s]:", keyParams.Key, keyParams.Bucket, proto.CRDTType_name[int32(keyParams.CrdtType)])
			fmt.Fprintf(&sb, "%v\n", obj.ReadLatest(crdt.StateReadArguments{}, nil))
		}
		fmt.Print(sb.String())
	*/
}

// Helper debug functions
func (part *partition) prepClocksToString() (res string) {
	/*var sb strings.Builder
	sb.WriteString("[")
	for i := 0; i < part.prepClks.Len(); i++ {
		str += fmt.Sprintf("(%d,%s); ", part.prepClks.entries[i].txnId, part.prepClks.entries[i].clk.ToSortedString())
	}
	str += "]"*/
	return
}

/*
if part.prepClks.Len() > 0 {
				fmt.Printf("[MAT%d]Prep clocks is not empty!!! Time: %s. NCommits: %d. Size: %d. PrepClocks: %v\n", part.id, currTime, part.nCommits, part.prepClks.Len(), part.prepClks.entries[:part.prepClks.Len()])
			}
			if part.commitsOnHold.Len() > 0 {
				fmt.Printf("[MAT%d]Commits on hold is not empty!!! Time: %s. NCommits: %d. Size: %d. CommitsOnHold: %v\n", part.id, currTime, part.nCommits, part.commitsOnHold.Len(), part.commitsOnHold.entries[:part.commitsOnHold.Len()])
*/
