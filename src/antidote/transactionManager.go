package antidote

//TODO (high priority): check where channels can be shared when sending requests to partitions - this would be more efficient
//TODO: Read-write lock for the clock? That might help when doing queries-only.

//A circular array. Each instance adds a read clock (in order) to the array and stores its position
//Then, when the read clears, said position is marked as clear and can be rewritten.
//If the array ever gets full, a new one must be created.
//As such we need to keep the start and end position of said circular array
//Thus it is safe to clean up to exactly the start clock.
//Problem: if we start using the max between client's clock and server's clock, it will no longer work.
//This'll have to be dealt it in some special way...

import (
	"container/heap"
	fmt "fmt"
	"math/rand"
	"potionDB/src/clocksi"
	"potionDB/src/crdt"
	"potionDB/src/proto"
	"potionDB/src/tools"
	"strings"
	"sync"
	"time"
)

/////*****************TYPE DEFINITIONS***********************/////
//TODO: Extract requests types, replies and methods to another file

type KeyParams struct {
	Key      string
	CrdtType proto.CRDTType
	Bucket   string
}

type UpdateObjectParams struct {
	KeyParams
	UpdateArgs *crdt.UpdateArguments
}

type ReadObjectParams struct {
	KeyParams
	ReadArgs crdt.ReadArguments
}

type TransactionManagerRequest struct {
	TransactionId //TODO: Remove this, as most requests don't need it (iirc, only staticWrite, commit and abort use it)
	Timestamp     clocksi.Timestamp
	Args          TMRequestArgs
}

type TMRequestArgs interface {
	getRequestType() (requestType TMRequestType)
}

type TMReadArgs struct {
	ReadParams []ReadObjectParams
	ReplyChan  chan []crdt.State
}

type TMUpdateArgs struct {
	UpdateParams []*UpdateObjectParams
	ReplyChan    chan TMUpdateReply
}

type TMStaticUpdateArgs struct {
	UpdateParams []*UpdateObjectParams
	ReplyChan    chan TMStaticUpdateReply
}

type TMStaticReadArgs struct {
	ReadParams []ReadObjectParams
	ReplyChan  chan TMStaticReadReply
}

type TMConnLostArgs struct {
}

type TMStartTxnArgs struct {
	ReplyChan chan TMStartTxnReply
}

type TMCommitArgs struct {
	ReplyChan chan TMCommitReply
}

type TMAbortArgs struct {
}

type TMNewTriggerArgs struct {
	Source, Target Link
	IsGeneric      bool
	ReplyChan      chan bool
}

type TMGetTriggersArgs struct {
	WaitFor   chan bool
	ReplyChan chan *TriggerDB
}

type TMS2SRequest struct {
	ClientID int32
	Args     TMRequestArgs
}

type TMS2SReply struct {
	ClientID  int32
	TxnID     TransactionId
	ReplyType proto.WrapperType
	Reply     interface{}
}

type TMServerConn struct {
	ReplyChan chan TMS2SReply
}

/*****Remote/Replicator interaction structs*****/

//Used by the Replication Layer. Use a different thread to handle this
/*
type TMRemoteTxn struct {
	ReplicaID int16
	Upds      []NewRemoteTxns
	StableTs  int64
}
*/

type TMRemoteMsg interface {
	getReplicaID() int16
}

type TMRemoteClk struct {
	ReplicaID int16
	StableTs  int64
}

/*
type TMRemoteTxn struct {
	ReplicaID int16
	Clk       clocksi.Timestamp
	Upds      map[int][]*UpdateObjectParams
}

type TMRemoteTxnGroup struct {
	ReplicaID int16
	Txns []
}*/

type TMGetSnapshot struct {
	Buckets   map[string]struct{}
	ReplyChan chan TMGetSnapshotReply
}

type TMApplySnapshot struct {
	clocksi.Timestamp
	PartStates [][]*proto.ProtoCRDT
}

type TMReplicaID struct {
	ReplicaID int16
	IP        string
	Buckets   []string
}

type TMRemoteTrigger struct {
	AutoUpdate
	IsGeneric bool
}

type TMStart struct {
}

/*****Msgs for handling ops generated by downstream remotes*****/

type TMDownstreamRemoteMsg interface {
}

type TMTxnForRemote struct {
	ops map[uint64][]*UpdateObjectParams
}

type TMMultipleTxnForRemote struct {
	ops map[uint64][][]*UpdateObjectParams
}

type TMNewRemoteTxn struct {
	clocksi.Timestamp
	nPartitions int
}

type TMDownstreamNewOps struct {
	clocksi.Timestamp
	partitionID int64
	newOps      []*UpdateObjectParams
}

/***** *****/

type TMStaticReadReply struct {
	States    []crdt.State
	Timestamp clocksi.Timestamp
}

type TMStaticUpdateReply struct {
	TransactionId
	Timestamp clocksi.Timestamp
	Err       error
}

type TMUpdateReply struct {
	Success bool
	Err     error
}

type TMStartTxnReply struct {
	TransactionId
	Timestamp clocksi.Timestamp
}

type TMCommitReply struct {
	Timestamp clocksi.Timestamp
	Err       error
}

type TMGetSnapshotReply struct {
	Timestamp  clocksi.Timestamp
	PartStates [][]*proto.ProtoCRDT
}

type TMNewTriggerReply struct{}

type TMGetTriggersReply struct {
}

type TMRequestType int

type ClientId uint64

/*
type TransactionId struct {
	ClientId  ClientId
	Timestamp clocksi.Timestamp
}
*/
type TransactionId uint64

type ongoingRemote struct {
	originalClk  clocksi.Timestamp
	txnDataToUse [][]byte
	nTxnsStarted int              //Number of connections with a txn started (txnDataToUse[i] != nil). When it's equal to len(conns), can skip a check to start txn.
	lockChans    []chan msgToSend //Channels to talk with other servers on non-static transactions
	replyChans   []chan msgReply
}

type ongoingTxn struct {
	TransactionId
	//partSet
	partitions []bool //true: partition participates; false: partition doesn't participate.
	debugID    int    //random ID just for debbuging purposes
	//conns      []net.Conn //connection to other replicas that have been created by this transaction
	ongoingRemote
}

//type partSet map[uint64]struct{}

type ProtectedClock struct {
	clocksi.Timestamp
	sync.Mutex
}

type ProtectedTriggerDB struct {
	TriggerDB
	sync.RWMutex
}

type TransactionManager struct {
	mat              *Materializer
	remoteChan       chan TMRemoteMsg
	localClock       ProtectedClock
	txnsSinceCompact int //Number of txns done since the last time history was compacted. Also protected by the above mutex.
	downstreamQueue  map[int16][]TMRemoteMsg
	replicator       *Replicator
	replicaID        int16
	downstreamOpsCh  chan TMTxnForRemote //Channel for handling ops that are generated when applying remote downstreams.
	waitStartChan    chan bool           //Channel for notifying ProtoServer when is TM ready to start processing requests
	triggerDB        ProtectedTriggerDB
	//Only used if doCompactHistory=true
	ongoingReads map[TransactionId]int //TxnID -> position in circular array
	clocksArray  *tools.CircularArray
	RemoteInfo
	connPool   *connPool
	commitChan chan TMCommitInfo //Clock updates go to this channel. When a client needs to wait for a clock, the request also goes here
}

type RemoteInfo struct {
	remoteBks     [][]string //Note: This gets turned to nil after all replicas are known
	remoteIPs     []string
	bucketToIndex map[string][]int
	ownBuckets    []string
	hasAll        bool //In case server is replicating "*"
}

/////*****************CONSTANTS AND VARIABLES***********************/////

const (
	readStaticTMRequest   TMRequestType = 0
	updateStaticTMRequest TMRequestType = 1
	readTMRequest         TMRequestType = 2
	updateTMRequest       TMRequestType = 3
	startTxnTMRequest     TMRequestType = 4
	commitTMRequest       TMRequestType = 5
	abortTMRequest        TMRequestType = 6
	newTriggerTMRequest   TMRequestType = 7
	getTriggersTMRequest  TMRequestType = 8
	serverConnRequest     TMRequestType = 80
	lostConnRequest       TMRequestType = 255

	downstreamOpsChBufferSize int = 100 //Default buffer size for the downstreamOpsCh
)

//Both are filled from configs
var (
	doCompactHistory         = false
	historyCompactInterval   = 60
	historyCompactTargetTxns = 1000
	circularArraySize        = 1000 //Array grows as needed
)

/////*****************TYPE METHODS***********************/////

//TransactionManagerRequest

func (args TMStaticReadArgs) getRequestType() (requestType TMRequestType) {
	return readStaticTMRequest
}

func (args TMStaticUpdateArgs) getRequestType() (requestType TMRequestType) {
	return updateStaticTMRequest
}

func (args TMReadArgs) getRequestType() (requestType TMRequestType) {
	return readTMRequest
}

func (args TMUpdateArgs) getRequestType() (requestType TMRequestType) {
	return updateTMRequest
}

func (args TMConnLostArgs) getRequestType() (requestType TMRequestType) {
	return lostConnRequest
}

func (args TMStartTxnArgs) getRequestType() (requestType TMRequestType) {
	return startTxnTMRequest
}

func (args TMCommitArgs) getRequestType() (requestType TMRequestType) {
	return commitTMRequest
}

func (args TMAbortArgs) getRequestType() (requestType TMRequestType) {
	return abortTMRequest
}

func (args TMNewTriggerArgs) getRequestType() (requestType TMRequestType) {
	return newTriggerTMRequest
}

func (args TMGetTriggersArgs) getRequestType() (requestType TMRequestType) {
	return getTriggersTMRequest
}

func (args TMServerConn) getRequestType() (requestType TMRequestType) {
	return serverConnRequest
}

func (args TMS2SRequest) getRequestType() (requestType TMRequestType) {
	return args.Args.getRequestType()
}

//TMRemoteMsg

func (req TMRemoteClk) getReplicaID() (id int16) {
	return req.ReplicaID
}

//Shared with replicator
func (req RemoteTxn) getReplicaID() (id int16) {
	return req.SenderID
}

//Shared with replicator
func (req RemoteTxnGroup) getReplicaID() (id int16) {
	return req.SenderID
}

func (args TMGetSnapshot) getReplicaID() (id int16) {
	return 0 //Irrelevant
}

func (args TMApplySnapshot) getReplicaID() (id int16) {
	return 0 //Irrelevant
}

func (args TMStart) getReplicaID() (id int16) {
	return 0 //Irrelevant
}

func (args TMReplicaID) getReplicaID() (id int16) {
	return args.ReplicaID
}

//ReplicaID is irrelevant. It's only here for interface
func (args TMRemoteTrigger) getReplicaID() (id int16) {
	return 0
}

func (req RemoteTxnGroup) getMinClk() (clk clocksi.Timestamp) {
	return req.Txns[0].Clk
}

func (req RemoteTxnGroup) getMaxClk() (clk clocksi.Timestamp) {
	return req.Txns[len(req.Txns)-1].Clk
}

//Others

/*
func makePartSet() (set partSet) {
	set = partSet(make(map[uint64]struct{}))
	return
}

func (set partSet) add(partId uint64) {
	set[partId] = struct{}{}
}
*/

func (txnPartitions *ongoingTxn) reset() {
	txnPartitions.TransactionId = 0
	//txnPartitions.partSet = nil
	txnPartitions.partitions = make([]bool, nGoRoutines)
	//txnPartitions.ongoingRemote = ongoingRemote{}
	txnPartitions.ongoingRemote.reset()
}

func (remote *ongoingRemote) reset() {
	remote.originalClk, remote.txnDataToUse, remote.nTxnsStarted = nil, nil, 0
}

/////*****************TRANSACTION MANAGER CODE***********************/////

func (tm *TransactionManager) ResetServer() {
	matChan := make(chan bool, len(tm.mat.channels))
	tm.mat.SendRequestToAllChannels(MaterializerRequest{MatRequestArgs: MatResetArgs{ReplyChan: matChan}})
	tm.replicator.Reset()
	tm.remoteChan = make(chan TMRemoteMsg)
	tm.localClock = ProtectedClock{Mutex: sync.Mutex{}, Timestamp: clocksi.NewClockSiTimestamp()}
	tm.downstreamQueue = make(map[int16][]TMRemoteMsg)
	for i := 0; i < len(tm.mat.channels); i++ {
		<-matChan
	}
	fmt.Println("[TM]Reset complete.")
}

func Initialize(replicaID int16) (tm *TransactionManager) {
	fmt.Println("[TM]TopKSize defined in configs:", tools.SharedConfig.GetIntConfig("topKSize", 100))
	crdt.SetTopKSize(tools.SharedConfig.GetIntConfig("topKSize", 100))
	clocksi.AddNewID(replicaID)
	downstreamOpsCh := make(chan TMTxnForRemote, downstreamOpsChBufferSize)
	commitCh := make(chan TMCommitInfo, 100) //TODO: Make the 100 a variable or constant
	mat, loggers := InitializeMaterializer(replicaID, commitCh)
	buckets := getBucketsFromConfig()
	//mat, _, _ := InitializeMaterializer(replicaID)
	tm = &TransactionManager{
		mat:              mat,
		remoteChan:       make(chan TMRemoteMsg, 100), //TODO: Make that 100 a variable too
		localClock:       ProtectedClock{Mutex: sync.Mutex{}, Timestamp: clocksi.NewClockSiTimestamp()},
		txnsSinceCompact: 0,
		downstreamQueue:  make(map[int16][]TMRemoteMsg),
		replicator:       &Replicator{},
		replicaID:        replicaID,
		downstreamOpsCh:  downstreamOpsCh,
		waitStartChan:    make(chan bool, 1),
		triggerDB:        ProtectedTriggerDB{RWMutex: sync.RWMutex{}, TriggerDB: InitializeTriggerDB()},
		RemoteInfo: RemoteInfo{
			bucketToIndex: make(map[string][]int),
			hasAll:        false,
		},
		commitChan: commitCh,
	}
	tm.ownBuckets = buckets
	//Check if server replicates all buckets
	for _, bkt := range tm.ownBuckets {
		if bkt == "*" {
			tm.hasAll = true
			break
		}
	}
	if doCompactHistory {
		tm.ongoingReads, tm.clocksArray = make(map[TransactionId]int), &tools.CircularArray{}
		tm.clocksArray.Initialize(circularArraySize)
		go tm.doHistoryCompact()
	}
	go tm.replicator.Initialize(tm, loggers, buckets, replicaID)
	go tm.handleCommitReplies()
	go tm.handleRemoteMsgs()
	nDownGenHandlers := 8 //TODO: Put this as some variable that can be configured.
	for i := 0; i < nDownGenHandlers; i++ {
		go tm.handleDownstreamGeneratedOps()
	}
	if debugMode {
		go tm.sanityCheck()
	}

	return tm
}

func getBucketsFromConfig() []string {
	stringBuckets, has := tools.SharedConfig.GetAndHasConfig("buckets")
	if !has {
		return []string{"*"}
	} else {
		return strings.Split(stringBuckets, " ")
	}
}

func CreateKeyParams(key string, crdtType proto.CRDTType, bucket string) (keyParams KeyParams) {
	return KeyParams{Key: key, CrdtType: crdtType, Bucket: bucket}
}

func (tm *TransactionManager) WaitUntilReady() {
	<-tm.waitStartChan
}

//Starts a goroutine to handle the client requests. Returns a channel to communicate with that goroutine
func (tm *TransactionManager) CreateClientHandler() (channel chan TransactionManagerRequest) {
	channel = make(chan TransactionManagerRequest)
	go tm.listenForProtobufRequests(channel)
	return
}

func (tm *TransactionManager) SendRemoteMsg(msg TMRemoteMsg) {
	tm.remoteChan <- msg
}

func (tm *TransactionManager) listenForProtobufRequests(channel chan TransactionManagerRequest) {
	stop := false
	var txnPartitions *ongoingTxn = &ongoingTxn{}
	txnPartitions.partitions = make([]bool, nGoRoutines)
	txnPartitions.debugID = rand.Intn(10)
	txnPartitions.ongoingRemote = ongoingRemote{}
	txnPartitions.lockChans = make([]chan msgToSend, len(tm.remoteIPs))
	txnPartitions.replyChans = make([]chan msgReply, len(tm.remoteIPs))

	firstReq := <-channel

	if firstReq.Args.getRequestType() == serverConnRequest {
		tm.handleServerRequests(channel, firstReq.Args.(TMServerConn).ReplyChan)
	}
	/*
		if firstReq.Args.getRequestType() != serverConnRequest {
			tm.connPool.newConn()
			stop = tm.handleTMRequest(firstReq, txnPartitions)
		}
	*/
	tm.connPool.newConn()
	stop = tm.handleTMRequest(firstReq, txnPartitions)
	for !stop {
		request := <-channel
		stop = tm.handleTMRequest(request, txnPartitions)
	}

	tools.FancyDebugPrint(tools.TM_PRINT, tm.replicaID, "connection lost, shutting down goroutine for client.")
}

func (tm *TransactionManager) handleTMRequest(request TransactionManagerRequest,
	txnPartitions *ongoingTxn) (shouldStop bool) {
	shouldStop = false

	switch request.Args.getRequestType() {
	case readStaticTMRequest:
		tm.handleStaticTMRead(request)
	case updateStaticTMRequest:
		tm.handleStaticTMUpdate(request)
	case readTMRequest:
		tm.handleTMRead(request, txnPartitions)
	case updateTMRequest:
		tm.handleTMUpdate(request, txnPartitions)
	case startTxnTMRequest:
		tm.handleTMStartTxn(request, txnPartitions)
	case commitTMRequest:
		tm.handleTMCommit(request, txnPartitions)
	case abortTMRequest:
		tm.handleTMAbort(request, txnPartitions)
	case newTriggerTMRequest:
		tm.handleNewTrigger(request)
	case getTriggersTMRequest:
		tm.handleGetTriggers(request)
	case lostConnRequest:
		shouldStop = true
		txnPartitions = nil
	}
	//remoteTxnRequest is handled separatelly

	return
}

func (tm *TransactionManager) handleRemoteMsgs() {
	for {
		request := <-tm.remoteChan
		//fmt.Println("[TM]Got request from Replicator")
		switch typedReq := request.(type) {
		case TMRemoteClk:
			tm.applyRemoteClk(&typedReq)
		case RemoteTxn:
			tm.applyRemoteTxn(&typedReq)
		case RemoteTxnGroup:
			tm.applyRemoteTxnGroup(&typedReq)
		case TMGetSnapshot:
			tm.handleTMGetSnapshot(&typedReq)
		case TMApplySnapshot:
			tm.handleTMApplySnapshot(&typedReq)
		case TMReplicaID:
			tm.handleReplicaID(&typedReq)
		case TMRemoteTrigger:
			tm.handleRemoteTrigger(&typedReq)
		case TMStart:
			tm.handleTMStart(&typedReq)
		}
		fmt.Println("[TM]Finished request from Replicator")
	}
}

/*
	What we need is to decouple the sendProto(reply) from the sendProto(request).
	That is, ProtoServer must be ready to receive another request before the reply finishes sending
	That is, TM can still be single threaded - receive request -> process -> reply.
*/

func (tm *TransactionManager) handleServerRequests(channel chan TransactionManagerRequest, replyChan chan TMS2SReply) {
	stop := false
	ongoingInfo := make(map[int32]*ongoingTxn)

	for !stop {
		//fmt.Println("[TM]Waiting for S2S request")
		request := <-channel
		innerArgs := request.Args.(TMS2SRequest)
		//fmt.Println("[TM]Got S2S request:", innerArgs.Args.getRequestType())
		clientID, txnID := innerArgs.ClientID, request.TransactionId
		switch innerArgs.Args.getRequestType() {
		case readStaticTMRequest:
			/*go func(clientID int32, txnID TransactionId, channel chan TMStaticReadReply) {
				replyChan <- TMS2SReply{ClientID: clientID, TxnID: txnID, ReplyType: proto.WrapperType_STATIC_READ_OBJS, Reply: <-channel}
			}(clientID, txnID, innerArgs.Args.(TMStaticReadArgs).ReplyChan)*/
			tm.handleStaticTMRead(TransactionManagerRequest{TransactionId: txnID,
				Timestamp: request.Timestamp, Args: innerArgs.Args})
		case updateStaticTMRequest:
			/*go func(clientID int32, txnID TransactionId, channel chan TMStaticUpdateReply) {
				replyChan <- TMS2SReply{ClientID: clientID, TxnID: txnID, ReplyType: proto.WrapperType_COMMIT, Reply: <-channel}
			}(clientID, txnID, innerArgs.Args.(TMStaticUpdateArgs).ReplyChan)*/
			tm.handleStaticTMUpdate(TransactionManagerRequest{TransactionId: txnID,
				Timestamp: request.Timestamp, Args: innerArgs.Args})
		case readTMRequest:
			/*go func(clientID int32, txnID TransactionId, channel chan []crdt.State) {
				replyChan <- TMS2SReply{ClientID: clientID, TxnID: txnID, ReplyType: proto.WrapperType_READ_OBJS, Reply: <-channel}
			}(clientID, txnID, innerArgs.Args.(TMReadArgs).ReplyChan)*/
			tm.handleTMRead(TransactionManagerRequest{TransactionId: txnID,
				Timestamp: request.Timestamp, Args: innerArgs.Args}, ongoingInfo[clientID])
		case updateTMRequest:
			/*go func(clientID int32, txnID TransactionId, channel chan TMUpdateReply) {
				replyChan <- TMS2SReply{ClientID: clientID, TxnID: txnID, ReplyType: proto.WrapperType_UPD, Reply: <-channel}
			}(clientID, txnID, innerArgs.Args.(TMUpdateArgs).ReplyChan)*/
			tm.handleTMUpdate(TransactionManagerRequest{TransactionId: txnID,
				Timestamp: request.Timestamp, Args: innerArgs.Args}, ongoingInfo[clientID])
		case startTxnTMRequest:
			var txnPartitions *ongoingTxn = &ongoingTxn{}
			txnPartitions.partitions, txnPartitions.debugID, txnPartitions.ongoingRemote = make([]bool, nGoRoutines), rand.Intn(10), ongoingRemote{}
			txnPartitions.lockChans, txnPartitions.replyChans = make([]chan msgToSend, len(tm.remoteIPs)), make([]chan msgReply, len(tm.remoteIPs))
			/*go func(clientID int32, txnID TransactionId, channel chan TMStartTxnReply) {
				replyChan <- TMS2SReply{ClientID: clientID, TxnID: txnID, ReplyType: proto.WrapperType_START_TXN, Reply: <-channel}
			}(clientID, txnID, innerArgs.Args.(TMStartTxnArgs).ReplyChan)*/
			tm.handleTMStartTxn(TransactionManagerRequest{TransactionId: txnID,
				Timestamp: request.Timestamp, Args: innerArgs.Args}, ongoingInfo[clientID])
		case commitTMRequest:
			/*go func(clientID int32, txnID TransactionId, channel chan TMCommitReply) {
				replyChan <- TMS2SReply{ClientID: clientID, TxnID: txnID, ReplyType: proto.WrapperType_COMMIT, Reply: <-channel}
			}(clientID, txnID, innerArgs.Args.(TMCommitArgs).ReplyChan)*/
			tm.handleTMCommit(TransactionManagerRequest{TransactionId: txnID,
				Timestamp: request.Timestamp, Args: innerArgs.Args}, ongoingInfo[clientID])
		case abortTMRequest:
			tm.handleTMAbort(TransactionManagerRequest{TransactionId: txnID,
				Timestamp: request.Timestamp, Args: innerArgs.Args}, ongoingInfo[clientID])
		//Doesn't need reply
		default:
			fmt.Println("[TM]Unknown request type for S2S:", innerArgs.Args.getRequestType())
		}
	}
}

func (tm *TransactionManager) handleStaticTMRead(request TransactionManagerRequest) {
	readArgs := request.Args.(TMStaticReadArgs)
	//tsToUse := request.Timestamp
	tsToUse := tm.getClockToUse(request.Timestamp)

	var historyPos int
	if doCompactHistory {
		historyPos = tm.clocksArray.Write(tsToUse)
	}

	var currRequest MaterializerRequest
	readChans := make([]chan crdt.State, len(readArgs.ReadParams))
	states := make([]crdt.State, len(readArgs.ReadParams))
	//remoteArgs := make([]ReadObjectParams, len(readArgs.ReadParams))

	reqsPerServer := make([][]ReadObjectParams, len(tm.remoteIPs))
	remoteReqsToChan := make([][]int, len(tm.remoteIPs))
	isRemote, serverIndex, hasRemote := true, 0, false

	for i, currRead := range readArgs.ReadParams {
		readChans[i] = make(chan crdt.State, 1)
		isRemote, serverIndex = tm.getReadLocation(currRead.Bucket)

		if !isRemote {
			currRequest = MaterializerRequest{
				MatRequestArgs: MatStaticReadArgs{MatReadCommonArgs: MatReadCommonArgs{
					Timestamp:        tsToUse,
					ReadObjectParams: currRead,
					ReplyChan:        readChans[i],
				}},
			}
			tm.mat.SendRequest(currRequest)
		} else {
			reqsPerServer[serverIndex] = append(reqsPerServer[serverIndex], currRead)
			remoteReqsToChan[serverIndex] = append(remoteReqsToChan[serverIndex], i)
			hasRemote = true
		}
	}

	if hasRemote {
		go tm.handleRemoteStaticReads(request.TransactionId, tsToUse, reqsPerServer, remoteReqsToChan, readChans)
	}

	for i, readChan := range readChans {
		states[i] = <-readChan
		close(readChan)
	}

	if doCompactHistory {
		tm.clocksArray.Delete(historyPos)
	}

	//fmt.Println("[TM]Static read with clk: ", tsToUse)
	readArgs.ReplyChan <- TMStaticReadReply{
		States:    states,
		Timestamp: tsToUse,
	}
}

//TODO: Separate in parts?
func (tm *TransactionManager) handleStaticTMUpdate(request TransactionManagerRequest) {
	updateArgs := request.Args.(TMStaticUpdateArgs)

	newTxnId := TransactionId(rand.Uint64())
	//1st step: discover involved partitions and group updates
	updsPerPartition, reqsPerServer, hasRemote := tm.groupWrites(updateArgs.UpdateParams)

	replyChan := make(chan clocksi.Timestamp, len(updsPerPartition))
	var currRequest MaterializerRequest
	waitFor := 0

	//2nd step: send update operations to each involved partition
	for partId, partUpdates := range updsPerPartition {
		if partUpdates != nil {
			waitFor++
			currRequest = MaterializerRequest{
				MatRequestArgs: MatStaticUpdateArgs{
					TransactionId: newTxnId,
					Updates:       partUpdates,
					ReplyChan:     replyChan,
				},
			}
			tm.mat.SendRequestToChannel(currRequest, uint64(partId))
		}
	}

	var maxTimestamp clocksi.Timestamp = clocksi.DummyTs
	//Also 2nd step: wait for reply of each partition
	for i := 0; i < waitFor; i++ {
		reply := <-replyChan
		if reply.IsHigherOrEqual(maxTimestamp) {
			maxTimestamp = reply
		}
	}

	//Step "2.5" - notify TM's handleCommitReplies() of the number of partitions for this txn.
	tm.commitChan <- TMCommitNPartitions{nPartitions: waitFor, txnId: newTxnId, clk: maxTimestamp}

	//3rd step: send commit to involved partitions
	//TODO: Should I not assume that the 2nd phase of commit is fail-safe?
	commitReq := MaterializerRequest{MatRequestArgs: MatCommitArgs{
		TransactionId:   newTxnId,
		CommitTimestamp: maxTimestamp,
	}}
	for partId, partUpdates := range updsPerPartition {
		if partUpdates != nil {
			tm.mat.SendRequestToChannel(commitReq, uint64(partId))
		}
	}

	//Request for remote operations. Wait for remote commit
	if hasRemote {
		tm.handleRemoteStaticUpds(request.TransactionId, maxTimestamp, reqsPerServer)
	}

	//4th step: send ok to client
	updateArgs.ReplyChan <- TMStaticUpdateReply{
		TransactionId: newTxnId,
		Timestamp:     maxTimestamp,
		Err:           nil,
	}

	/*
		Algorithm:
			1st step: discover involved partitions and group writes
				- for update in writeRequest.UpdateParams
					- getPartitionKey
					- add update to list
			2nd step: send update operations to each involved partition and collect proposed timestamp
				- for each partition involved
					- send list of updates
					- wait for proposed timestamp
					- if proposed timestamp > highest proposed timestamp so far
						highest timestamp = proposed timestamp
			3rd step: send commit to involved partitions
				- for each partition
					- commit(highest timestamp)
			4th step: send ok to client
	*/
}

//TODO: Group reads that go for the same partition
func (tm *TransactionManager) handleTMRead(request TransactionManagerRequest, txnPartitions *ongoingTxn) {
	readArgs := request.Args.(TMReadArgs)
	tsToUse := request.Timestamp

	/*
		var currReadChan chan crdt.State = nil
		var currRequest MaterializerRequest
		states := make([]crdt.State, len(readArgs.ReadParams))

		//Now, ask to read the client requested version.
		for i, currRead := range readArgs.ReadParams {
			currReadChan = make(chan crdt.State, 1)

			currRequest = MaterializerRequest{
				MatRequestArgs: MatReadArgs{MatReadCommonArgs: MatReadCommonArgs{
					Timestamp:        tsToUse,
					ReadObjectParams: currRead,
					ReplyChan:        currReadChan,
				}, TransactionId: request.TransactionId},
			}
			tm.mat.SendRequest(currRequest)
			states[i] = <-currReadChan
			close(currReadChan)
		}

		readArgs.ReplyChan <- states
		//++fmt.Println(tm.replicaID, "TM - finished handling read.")
	*/

	var currRequest MaterializerRequest
	readChans := make([]chan crdt.State, len(readArgs.ReadParams))
	states := make([]crdt.State, len(readArgs.ReadParams))

	reqsPerServer := make([][]ReadObjectParams, len(tm.remoteIPs))
	remoteReqsToChan := make([][]int, len(tm.remoteIPs))
	isRemote, serverIndex, hasRemote := true, 0, false

	for i, currRead := range readArgs.ReadParams {
		readChans[i] = make(chan crdt.State, 1)
		isRemote, serverIndex = tm.getReadLocation(currRead.Bucket)

		if !isRemote {
			currRequest = MaterializerRequest{
				MatRequestArgs: MatReadArgs{MatReadCommonArgs: MatReadCommonArgs{
					Timestamp:        tsToUse,
					ReadObjectParams: currRead,
					ReplyChan:        readChans[i],
				}, TransactionId: request.TransactionId},
			}
			tm.mat.SendRequest(currRequest)
		} else {
			reqsPerServer[serverIndex] = append(reqsPerServer[serverIndex], currRead)
			remoteReqsToChan[serverIndex] = append(remoteReqsToChan[serverIndex], i)
			hasRemote = true
		}
	}

	if hasRemote {
		go tm.handleRemoteReads(txnPartitions, reqsPerServer, remoteReqsToChan, readChans)
	}

	for i, readChan := range readChans {
		states[i] = <-readChan
		close(readChan)
	}

	readArgs.ReplyChan <- states
	//++fmt.Println(tm.replicaID, "TM - finished handling read.")
}

func (tm *TransactionManager) handleTMUpdate(request TransactionManagerRequest, txnPartitions *ongoingTxn) {
	//++fmt.Printf("%d TM%d - Started handling update.\n", tm.replicaID, txnPartitions.debugID)
	updateArgs := request.Args.(TMUpdateArgs)

	updsPerPartition, reqsPerServer, hasRemote := tm.groupWrites(updateArgs.UpdateParams)

	var currRequest MaterializerRequest
	var partId uint64

	for id, partUpdates := range updsPerPartition {
		if partUpdates != nil {
			partId = uint64(id)
			currRequest = MaterializerRequest{
				MatRequestArgs: MatUpdateArgs{
					TransactionId: request.TransactionId,
					Updates:       partUpdates,
				},
			}
			txnPartitions.partitions[partId] = true
			//++fmt.Printf("%d TM%d - Trying to send upds list.\n", tm.replicaID, txnPartitions.debugID)
			tm.mat.SendRequestToChannel(currRequest, partId)
			//++fmt.Printf("%d TM%d - Upds list sent.\n", tm.replicaID, txnPartitions.debugID)
		}
	}

	if hasRemote {
		tm.handleRemoteUpds(txnPartitions, reqsPerServer)
	}

	updateArgs.ReplyChan <- TMUpdateReply{Success: true, Err: nil}
	//++fmt.Printf("%d TM%d - Finished handling update.\n", tm.replicaID, txnPartitions.debugID)
}

func (tm *TransactionManager) handleNewTrigger(request TransactionManagerRequest) {
	args := request.Args.(TMNewTriggerArgs)
	src, target := args.Source, args.Target

	tm.triggerDB.Lock()
	fmt.Printf("Handling client trigger: %v\n", args)
	if args.IsGeneric {
		matchParams := tm.triggerDB.GetMatchableKeyParams(src.Key, src.Bucket, src.CrdtType)
		tm.triggerDB.AddGenericLink(matchParams, src, target)
	} else {
		tm.triggerDB.AddObject(src.KeyParams)
		tm.triggerDB.AddLink(src, target)
	}
	args.ReplyChan <- true
	tm.triggerDB.DebugPrint("[TM@NewT]")
	tm.triggerDB.Unlock()
	tm.replicator.remoteConn.SendTrigger(AutoUpdate{Trigger: src, Target: target}, args.IsGeneric)
}

func (tm *TransactionManager) handleGetTriggers(request TransactionManagerRequest) {
	args := request.Args.(TMGetTriggersArgs)

	tm.triggerDB.RLock()
	tm.triggerDB.DebugPrint("[TM@GetT]")
	args.ReplyChan <- &tm.triggerDB.TriggerDB
	<-args.WaitFor
	tm.triggerDB.RUnlock()
}

/*
	Returns an array in which each index corresponds to one partition.
	Associated to each index is the list of reads that belong to the referred partition
*/
func groupReads(reads []KeyParams) (readsPerPartition [][]KeyParams) {
	readsPerPartition = make([][]KeyParams, nGoRoutines)
	var currChanKey uint64

	for _, read := range reads {
		currChanKey = GetChannelKey(read)
		if readsPerPartition[currChanKey] == nil {
			readsPerPartition[currChanKey] = make([]KeyParams, 0, len(reads)*2/int(nGoRoutines))
		}
		readsPerPartition[currChanKey] = append(readsPerPartition[currChanKey], read)
	}

	return
}

/*
	Returns an array in which each index corresponds to one partition.
	Associated to each index is the list of writes that belong to the referred partition
	It also separates local updates from updates for objects non-locally replicated
*/
func (tm TransactionManager) groupWrites(updates []*UpdateObjectParams) (updsPerPartition [][]*UpdateObjectParams, reqsPerServer [][]UpdateObjectParams, hasRemote bool) {
	updsPerPartition, reqsPerServer = make([][]*UpdateObjectParams, nGoRoutines), make([][]UpdateObjectParams, len(tm.remoteIPs))
	var currChanKey uint64
	isRemote, serverIndex, hasRemote := true, 0, false

	for _, upd := range updates {
		isRemote, serverIndex = tm.getReadLocation(upd.Bucket)

		if !isRemote {
			currChanKey = GetChannelKey(upd.KeyParams)
			if updsPerPartition[currChanKey] == nil {
				updsPerPartition[currChanKey] = make([]*UpdateObjectParams, 0, len(updates)*2/int(nGoRoutines))
			}
			updsPerPartition[currChanKey] = append(updsPerPartition[currChanKey], upd)
		} else {
			reqsPerServer[serverIndex] = append(reqsPerServer[serverIndex], *upd)
			hasRemote = true
		}
	}

	return
}

func (tm *TransactionManager) handleTMStartTxn(request TransactionManagerRequest, txnPartitions *ongoingTxn) {
	//++fmt.Printf("%d TM%d - Started handling startTxn.\n", tm.replicaID, txnPartitions.debugID)
	startTxnArgs := request.Args.(TMStartTxnArgs)

	newClock := tm.getClockToUse(request.Timestamp)
	//fmt.Println("[TM]Got clock")
	txnPartitions.originalClk = newClock.Copy()
	txnPartitions.TransactionId = TransactionId(rand.Uint64())

	//Remote data
	//txnPartitions.conns = make([]net.Conn, len(tm.remoteIPs))
	txnPartitions.txnDataToUse = make([][]byte, len(tm.remoteIPs))

	if doCompactHistory {
		tm.ongoingReads[txnPartitions.TransactionId] = tm.clocksArray.Write(newClock)
	}
	//txnPartitions.partSet = makePartSet()
	//txnPartitions.partitions = make([]bool, nGoRoutines)
	//It's already initialized.

	startTxnArgs.ReplyChan <- TMStartTxnReply{TransactionId: txnPartitions.TransactionId, Timestamp: newClock}
	//++fmt.Printf("%d TM%d - Finished handling finishTxn.\n", tm.replicaID, txnPartitions.debugID)
}

func (tm *TransactionManager) handleTMCommit(request TransactionManagerRequest, txnPartitions *ongoingTxn) {
	//++fmt.Printf("%d TM%d - Started handling commit.\n", tm.replicaID, txnPartitions.debugID)
	commitArgs := request.Args.(TMCommitArgs)

	//PREPARE
	//involvedPartitions := txnPartitions.partSet
	involvedPartitions := txnPartitions.partitions
	var currRequest MaterializerRequest
	replyChan := make(chan clocksi.Timestamp, len(involvedPartitions))
	remoteChan := make(chan bool, 1)
	//if txnPartitions.nConns > 0 {
	if txnPartitions.nTxnsStarted > 0 {
		go tm.handleRemoteCommit(txnPartitions, remoteChan)
	}
	nPartitions := 0

	//Send prepare to each partition involved
	//++fmt.Printf("%d TM%d - Sending prepares to Materializers for id %d.\n", tm.replicaID, txnPartitions.debugID, request.TransactionId)
	//for partId, _ := range involvedPartitions {
	for partId, isOn := range involvedPartitions {
		if isOn {
			nPartitions++
			currRequest = MaterializerRequest{MatRequestArgs: MatPrepareArgs{TransactionId: request.TransactionId, ReplyChan: replyChan}}
			//++fmt.Printf("%d TM%d - Sending prepare to Materializer %d for id %d.\n", tm.replicaID, txnPartitions.debugID, partId, request.TransactionId)
			tm.mat.SendRequestToChannel(currRequest, uint64(partId))
		}
	}
	//}

	//Collect proposed timestamps and accept the maximum one
	var maxTimestamp clocksi.Timestamp = clocksi.DummyTs
	//++fmt.Printf("%d TM%d - Waiting for prepares from Materializers.\n", tm.replicaID, txnPartitions.debugID)
	//Wait for a reply from all partitions, by no order in particular
	for i := 0; i < nPartitions; i++ {
		replyTs := <-replyChan
		if replyTs.IsHigherOrEqual(maxTimestamp) {
			maxTimestamp = replyTs
		}
	}

	//Notify TM's handleCommitReplies() of the number of partitions for this txn.
	tm.commitChan <- TMCommitNPartitions{nPartitions: nPartitions, txnId: request.TransactionId, clk: maxTimestamp}

	//COMMIT
	//Send commit to involved partitions

	//++fmt.Printf("%d TM%d - Sending commits to Materializers.\n", tm.replicaID, txnPartitions.debugID)
	for partId, isOn := range involvedPartitions {
		if isOn {
			currRequest = MaterializerRequest{MatRequestArgs: MatCommitArgs{
				TransactionId:   request.TransactionId,
				CommitTimestamp: maxTimestamp,
			}}
			tm.mat.SendRequestToChannel(currRequest, uint64(partId))
		}
	}

	if doCompactHistory {
		tm.clocksArray.Delete(tm.ongoingReads[txnPartitions.TransactionId])
		delete(tm.ongoingReads, txnPartitions.TransactionId)
	}

	//if txnPartitions.nConns > 0 {
	if txnPartitions.nTxnsStarted > 0 {
		<-remoteChan //Wait for remote commits to be confirmed.
	}

	txnPartitions.reset()

	//Send ok to client
	commitArgs.ReplyChan <- TMCommitReply{
		Timestamp: maxTimestamp,
		Err:       nil,
	}
	//++fmt.Printf("%d TM%d - Finished handling commit.\n", tm.replicaID, txnPartitions.debugID)
}

func (tm *TransactionManager) handleTMAbort(request TransactionManagerRequest, txnPartitions *ongoingTxn) {
	abortReq := MaterializerRequest{MatRequestArgs: MatAbortArgs{TransactionId: request.TransactionId}}
	//for partId, _ := range txnPartitions.partSet {
	for partId, isOn := range txnPartitions.partitions {
		if isOn {
			tm.mat.SendRequestToChannel(abortReq, uint64(partId))
		}
	}
	if doCompactHistory {
		tm.clocksArray.Delete(tm.ongoingReads[txnPartitions.TransactionId])
	}

	//if txnPartitions.nConns > 0 {
	if txnPartitions.nTxnsStarted > 0 {
		remoteChan := make(chan bool, 1)
		tm.handleRemoteAbort(txnPartitions, remoteChan)
		<-remoteChan
	}
	txnPartitions.reset()
}

/*
type RemoteInfo struct {
	idToBuckets map[int16][]string
	idToIp      map[int16]string
	bucketToIPs map[string][]string
	ownBuckets  []string
}
*/

func (tm *TransactionManager) getClockToUse(clientTs clocksi.Timestamp) (tsToUse clocksi.Timestamp) {
	//fmt.Println("[TM]Requesting clock...")
	tm.localClock.Lock()
	copyClk := tm.localClock.Copy()
	tm.localClock.Unlock()
	tsToUse = copyClk.Merge(clientTs)
	//fmt.Printf("[TM]Clocks. TM: %s; Merged: %s; Client: %s\n",
	//copyClk.ToSortedString(), tsToUse.ToSortedString(), clientTs.ToSortedString())
	if tsToUse.IsEqual(copyClk) {
		//fmt.Println("[TM]TM's clock is higher than client, can return")
		return
	}
	//Have to wait
	//fmt.Println("[TM]Waiting for clock")
	req := TMWaitClock{targetClk: tsToUse, replyChan: make(chan clocksi.Timestamp, 1)}
	tm.commitChan <- req
	return <-req.replyChan
}

func (tm *TransactionManager) getReadLocation(bkt string) (isRemote bool, remoteIndex int) {
	if tm.hasAll || bkt == "" {
		//fmt.Println("[ReadLoc]Read for", bkt, "this replica has all or bkt is empty.")
		return false, 0
	}
	for _, localBkt := range tm.ownBuckets {
		if bkt == localBkt {
			//fmt.Println("[ReadLoc]Read for", bkt, "found local matching bucket", localBkt)
			return false, 0
		}
	}
	serverMap, has := tm.bucketToIndex[bkt]
	if !has {
		//Must use a server with *

		//fmt.Println("[ReadLoc]Read for", bkt, "unknown bucket, trying to match with '*'")
		serverMap = tm.bucketToIndex["*"]
	}

	//fmt.Println("[ReadLoc]Read for", bkt, "sending to first server of this bucket", serverMap)
	return true, serverMap[0]
}

//Temporary method. This is used to avoid compile errors on unused variables
//This unused variables mark stuff that isn't being processed yet.
func ignore(any ...interface{}) {

}

func (tm *TransactionManager) applyRemoteClk(request *TMRemoteClk) {
	//Can only apply clock if there's no transaction on hold for this clock.
	if len(tm.downstreamQueue[request.ReplicaID]) == 0 {
		replyChan := make(chan bool, nGoRoutines)
		tm.mat.SendRequestToAllChannels(MaterializerRequest{MatRequestArgs: MatClkPosUpdArgs{ReplicaID: request.ReplicaID, StableTs: request.StableTs, ReplyChan: replyChan}})
		//Wait for all partitions to apply clock
		for i := uint64(0); i < nGoRoutines; i++ {
			<-replyChan
		}
		tm.localClock.Lock()
		tm.localClock.Timestamp = tm.localClock.Timestamp.UpdatePos(request.ReplicaID, request.StableTs)
		copyClk := tm.localClock.Timestamp.Copy()
		tm.localClock.Unlock()
		fmt.Println("[TM]Remote clk applied.")
		tm.checkPendingRemoteTxns(copyClk)
	} else {
		//Queue
		fmt.Println("[TM]Remote clk queued.")
		tm.downstreamQueue[request.ReplicaID] = append(tm.downstreamQueue[request.ReplicaID], request)
	}
}

func (tm *TransactionManager) applyRemoteTxnGroup(request *RemoteTxnGroup) {
	//I think I can use something similar to what's used for holding txns
	tm.localClock.Lock()
	isLowerOrEqual := request.getMinClk().IsLowerOrEqualExceptFor(tm.localClock.Timestamp, tm.replicaID, request.getReplicaID())
	tm.localClock.Unlock()
	if isLowerOrEqual {
		//Can apply
		split := tm.splitGroupByPartition(request.Txns)
		replyChans := make([]chan []*UpdateObjectParams, nGoRoutines)
		for i, txns := range split {
			replyChan := make(chan []*UpdateObjectParams, 1)
			replyChans[i] = replyChan
			tm.mat.SendRequestToChannel(MaterializerRequest{
				MatRequestArgs: MatRemoteGroupTxnArgs{Txns: txns, FinalClk: request.getMaxClk(), ReplyChan: replyChan},
			}, uint64(i))
		}
		tm.processMatRemoteReply(replyChans, request.getReplicaID(), request.getMaxClk())
	} else {
		//Queue
		//fmt.Println("[TM]Remote txn group in queue.")
		tm.downstreamQueue[request.getReplicaID()] = append(tm.downstreamQueue[request.getReplicaID()], request)
	}
}

func (tm *TransactionManager) splitGroupByPartition(toSplit []RemoteTxn) (split [][]MatRemoteTxn) {
	replicaID, pos := toSplit[0].getReplicaID(), make([]int, nGoRoutines)
	split = make([][]MatRemoteTxn, nGoRoutines)
	for i := 0; i < int(nGoRoutines); i++ {
		split[i] = make([]MatRemoteTxn, len(toSplit))
	}
	for _, txn := range toSplit {
		for partID, partUpds := range txn.Upds {
			split[partID][pos[partID]] = MatRemoteTxn{ReplicaID: replicaID, Timestamp: txn.Clk, Upds: partUpds}
			pos[partID]++
		}
	}
	for i, posValue := range pos {
		split[i] = split[i][:posValue]
	}
	return
}

func (tm *TransactionManager) applyRemoteTxn(request *RemoteTxn) {
	//May have to put the transaction on hold. An hold only for remote transactions.
	tm.localClock.Lock()
	isLowerOrEqual := request.Clk.IsLowerOrEqualExceptFor(tm.localClock.Timestamp, tm.replicaID, request.getReplicaID())
	tm.localClock.Unlock()
	if isLowerOrEqual {
		//fmt.Println("[TM]Starting to apply remote txn")
		//Can apply
		//In theory, doesn't need to update the clock as that will happen when remoteClk gets applied.
		//In practice, I think I want to only send that once after all transactions are done.
		replyChans := make([]chan []*UpdateObjectParams, nGoRoutines)
		for i, upds := range request.Upds {
			replyChan := make(chan []*UpdateObjectParams, 1)
			replyChans[i] = replyChan
			tm.mat.SendRequestToChannel(MaterializerRequest{
				MatRequestArgs: MatRemoteTxnArgs{MatRemoteTxn: tm.makeMatRemoteTxn(request.getReplicaID(), request.Clk, upds), ReplyChan: replyChan},
			}, uint64(i))
		}
		tm.processMatRemoteReply(replyChans, request.getReplicaID(), request.Clk)
	} else {
		//Queue
		//Good thing is, for each ID, we will receive the transactions in order.
		//fmt.Println("[TM]Remote txn in queue")
		tm.downstreamQueue[request.getReplicaID()] = append(tm.downstreamQueue[request.getReplicaID()], request)
	}
}

func (tm *TransactionManager) processMatRemoteReply(replyChans []chan []*UpdateObjectParams, replicaID int16, clk clocksi.Timestamp) {
	newDowns := make(map[uint64][]*UpdateObjectParams) //int: partitionID
	hasNewDowns := false
	//Receive replies; check if there's any new downstream.
	for i, channel := range replyChans {
		if channel != nil {
			reply := <-channel
			if len(reply) > 0 {
				hasNewDowns = true
				newDowns[uint64(i)] = reply
			}
		}
	}
	tm.localClock.Lock()
	tm.localClock.Timestamp.UpdatePos(replicaID, clk.GetPos(replicaID))
	copyClk := tm.localClock.Timestamp.Copy()
	tm.localClock.Unlock()
	if hasNewDowns {
		tm.downstreamOpsCh <- TMTxnForRemote{ops: newDowns}
	}
	tm.checkPendingRemoteTxns(copyClk)
}

//TODO: When this gets called, maybe I can use a clock that was already read and avoid a lock.
func (tm *TransactionManager) checkPendingRemoteTxns(copyClk clocksi.Timestamp) {
	//Idea (I think somewhat similar to the previous one): go through requests until nothing can be applied
	//Steps:
	//Repeats
	//2 - Search if any ID can be applied. If it can, queue everything of that ID that can be applied. Update the clock.
	//3 - Keep doing the search, until a full cycle is done without any findings.
	//End of repeats
	//4 - Execute everything. At the end, send a clock update to every partition
	//5 - Wait for all partitions to finish commiting
	//6 - Update the clock.
	//The idea is that I can send a big request to the materializer and avoid a lot of the overhead.
	//This is "cheap" to do as this is a separate thread that is doing all the work gathering, so does not affect ongoing transactions.

	//fmt.Println("[TM]Pending check")
	//The structure to store can be something like... per partition? I still need to have txns separate for VM purposes.
	reqsPerPart := make([][]MatRemoteTxn, nGoRoutines)
	replyChans := make([]chan []*UpdateObjectParams, nGoRoutines)
	for i := range reqsPerPart {
		reqsPerPart[i] = make([]MatRemoteTxn, 0, 10)
		replyChans[i] = make(chan []*UpdateObjectParams, 1)
	}
	newDowns := make(map[uint64][]*UpdateObjectParams) //int: partitionID

	atLeastOne := false    //Keeps track if there's at least one txn or clock to apply
	foundSomething := true //For as long as one transaction of any replica is found to be appliable, the external cycle can continue
	posToHide := 0         //Auxiliary variable that states until which point requests were processed for a given remoteID.

	//Gather list of txns that can be applied
	for foundSomething {
		foundSomething = false
		for remoteID, msgs := range tm.downstreamQueue {
			posToHide = 0
			for _, req := range msgs {
				switch typedReq := req.(type) {
				case RemoteTxn:
					if typedReq.Clk.IsLowerOrEqualExceptFor(copyClk, tm.replicaID, typedReq.getReplicaID()) {
						//Safe to commit. Add to list. Update copyClk
						copyClk.UpdatePos(typedReq.getReplicaID(), typedReq.Clk.GetPos(typedReq.getReplicaID()))
						for i, upds := range typedReq.Upds {
							reqsPerPart[i] = append(reqsPerPart[i], tm.makeMatRemoteTxn(remoteID, typedReq.Clk, upds))
						}
						foundSomething, atLeastOne = true, true
						posToHide++
					} else {
						//Need to go to next replica.
						break
					}
				case RemoteTxnGroup:
					if typedReq.getMaxClk().IsLowerOrEqualExceptFor(copyClk, tm.replicaID, typedReq.getReplicaID()) {
						//Safe to commit. Add to list. Update copyClk
						copyClk.UpdatePos(typedReq.getReplicaID(), typedReq.getMaxClk().GetPos(typedReq.getReplicaID()))
						for _, txn := range typedReq.Txns {
							for i, upds := range txn.Upds {
								reqsPerPart[i] = append(reqsPerPart[i], tm.makeMatRemoteTxn(remoteID, txn.Clk, upds))
							}
						}
						foundSomething, atLeastOne = true, true
						posToHide++
					} else {
						//Need to go to next replica.
						break
					}
				case TMRemoteClk:
					copyClk.UpdatePos(typedReq.ReplicaID, typedReq.StableTs)
					foundSomething, atLeastOne = true, true
					posToHide++
				}
			}
			for i := 0; i < posToHide; i++ {
				//For GC purposes
				msgs[i] = nil
			}
			tm.downstreamQueue[remoteID] = msgs[posToHide:]
		}
	}
	//fmt.Println("[TM]Pending check end")

	if !atLeastOne {
		//Nothing to apply, can return
		return
	}
	//To every partition, send a "big" request with all the transactions that were on hold + clock update.
	for i, reqs := range reqsPerPart {
		tm.mat.SendRequestToChannel(MaterializerRequest{MatRequestArgs: MatRemoteGroupTxnArgs{
			Txns:      reqs,
			FinalClk:  copyClk,
			ReplyChan: replyChans[i],
		}}, uint64(i))
	}

	hasNewDowns := false
	//Wait for replies and update the clock here.
	for i, replyChan := range replyChans {
		reply := <-replyChan
		if len(reply) > 0 {
			hasNewDowns = true
			newDowns[uint64(i)] = reply
		}
	}
	tm.localClock.Lock()
	tm.localClock.Timestamp = tm.localClock.Merge(copyClk)
	tm.localClock.Unlock()
	if hasNewDowns {
		tm.downstreamOpsCh <- TMTxnForRemote{ops: newDowns} //Sending all grouped
	}
	//TODO: Forced GC for when downstreamQueue grows too big in capacity? Like make new slices.
}

func (tm *TransactionManager) makeMatRemoteTxn(id int16, clk clocksi.Timestamp, upds []*UpdateObjectParams) MatRemoteTxn {
	return MatRemoteTxn{ReplicaID: id, Timestamp: clk, Upds: upds}
}

func (tm *TransactionManager) handleTMGetSnapshot(snapshot *TMGetSnapshot) {
	buckets, replChan := snapshot.Buckets, snapshot.ReplyChan

	tm.localClock.Lock()
	tsToUse := tm.localClock.Timestamp
	tm.localClock.Unlock()
	nParts := len(tm.mat.channels)

	//Ask to read snapshots based on the localClock.
	replyChans := make([]chan []*proto.ProtoCRDT, nParts)
	for i := range replyChans {
		channel := make(chan []*proto.ProtoCRDT)
		replyChans[i] = channel
		tm.mat.SendRequestToChannel(MaterializerRequest{MatRequestArgs: MatGetSnapshotArgs{
			Timestamp: tsToUse, Buckets: buckets, ReplyChan: channel}}, uint64(i))
	}
	partStates := make([][]*proto.ProtoCRDT, nParts)
	for i, channel := range replyChans {
		partStates[i] = <-channel
		close(channel)
	}

	replChan <- TMGetSnapshotReply{Timestamp: tsToUse, PartStates: partStates}
}

func (tm *TransactionManager) handleTMApplySnapshot(snapshot *TMApplySnapshot) {
	ts, states := snapshot.Timestamp, snapshot.PartStates
	tm.localClock.Lock()
	ts.UpdatePos(tm.replicaID, tm.localClock.GetPos(tm.replicaID))
	tm.localClock.Unlock()

	for i, partState := range states {
		tm.mat.SendRequestToChannel(MaterializerRequest{MatRequestArgs: MatApplySnapshotArgs{
			Timestamp: ts, ProtoCRDTs: partState}}, uint64(i))
	}

	//TODO: Need to update remote entries at the end of this... or right at start?
	tm.localClock.Lock()
	tm.localClock.Timestamp = tm.localClock.Merge(ts)
	tm.localClock.Unlock()
}

func (tm *TransactionManager) handleReplicaID(replica *TMReplicaID) {
	remoteID := replica.ReplicaID
	fmt.Println("Adding ID", remoteID)
	clocksi.AddNewID(remoteID)
	tm.RemoteInfo.remoteBks = append(tm.RemoteInfo.remoteBks, replica.Buckets)
	tm.RemoteInfo.remoteIPs = append(tm.RemoteInfo.remoteIPs, replica.IP)
}

func (tm *TransactionManager) handleRemoteTrigger(trigger *TMRemoteTrigger) {
	tm.triggerDB.Lock()
	fmt.Printf("[TM]Handling remote trigger: %v\n", *trigger)
	if trigger.IsGeneric {
		src := trigger.Trigger
		matchable := tm.triggerDB.GetMatchableKeyParams(src.Key, src.Bucket, src.CrdtType)
		tm.triggerDB.AddGenericLink(matchable, src, trigger.Target)
	} else {
		tm.triggerDB.AddLink(trigger.AutoUpdate.Trigger, trigger.AutoUpdate.Target)
	}
	tm.triggerDB.Unlock()
}

//This code is run before the server starts accepting client requests, so it doesn't need to be efficient.
func (tm *TransactionManager) handleTMStart(start *TMStart) {
	tm.localClock.Timestamp = clocksi.NewClockSiTimestamp()
	tm.mat.SendRequestToAllChannels(MaterializerRequest{MatRequestArgs: MatWaitForReplicasArgs{}})
	for i, buckets := range tm.remoteBks {
		for _, bkt := range buckets {
			tm.bucketToIndex[bkt] = append(tm.bucketToIndex[bkt], i)
		}
	}
	tm.remoteBks = nil
	MAX_POOL_PER_SERVER = tools.SharedConfig.GetIntConfig("poolMax", 100)
	pool := initializeConnPool(tm.remoteIPs)
	tm.connPool = pool
	tm.waitStartChan <- true
}

func (tm *TransactionManager) handleDownstreamGeneratedOps() {
	for {
		req := <-tm.downstreamOpsCh
		newTxnId := TransactionId(rand.Uint64())
		replyChan := make(chan clocksi.Timestamp, len(req.ops))

		//Send prepare
		for partId, partUpds := range req.ops {
			tm.mat.SendRequestToChannel(MaterializerRequest{
				MatRequestArgs: MatPrepareForRemoteArgs{TransactionId: newTxnId, Updates: partUpds, ReplyChan: replyChan},
			}, partId)
		}

		var maxTimestamp clocksi.Timestamp = clocksi.DummyTs
		//Wait for reply of each partition
		for i := 0; i < len(req.ops); i++ {
			replyTs := <-replyChan
			if replyTs.IsHigherOrEqual(maxTimestamp) {
				maxTimestamp = replyTs
			}
		}

		//Notify TM's handleCommitReplies() of the number of partitions for this txn.
		tm.commitChan <- TMCommitNPartitions{nPartitions: len(req.ops), txnId: newTxnId, clk: maxTimestamp}

		//Send commit to involved partitions
		commitReq := MaterializerRequest{MatRequestArgs: MatCommitArgs{TransactionId: newTxnId, CommitTimestamp: maxTimestamp}}
		for partId := range req.ops {
			tm.mat.SendRequestToChannel(commitReq, partId)
		}
		//The partitions will request the clock to be updated.
		fmt.Println("[TM][downGen]Commited txn with NuCRDTs for other replicas with clk", maxTimestamp.ToSortedString())
	}
}

func (tm *TransactionManager) doHistoryCompact() {
	oldestRead := tm.clocksArray.ReadFirst()
	if oldestRead == nil {
		//TODO: Clean everything until last commited clock
	} else {
		//oldestTxnClk := oldestRead.(clocksi.Timestamp)
		//TODO: Send requests to clean history
	}
}

//Remote handling functions

//Only for non-static transactions
func (tm *TransactionManager) startTxnForRemote(txnPartitions *ongoingTxn, toContact []bool) {
	indexesToWait := make([]int, 0, len(toContact))

	for i, hasTo := range toContact {
		if hasTo && txnPartitions.txnDataToUse[i] == nil {
			/*
					conn, err := net.Dial("tcp", tm.remoteIPs[i])
					tools.CheckErr("Network connection establishment err on remote read", err)
					txnPartitions.conns[i] = conn
				SendProto(StartTrans, CreateStartTransaction(txnPartitions.originalClk.ToBytes()), conn)
			*/
			//SendProto(StartTrans, CreateStartTransaction(txnPartitions.originalClk.ToBytes()), txnPartitions.conns[i])
			//txnPartitions.replyChans[i], txnPartitions.lockChans[i] = tm.connPool.sendAndLockRequest(StartTrans, CreateStartTransaction(txnPartitions.originalClk.ToBytes()), i)
			//txnPartitions.replyChans[i] = tm.connPool.sendAndLockRequest(StartTrans, CreateStartTransaction(txnPartitions.originalClk.ToBytes()), i)
			txnPartitions.replyChans[i] = tm.connPool.sendAndLockRequest(S2S, CreateS2SWrapperProto(int32(txnPartitions.TransactionId),
				proto.WrapperType_START_TXN, CreateStartTransaction(txnPartitions.originalClk.ToBytes())), i)
			indexesToWait = append(indexesToWait, i)
			//txnPartitions.nConns++
			txnPartitions.nTxnsStarted++
		}
	}

	for _, index := range indexesToWait {
		reply := <-txnPartitions.replyChans[index]
		txnPartitions.lockChans[index] = reply.lockChan
		//replyProto := reply.msg.(*proto.S2SWrapperReply).StartTxn
		//_, replyProto, _ := ReceiveProto(txnPartitions.conns[index])
		//txnPartitions.txnDataToUse[index] = replyProto.(*proto.ApbStartTransactionResp).GetTransactionDescriptor()
		txnPartitions.txnDataToUse[index] = reply.msg.StartTxn.GetTransactionDescriptor()
	}

}

//Note: Done by a separate goroutine
func (tm *TransactionManager) handleRemoteReads(txnPartitions *ongoingTxn, reqsPerServer [][]ReadObjectParams,
	remoteReqsToChan [][]int, readChans []chan crdt.State) {

	//if txnPartitions.nConns < len(reqsPerServer) {
	if txnPartitions.nTxnsStarted < len(reqsPerServer) {
		toContact, has := make([]bool, len(reqsPerServer)), false
		for i, reqs := range reqsPerServer {
			if len(reqs) > 0 {
				toContact[i], has = true, true
			}
		}
		if has {
			tm.startTxnForRemote(txnPartitions, toContact)
		}
	}

	for i, reqs := range reqsPerServer {
		if len(reqs) > 0 {
			//SendProto(ReadObjs, CreateReadObjs(txnPartitions.txnDataToUse[i], reqs), txnPartitions.conns[i])
			//txnPartitions.lockChans[i] <- msgToSend{code: ReadObjs, needsLock: true, msg: CreateReadObjs(txnPartitions.txnDataToUse[i], reqs), replyChan: txnPartitions.replyChans[i]}
			txnPartitions.lockChans[i] <- msgToSend{code: S2S, needsLock: true, msg: CreateS2SWrapperProto(int32(txnPartitions.TransactionId),
				proto.WrapperType_READ, CreateRead(txnPartitions.txnDataToUse[i], nil, reqs)), replyChan: txnPartitions.replyChans[i]}
		}
	}

	var readParams ReadObjectParams
	var currReqs []ReadObjectParams
	var currIndexes []int
	//Receiving replies and redirecting to state
	//for i, conn := range txnPartitions.conns {
	for i, replyChan := range txnPartitions.replyChans {
		currReqs, currIndexes = reqsPerServer[i], remoteReqsToChan[i]
		if len(currReqs) > 0 {
			//_, protobuf, _ := ReceiveProto(conn)
			//readReply := protobuf.(*proto.ApbReadObjectsResp).GetObjects()
			reply := <-replyChan
			//readReply := reply.msg.(*proto.ApbReadObjectsResp).GetObjects()
			readReply := reply.msg.ReadObjs.GetObjects()
			for j, obj := range readReply {
				readParams = currReqs[j]
				readChans[currIndexes[j]] <- crdt.ReadRespProtoToAntidoteState(obj, readParams.CrdtType, readParams.ReadArgs.GetREADType())
			}
		}
	}
}

func (tm *TransactionManager) handleRemoteStaticReads(txnID TransactionId, ts clocksi.Timestamp, reqsPerServer [][]ReadObjectParams,
	remoteReqsToChan [][]int, readChans []chan crdt.State) {

	//conns := make([]net.Conn, len(reqsPerServer))
	poolChans := make([]chan msgReply, len(reqsPerServer))
	//Sending reqs
	for i, reqs := range reqsPerServer {
		if len(reqs) > 0 {
			/*
				conn, err := net.Dial("tcp", tm.remoteIPs[i])
				tools.CheckErr("Network connection establishment err on remote read", err)
				conns[i] = conn
				SendProto(StaticReadObjs, CreateStaticReadObjs(ts.ToBytes(), reqs), conn)
			*/
			//SendProto(StaticReadObjs, CreateStaticReadObjs(ts.ToBytes(), reqs), ongoingRemote.conns[i])
			poolChans[i] = tm.connPool.sendRequest(S2S, CreateS2SWrapperProto(int32(txnID),
				proto.WrapperType_STATIC_READ, CreateStaticRead(ts.ToBytes(), nil, reqs)), i)
		}
	}

	var readParams ReadObjectParams
	var currReqs []ReadObjectParams
	var currIndexes []int
	//Receiving replies and redirecting to state
	//for i, conn := range conns {
	for i, poolChan := range poolChans {
		currReqs, currIndexes = reqsPerServer[i], remoteReqsToChan[i]
		if len(currReqs) > 0 {
			//fmt.Println("[TM][StaticReadRemote]Waiting on channel with ID", int32(txnID))
			msgReply := <-poolChan
			//fmt.Println("[TM][StaticReadRemote]Got reply from channel with ID", int32(txnID))
			//_, protobuf, _ := ReceiveProto(conn)
			protobuf := msgReply.msg
			//readReply := protobuf.(*proto.ApbStaticReadObjectsResp).GetObjects().GetObjects()
			readReply := protobuf.StaticReadObjs.GetObjects().GetObjects()
			for j, obj := range readReply {
				readParams = currReqs[j]
				readChans[currIndexes[j]] <- crdt.ReadRespProtoToAntidoteState(obj, readParams.CrdtType, readParams.ReadArgs.GetREADType())
			}
			//conns[i].Close()
		}
	}
	//fmt.Println("[TM][StaticReadRemote]Got reply from all channels")
}

func (tm *TransactionManager) handleRemoteUpds(txnPartitions *ongoingTxn, reqsPerServer [][]UpdateObjectParams) {

	//if txnPartitions.nConns < len(reqsPerServer) {
	if txnPartitions.nTxnsStarted < len(reqsPerServer) {
		toContact, has := make([]bool, len(reqsPerServer)), false
		for i, reqs := range reqsPerServer {
			if len(reqs) > 0 {
				toContact[i], has = true, true
			}
		}
		if has {
			tm.startTxnForRemote(txnPartitions, toContact)
		}
	}

	//Sending reqs
	for i, reqs := range reqsPerServer {
		if len(reqs) > 0 {
			//SendProto(UpdateObjs, CreateUpdateObjs(txnPartitions.txnDataToUse[i], reqs), txnPartitions.conns[i])
			//txnPartitions.lockChans[i] <- msgToSend{code: UpdateObjs, needsLock: true, msg: CreateUpdateObjs(txnPartitions.txnDataToUse[i], reqs), replyChan: txnPartitions.replyChans[i]}
			txnPartitions.lockChans[i] <- msgToSend{code: S2S, needsLock: true, msg: CreateS2SWrapperProto(int32(txnPartitions.TransactionId),
				proto.WrapperType_UPD, CreateUpdateObjs(txnPartitions.txnDataToUse[i], reqs)), replyChan: txnPartitions.replyChans[i]}
		}
	}

	var currReqs []UpdateObjectParams
	//for i, conn := range txnPartitions.conns {
	for i, replyChan := range txnPartitions.replyChans {
		currReqs = reqsPerServer[i]
		if len(currReqs) > 0 {
			//ReceiveProto(conn) //Waits until the other server acks the write
			<-replyChan
		}
	}
}

//Note: done by a separate goroutine
func (tm *TransactionManager) handleRemoteCommit(txnPartitions *ongoingTxn, remoteChan chan bool) {
	//for i, conn := range txnPartitions.conns {
	for i, lockChan := range txnPartitions.lockChans {
		//if conn != nil {
		if txnPartitions.txnDataToUse[i] != nil {
			//SendProto(CommitTrans, CreateCommitTransaction(txnPartitions.txnDataToUse[i]), conn)
			//lockChan <- msgToSend{code: CommitTrans, needsLock: false, msg: CreateCommitTransaction(txnPartitions.txnDataToUse[i]), replyChan: txnPartitions.replyChans[i]}
			lockChan <- msgToSend{code: S2S, needsLock: false, msg: CreateS2SWrapperProto(int32(txnPartitions.TransactionId),
				proto.WrapperType_COMMIT, CreateCommitTransaction(txnPartitions.txnDataToUse[i])), replyChan: txnPartitions.replyChans[i]}
			//close(lockChan)
		}
	}

	//for i, conn := range txnPartitions.conns {
	for i, replyChan := range txnPartitions.replyChans {
		//if conn != nil {
		if txnPartitions.txnDataToUse[i] != nil {
			//ReceiveProto(conn) //Waits until the other server acks the commit
			//conn.Close()
			<-replyChan
		}
	}

	remoteChan <- true
}

func (tm *TransactionManager) handleRemoteAbort(txnPartitions *ongoingTxn, remoteChan chan bool) {
	//for i, conn := range txnPartitions.conns {
	for i, lockChan := range txnPartitions.lockChans {
		//if conn != nil {
		if txnPartitions.txnDataToUse[i] != nil {
			//SendProto(AbortTrans, CreateAbortTransaction(txnPartitions.txnDataToUse[i]), conn)
			//lockChan <- msgToSend{code: AbortTrans, needsLock: false, msg: CreateAbortTransaction(txnPartitions.txnDataToUse[i]), replyChan: txnPartitions.replyChans[i]}
			lockChan <- msgToSend{code: S2S, needsLock: false, msg: CreateS2SWrapperProto(int32(txnPartitions.TransactionId),
				proto.WrapperType_ABORT, CreateAbortTransaction(txnPartitions.txnDataToUse[i])), replyChan: txnPartitions.replyChans[i]}
			//close(lockChan)
		}
	}

	//for i, conn := range txnPartitions.conns {
	for i, replyChan := range txnPartitions.replyChans {
		//if conn != nil {
		if txnPartitions.txnDataToUse[i] != nil {
			//ReceiveProto(conn) //Waits until the other server acks the abort
			//conn.Close()
			<-replyChan
		}
	}
	remoteChan <- true
}

func (tm *TransactionManager) handleRemoteStaticUpds(txnID TransactionId, ts clocksi.Timestamp, reqsPerServer [][]UpdateObjectParams) {
	//conns := make([]net.Conn, len(reqsPerServer))
	poolChans := make([]chan msgReply, len(reqsPerServer))
	//Sending reqs
	for i, reqs := range reqsPerServer {
		if len(reqs) > 0 {
			/*
				conn, err := net.Dial("tcp", tm.remoteIPs[i])
				tools.CheckErr("Network connection establishment err on remote read", err)
				conns[i] = conn
				SendProto(StaticUpdateObjs, CreateStaticUpdateObjs(ts.ToBytes(), reqs), conn)
			*/
			//SendProto(StaticUpdateObjs, CreateStaticUpdateObjs(ts.ToBytes(), reqs), ongoingRemote.conns[i])
			poolChans[i] = tm.connPool.sendRequest(S2S, CreateS2SWrapperProto(int32(txnID),
				proto.WrapperType_STATIC_UPDATE, CreateStaticUpdateObjs(ts.ToBytes(), reqs)), i)
		}
	}

	var currReqs []UpdateObjectParams
	//for i, conn := range conns {
	//for i, conn := range ongoingRemote.conns {
	for i, poolChan := range poolChans {
		currReqs = reqsPerServer[i]
		if len(currReqs) > 0 {
			<-poolChan //Waits until the other server acks the write
			//ReceiveProto(conn) //Waits until the other server acks the write
			//conns[i].Close()
		}
	}

}

type TMCommitInfo interface{}

//Sent by each partition of the materializer
type TMPartCommitReply struct {
	txnId TransactionId
}

//Sent by the goroutine who asked for the commit
type TMCommitNPartitions struct {
	nPartitions int
	txnId       TransactionId
	clk         clocksi.Timestamp
}

//Message sent by StaticRead() and StartTransaction() when the client's clock is too new.
type TMWaitClock struct {
	targetClk clocksi.Timestamp
	replyChan chan clocksi.Timestamp //Replies with the actual clock of TM
}

//Used to get a copy of the TM's current clock
type TMGetClock struct {
	replyChan chan clocksi.Timestamp
}

//This will also need to handle requests for transactions on hold. Can use a sorted heap for this.
func (tm *TransactionManager) handleCommitReplies() {
	txnToClk := make(map[TransactionId]clocksi.Timestamp)
	txnWaitFor := make(map[TransactionId]*int)
	waitingTMs := ClockHeap{entries: make([]TMWaitClock, 100), nEntries: new(int)} //TODO: Put this as a variable somewhere?
	var count *int
	var clk clocksi.Timestamp
	for {
		switch typedReq := (<-tm.commitChan).(type) {
		case TMCommitNPartitions:
			txnToClk[typedReq.txnId], txnWaitFor[typedReq.txnId] = typedReq.clk, &typedReq.nPartitions
		case TMPartCommitReply:
			count = txnWaitFor[typedReq.txnId]
			*count--
			if *count == 0 {
				clk = txnToClk[typedReq.txnId]
				delete(txnToClk, typedReq.txnId)
				delete(txnWaitFor, typedReq.txnId)
				tm.localClock.Lock()
				tm.localClock.Timestamp = tm.localClock.Merge(clk)
				copyClk := tm.localClock.Timestamp.Copy()
				tm.txnsSinceCompact++
				tm.localClock.Unlock()
				//Notify all
				for waitingTMs.Len() > 0 && waitingTMs.PeekMin().targetClk.IsLowerOrEqual(copyClk) {
					heap.Pop(waitingTMs).(TMWaitClock).replyChan <- copyClk
				}
				//fmt.Println("[TM]Clock updated.")
			}
		case TMWaitClock:
			heap.Push(waitingTMs, typedReq)
		}
	}
}

type ClockHeap struct {
	entries  []TMWaitClock
	nEntries *int
}

func (c ClockHeap) Len() int { return *c.nEntries }

//We want the heap's Pop() to return the lowest element, so we use < on "less". The smallest element is on h[0].
func (c ClockHeap) Less(i, j int) bool {
	return c.entries[i].targetClk.IsLowerOrEqualTotalOrder(c.entries[i].targetClk)
}

func (c ClockHeap) Swap(i, j int) {
	c.entries[i], c.entries[j] = c.entries[j], c.entries[i]
}

func (c ClockHeap) Push(value interface{}) {
	convValue := value.(TMWaitClock)
	if *c.nEntries == cap(c.entries) {
		c.entries = append(c.entries, convValue)
		c.entries = c.entries[:cap(c.entries)] //Extending to capacity
		*c.nEntries += 1
	} else {
		c.entries[*c.nEntries], *c.nEntries = convValue, *c.nEntries+1
	}

}

func (c ClockHeap) Pop() interface{} {
	if *c.nEntries == 0 {
		return nil
	}
	old := c.entries[*c.nEntries-1]
	c.entries[*c.nEntries-1] = TMWaitClock{}
	*c.nEntries--
	return old
}

//Returns the lowest value, but does not remove it.
func (c ClockHeap) PeekMin() TMWaitClock {
	if *c.nEntries == 0 {
		return TMWaitClock{}
	}
	return c.entries[0]
}

//Debug

func (tm *TransactionManager) sanityCheck() {
	for {
		time.Sleep(50 * time.Second)
		fmt.Println("[TM][SC]Clock: ", tm.localClock.Timestamp.ToSortedString())
		for id, msgs := range tm.downstreamQueue {
			if len(msgs) > 0 {
				fmt.Println("[TM][SC]There's still leftover msgs in downstreamQueue! ID:", id, msgs)
			}
		}
	}
}
