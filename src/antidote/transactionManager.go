package antidote

//TODO: Try refactoring this to be inside another folder in antidote
//(e.g: antidote/transaction), and check if that still has circular import issues

import (
	"clocksi"
	"crdt"
	fmt "fmt"
	"math/rand"
	"proto"
	"sync"
	"tools"
)

/////*****************TYPE DEFINITIONS***********************/////
//TODO: Extract requests types, replies and methods to another file

type KeyParams struct {
	Key      string
	CrdtType proto.CRDTType
	Bucket   string
}

type UpdateObjectParams struct {
	KeyParams
	UpdateArgs *crdt.UpdateArguments
}

type ReadObjectParams struct {
	KeyParams
	ReadArgs crdt.ReadArguments
}

type TransactionManagerRequest struct {
	TransactionId //TODO: Remove this, as most requests don't need it (iirc, only staticWrite, commit and abort use it)
	Timestamp     clocksi.Timestamp
	Args          TMRequestArgs
}

type TMRequestArgs interface {
	getRequestType() (requestType TMRequestType)
}

type TMReadArgs struct {
	ReadParams []ReadObjectParams
	ReplyChan  chan []crdt.State
}

type TMUpdateArgs struct {
	UpdateParams []*UpdateObjectParams
	ReplyChan    chan TMUpdateReply
}

type TMStaticUpdateArgs struct {
	UpdateParams []*UpdateObjectParams
	ReplyChan    chan TMStaticUpdateReply
}

type TMStaticReadArgs struct {
	ReadParams []ReadObjectParams
	ReplyChan  chan TMStaticReadReply
}

type TMConnLostArgs struct {
}

type TMStartTxnArgs struct {
	ReplyChan chan TMStartTxnReply
}

type TMCommitArgs struct {
	ReplyChan chan TMCommitReply
}

type TMAbortArgs struct {
}

/*****Remote/Replicator interaction structs*****/

//Used by the Replication Layer. Use a different thread to handle this
/*
type TMRemoteTxn struct {
	ReplicaID int16
	Upds      []NewRemoteTxns
	StableTs  int64
}
*/

type TMRemoteMsg interface {
	getReplicaID() int16
}

type TMRemoteClk struct {
	ReplicaID int16
	StableTs  int64
}

type TMRemotePartTxn struct {
	PartitionID int64
	ReplicaID   int16
	clocksi.Timestamp
	Upds []*UpdateObjectParams
}

type TMGetSnapshot struct {
	Buckets   map[string]struct{}
	ReplyChan chan TMGetSnapshotReply
}

type TMApplySnapshot struct {
	clocksi.Timestamp
	PartStates [][]*proto.ProtoCRDT
}

type TMStart struct{}

/*****Msgs for handling ops generated by downstream remotes*****/

type TMDownstreamRemoteMsg interface {
}

type TMNewRemoteTxn struct {
	clocksi.Timestamp
	nPartitions int
}

type TMDownstreamNewOps struct {
	clocksi.Timestamp
	partitionID int64
	newOps      []*UpdateObjectParams
}

/***** *****/

type TMStaticReadReply struct {
	States    []crdt.State
	Timestamp clocksi.Timestamp
}

type TMStaticUpdateReply struct {
	TransactionId
	Timestamp clocksi.Timestamp
	Err       error
}

type TMUpdateReply struct {
	Success bool
	Err     error
}

type TMStartTxnReply struct {
	TransactionId
	Timestamp clocksi.Timestamp
}

type TMCommitReply struct {
	Timestamp clocksi.Timestamp
	Err       error
}

type TMGetSnapshotReply struct {
	Timestamp  clocksi.Timestamp
	PartStates [][]*proto.ProtoCRDT
}

type TMRequestType int

type ClientId uint64

/*
type TransactionId struct {
	ClientId  ClientId
	Timestamp clocksi.Timestamp
}
*/
type TransactionId uint64

type ongoingTxn struct {
	TransactionId
	partSet
	debugID int //random ID just for debbuging purposes
}

type partSet map[uint64]struct{}

type ProtectedClock struct {
	clocksi.Timestamp
	sync.Mutex
}

type TransactionManager struct {
	mat        *Materializer
	remoteChan chan TMRemoteMsg
	//TODO: Currently downstreaming txns holds the lock until the whole process is done (i.e., all transactions are either downstreamed to Materializer or queued). This can be a problem if we need to wait for Materializer - use bounded chans?
	localClock              ProtectedClock
	downstreamQueue         map[int16][]TMRemoteMsg
	replicator              *Replicator
	replicaID               int16
	downstreamOpsCh         chan TMDownstreamRemoteMsg  //Channel for handling ops that are generated when applying remote downstreams.
	nPartitionsForRemoteTxn map[int16]*int              //Number of partitions that are involved in the current remote txn for a given replicaID.
	clockOfRemoteTxn        map[int16]clocksi.Timestamp //Stores the timestamps for the remote txns referred in the map above
	waitStartChan           chan bool                   //Channel for notifying ProtoServer when is TM ready to start processing requests
}

/////*****************CONSTANTS AND VARIABLES***********************/////

const (
	readStaticTMRequest   TMRequestType = 0
	updateStaticTMRequest TMRequestType = 1
	readTMRequest         TMRequestType = 2
	updateTMRequest       TMRequestType = 3
	startTxnTMRequest     TMRequestType = 4
	commitTMRequest       TMRequestType = 5
	abortTMRequest        TMRequestType = 6
	lostConnRequest       TMRequestType = 255

	downstreamOpsChBufferSize int = 100 //Default buffer size for the downstreamOpsCh
)

/////*****************TYPE METHODS***********************/////

//TransactionManagerRequest

func (args TMStaticReadArgs) getRequestType() (requestType TMRequestType) {
	return readStaticTMRequest
}

func (args TMStaticUpdateArgs) getRequestType() (requestType TMRequestType) {
	return updateStaticTMRequest
}

func (args TMReadArgs) getRequestType() (requestType TMRequestType) {
	return readTMRequest
}

func (args TMUpdateArgs) getRequestType() (requestType TMRequestType) {
	return updateTMRequest
}

func (args TMConnLostArgs) getRequestType() (requestType TMRequestType) {
	return lostConnRequest
}

func (args TMStartTxnArgs) getRequestType() (requestType TMRequestType) {
	return startTxnTMRequest
}

func (args TMCommitArgs) getRequestType() (requestType TMRequestType) {
	return commitTMRequest
}

func (args TMAbortArgs) getRequestType() (requestType TMRequestType) {
	return abortTMRequest
}

//TMRemoteMsg

func (req TMRemoteClk) getReplicaID() (id int16) {
	return req.ReplicaID
}

func (req TMRemotePartTxn) getReplicaID() (id int16) {
	return req.ReplicaID
}

func (args TMGetSnapshot) getReplicaID() (id int16) {
	return 0 //Irrelevant
}

func (args TMApplySnapshot) getReplicaID() (id int16) {
	return 0 //Irrelevant
}

func (args TMStart) getReplicaID() (id int16) {
	return 0 //Irrelevant
}

//Others

func makePartSet() (set partSet) {
	set = partSet(make(map[uint64]struct{}))
	return
}

func (set partSet) add(partId uint64) {
	set[partId] = struct{}{}
}

func (txnPartitions *ongoingTxn) reset() {
	txnPartitions.TransactionId = 0
	txnPartitions.partSet = nil
}

/////*****************TRANSACTION MANAGER CODE***********************/////

func Initialize(replicaID int16) (tm *TransactionManager) {
	downstreamOpsCh := make(chan TMDownstreamRemoteMsg, downstreamOpsChBufferSize)
	mat, loggers := InitializeMaterializer(replicaID, downstreamOpsCh)
	//mat, _, _ := InitializeMaterializer(replicaID)
	tm = &TransactionManager{
		mat:                     mat,
		remoteChan:              make(chan TMRemoteMsg),
		localClock:              ProtectedClock{Mutex: sync.Mutex{}, Timestamp: clocksi.NewClockSiTimestamp(replicaID)},
		downstreamQueue:         make(map[int16][]TMRemoteMsg),
		replicator:              &Replicator{},
		replicaID:               replicaID,
		downstreamOpsCh:         downstreamOpsCh,
		nPartitionsForRemoteTxn: make(map[int16]*int),
		clockOfRemoteTxn:        make(map[int16]clocksi.Timestamp),
		waitStartChan:           make(chan bool),
	}
	tm.replicator.Initialize(tm, loggers, replicaID)
	go tm.handleRemoteMsgs()
	go tm.handleDownstreamGeneratedOps()
	return tm
}

func CreateKeyParams(key string, crdtType proto.CRDTType, bucket string) (keyParams KeyParams) {
	return KeyParams{Key: key, CrdtType: crdtType, Bucket: bucket}
}

func (tm *TransactionManager) WaitUntilReady() {
	<-tm.waitStartChan
}

//Starts a goroutine to handle the client requests. Returns a channel to communicate with that goroutine
func (tm *TransactionManager) CreateClientHandler() (channel chan TransactionManagerRequest) {
	channel = make(chan TransactionManagerRequest)
	go tm.listenForProtobufRequests(channel)
	return
}

func (tm *TransactionManager) SendRemoteMsg(msg TMRemoteMsg) {
	tm.remoteChan <- msg
}

func (tm *TransactionManager) listenForProtobufRequests(channel chan TransactionManagerRequest) {
	stop := false
	var txnPartitions *ongoingTxn = &ongoingTxn{}
	txnPartitions.debugID = rand.Intn(10)
	for !stop {
		request := <-channel
		stop = tm.handleTMRequest(request, txnPartitions)
	}

	tools.FancyDebugPrint(tools.TM_PRINT, tm.replicaID, "connection lost, shutting down goroutine for client.")
}

func (tm *TransactionManager) handleTMRequest(request TransactionManagerRequest,
	txnPartitions *ongoingTxn) (shouldStop bool) {
	shouldStop = false

	switch request.Args.getRequestType() {
	case readStaticTMRequest:
		tm.handleStaticTMRead(request)
	case updateStaticTMRequest:
		tm.handleStaticTMUpdate(request)
	case readTMRequest:
		tm.handleTMRead(request)
	case updateTMRequest:
		tm.handleTMUpdate(request, txnPartitions)
	case startTxnTMRequest:
		tm.handleTMStartTxn(request, txnPartitions)
	case commitTMRequest:
		tm.handleTMCommit(request, txnPartitions)
	case abortTMRequest:
		tm.handleTMAbort(request, txnPartitions)
	case lostConnRequest:
		shouldStop = true
	}
	//remoteTxnRequest is handled separatelly

	return
}

func (tm *TransactionManager) handleRemoteMsgs() {
	for {
		request := <-tm.remoteChan
		switch typedReq := request.(type) {
		case TMRemoteClk:
			tm.applyRemoteClk(&typedReq)
		case TMRemotePartTxn:
			tm.applyRemoteTxn(&typedReq)
		case TMGetSnapshot:
			tm.handleTMGetSnapshot(&typedReq)
		case TMApplySnapshot:
			tm.handleTMApplySnapshot(&typedReq)
		case TMStart:
			tm.waitStartChan <- true
		}
	}
}

func (tm *TransactionManager) handleStaticTMRead(request TransactionManagerRequest) {
	readArgs := request.Args.(TMStaticReadArgs)
	//tsToUse := request.Timestamp
	tm.localClock.Lock()
	tsToUse := tm.localClock.Timestamp //TODO: Should we use here directly localClock or localClock.NextTimestamp()?
	tm.localClock.Unlock()

	var currReadChan chan crdt.State = nil
	var currRequest MaterializerRequest
	states := make([]crdt.State, len(readArgs.ReadParams))

	//Now, ask to read the client requested version.
	for i, currRead := range readArgs.ReadParams {
		currReadChan = make(chan crdt.State)

		currRequest = MaterializerRequest{
			MatRequestArgs: MatStaticReadArgs{MatReadCommonArgs: MatReadCommonArgs{
				Timestamp:        tsToUse,
				ReadObjectParams: currRead,
				ReplyChan:        currReadChan,
			}},
		}
		tm.mat.SendRequest(currRequest)
		//TODO: Wait for reply in different for
		states[i] = <-currReadChan
		close(currReadChan)
	}

	//fmt.Println("[TM]Static read with clk: ", tsToUse)
	readArgs.ReplyChan <- TMStaticReadReply{
		States:    states,
		Timestamp: tsToUse,
	}
}

//TODO: Separate in parts?
func (tm *TransactionManager) handleStaticTMUpdate(request TransactionManagerRequest) {
	updateArgs := request.Args.(TMStaticUpdateArgs)

	newTxnId := TransactionId(rand.Uint64())
	//1st step: discover involved partitions and group updates
	updsPerPartition := groupWrites(updateArgs.UpdateParams)

	replyChannels := make([]chan TimestampErrorPair, 0, len(updsPerPartition))
	var currChan chan TimestampErrorPair
	var currRequest MaterializerRequest
	//2nd step: send update operations to each involved partition
	for partId, partUpdates := range updsPerPartition {
		if partUpdates != nil {
			currChan = make(chan TimestampErrorPair)
			currRequest = MaterializerRequest{
				MatRequestArgs: MatStaticUpdateArgs{
					TransactionId: newTxnId,
					Updates:       partUpdates,
					ReplyChan:     currChan,
				},
			}
			replyChannels = append(replyChannels, currChan)
			tm.mat.SendRequestToChannel(currRequest, uint64(partId))
		}
	}

	var maxTimestamp *clocksi.Timestamp = &clocksi.DummyTs
	//Also 2nd step: wait for reply of each partition
	//TODO: Possibly paralelize? What if errors occour?
	//TODO: Doesn't this clock certain partitions forever if an error occours?
	for _, channel := range replyChannels {
		reply := <-channel
		if reply.Timestamp == nil {
			updateArgs.ReplyChan <- TMStaticUpdateReply{
				Timestamp: nil,
				Err:       reply.error,
			}
			return
		}
		if reply.Timestamp.IsHigherOrEqual(*maxTimestamp) {
			maxTimestamp = &reply.Timestamp
		}
	}

	//3rd step: send commit to involved partitions
	//TODO: Should I not assume that the 2nd phase of commit is fail-safe?
	commitReq := MaterializerRequest{MatRequestArgs: MatCommitArgs{
		TransactionId:   newTxnId,
		CommitTimestamp: *maxTimestamp,
	}}
	for partId, partUpdates := range updsPerPartition {
		if partUpdates != nil {
			tm.mat.SendRequestToChannel(commitReq, uint64(partId))
		}
	}

	tm.localClock.Lock()
	tm.localClock.Timestamp = tm.localClock.Merge(*maxTimestamp)
	tm.localClock.Unlock()

	//4th step: send ok to client
	updateArgs.ReplyChan <- TMStaticUpdateReply{
		TransactionId: newTxnId,
		Timestamp:     *maxTimestamp,
		Err:           nil,
	}

	/*
		Algorithm:
			1st step: discover involved partitions and group writes
				- for update in writeRequest.UpdateParams
					- getPartitionKey
					- add update to list
			2nd step: send update operations to each involved partition and collect proposed timestamp
				- for each partition involved
					- send list of updates
					- wait for proposed timestamp
					- if proposed timestamp > highest proposed timestamp so far
						highest timestamp = proposed timestamp
			3rd step: send commit to involved partitions
				- for each partition
					- commit(highest timestamp)
			4th step: send ok to client
	*/
}

//TODO: Group reads and wait for their replies at the end, preferably in a common channel to all partitions
func (tm *TransactionManager) handleTMRead(request TransactionManagerRequest) {
	//++fmt.Println(tm.replicaID, "TM - Started handling read.")
	readArgs := request.Args.(TMReadArgs)
	tsToUse := request.Timestamp

	var currReadChan chan crdt.State = nil
	var currRequest MaterializerRequest
	states := make([]crdt.State, len(readArgs.ReadParams))

	//Now, ask to read the client requested version.
	for i, currRead := range readArgs.ReadParams {
		currReadChan = make(chan crdt.State)

		currRequest = MaterializerRequest{
			MatRequestArgs: MatReadArgs{MatReadCommonArgs: MatReadCommonArgs{
				Timestamp:        tsToUse,
				ReadObjectParams: currRead,
				ReplyChan:        currReadChan,
			}, TransactionId: request.TransactionId},
		}
		tm.mat.SendRequest(currRequest)
		//TODO: Wait for reply in different for
		states[i] = <-currReadChan
		close(currReadChan)
	}

	readArgs.ReplyChan <- states
	//++fmt.Println(tm.replicaID, "TM - finished handling read.")
}

func (tm *TransactionManager) handleTMUpdate(request TransactionManagerRequest, txnPartitions *ongoingTxn) {
	//++fmt.Printf("%d TM%d - Started handling update.\n", tm.replicaID, txnPartitions.debugID)
	updateArgs := request.Args.(TMUpdateArgs)

	updsPerPartition := groupWrites(updateArgs.UpdateParams)

	replyChannels := make([]chan BoolErrorPair, 0, len(updsPerPartition))
	var currChan chan BoolErrorPair
	var currRequest MaterializerRequest
	var partId uint64

	for id, partUpdates := range updsPerPartition {
		if partUpdates != nil {
			partId = uint64(id)
			currChan = make(chan BoolErrorPair)
			currRequest = MaterializerRequest{
				MatRequestArgs: MatUpdateArgs{
					TransactionId: request.TransactionId,
					Updates:       partUpdates,
					ReplyChan:     currChan,
				},
			}
			replyChannels = append(replyChannels, currChan)
			//Mark this partition as one of the involved in this txnId
			if _, hasPart := txnPartitions.partSet[partId]; !hasPart {
				txnPartitions.add(partId)
			}
			//++fmt.Printf("%d TM%d - Trying to send upds list.\n", tm.replicaID, txnPartitions.debugID)
			tm.mat.SendRequestToChannel(currRequest, partId)
			//++fmt.Printf("%d TM%d - Upds list sent.\n", tm.replicaID, txnPartitions.debugID)
		}
	}

	//++fmt.Printf("%d TM%d - Listening to upd replies.\n", tm.replicaID, txnPartitions.debugID)
	var errString = ""
	//TODO: Possibly paralelize? What if errors occour?
	for _, channel := range replyChannels {
		reply := <-channel
		if reply.error != nil {
			errString += reply.Error()
		}
	}

	if errString == "" {
		updateArgs.ReplyChan <- TMUpdateReply{
			Success: true,
			Err:     nil,
		}
	} else {
		//TODO: Send abort on error?
		updateArgs.ReplyChan <- TMUpdateReply{
			Success: false,
			Err:     fmt.Errorf(errString),
		}
	}
	//++fmt.Printf("%d TM%d - Finished handling update.\n", tm.replicaID, txnPartitions.debugID)
}

/*
	Returns an array in which each index corresponds to one partition.
	Associated to each index is the list of reads that belong to the referred partition
*/
func groupReads(reads []KeyParams) (readsPerPartition [][]KeyParams) {
	readsPerPartition = make([][]KeyParams, nGoRoutines)
	var currChanKey uint64

	for _, read := range reads {
		currChanKey = GetChannelKey(read)
		if readsPerPartition[currChanKey] == nil {
			readsPerPartition[currChanKey] = make([]KeyParams, 0, len(reads)*2/int(nGoRoutines))
		}
		readsPerPartition[currChanKey] = append(readsPerPartition[currChanKey], read)
	}

	return
}

/*
	Returns an array in which each index corresponds to one partition.
	Associated to each index is the list of writes that belong to the referred partition
*/
func groupWrites(updates []*UpdateObjectParams) (updsPerPartition [][]*UpdateObjectParams) {
	updsPerPartition = make([][]*UpdateObjectParams, nGoRoutines)
	var currChanKey uint64

	for _, upd := range updates {
		currChanKey = GetChannelKey(upd.KeyParams)
		if updsPerPartition[currChanKey] == nil {
			updsPerPartition[currChanKey] = make([]*UpdateObjectParams, 0, len(updates)*2/int(nGoRoutines))
		}
		updsPerPartition[currChanKey] = append(updsPerPartition[currChanKey], upd)
	}

	return
}

func (tm *TransactionManager) handleTMStartTxn(request TransactionManagerRequest, txnPartitions *ongoingTxn) {
	//++fmt.Printf("%d TM%d - Started handling startTxn.\n", tm.replicaID, txnPartitions.debugID)
	startTxnArgs := request.Args.(TMStartTxnArgs)

	//TODO: Ensure that the new clock is higher than the one received from the client. Also, take in consideration txn properties?
	tm.localClock.Lock()
	newClock := tm.localClock.NextTimestamp(tm.replicaID)
	tm.localClock.Unlock()
	txnPartitions.TransactionId = TransactionId(rand.Uint64())
	txnPartitions.partSet = makePartSet()

	startTxnArgs.ReplyChan <- TMStartTxnReply{TransactionId: txnPartitions.TransactionId, Timestamp: newClock}
	//++fmt.Printf("%d TM%d - Finished handling finishTxn.\n", tm.replicaID, txnPartitions.debugID)
}

func (tm *TransactionManager) handleTMCommit(request TransactionManagerRequest, txnPartitions *ongoingTxn) {
	//++fmt.Printf("%d TM%d - Started handling commit.\n", tm.replicaID, txnPartitions.debugID)
	commitArgs := request.Args.(TMCommitArgs)

	//PREPARE
	involvedPartitions := txnPartitions.partSet
	replyChannels := make([]chan clocksi.Timestamp, 0, len(involvedPartitions))
	var currRequest MaterializerRequest
	//TODO: Use bounded channels and send the same channel to every partition?
	var currChan chan clocksi.Timestamp

	//Send prepare to each partition involved
	//++fmt.Printf("%d TM%d - Sending prepares to Materializers for id %d.\n", tm.replicaID, txnPartitions.debugID, request.TransactionId)
	for partId, _ := range involvedPartitions {
		currChan = make(chan clocksi.Timestamp)
		currRequest = MaterializerRequest{MatRequestArgs: MatPrepareArgs{TransactionId: request.TransactionId, ReplyChan: currChan}}
		replyChannels = append(replyChannels, currChan)
		//++fmt.Printf("%d TM%d - Sending prepare to Materializer %d for id %d.\n", tm.replicaID, txnPartitions.debugID, partId, request.TransactionId)
		tm.mat.SendRequestToChannel(currRequest, partId)
	}

	//Collect proposed timestamps and accept the maximum one
	//var maxTimestamp clocksi.Timestamp = nil
	var maxTimestamp clocksi.Timestamp = clocksi.DummyTs
	//TODO: Possibly paralelize?
	//++fmt.Printf("%d TM%d - Waiting for prepares from Materializers.\n", tm.replicaID, txnPartitions.debugID)
	for _, channel := range replyChannels {
		replyTs := <-channel
		if replyTs.IsHigherOrEqual(maxTimestamp) {
			maxTimestamp = replyTs
		}
	}

	//COMMIT
	//Send commit to involved partitions
	//TODO: Should I not assume that the 2nd phase of commit is fail-safe?

	//++fmt.Printf("%d TM%d - Sending commits to Materializers.\n", tm.replicaID, txnPartitions.debugID)
	for partId, _ := range involvedPartitions {
		currRequest = MaterializerRequest{MatRequestArgs: MatCommitArgs{
			TransactionId:   request.TransactionId,
			CommitTimestamp: maxTimestamp,
		}}
		tm.mat.SendRequestToChannel(currRequest, uint64(partId))
	}

	tm.localClock.Lock()
	tm.localClock.Timestamp = tm.localClock.Merge(maxTimestamp)
	tm.localClock.Unlock()

	txnPartitions.reset()

	//Send ok to client
	commitArgs.ReplyChan <- TMCommitReply{
		Timestamp: maxTimestamp,
		Err:       nil,
	}
	//++fmt.Printf("%d TM%d - Finished handling commit.\n", tm.replicaID, txnPartitions.debugID)
}

func (tm *TransactionManager) handleTMAbort(request TransactionManagerRequest, txnPartitions *ongoingTxn) {
	abortReq := MaterializerRequest{MatRequestArgs: MatAbortArgs{TransactionId: request.TransactionId}}
	for partId, _ := range txnPartitions.partSet {
		tm.mat.SendRequestToChannel(abortReq, uint64(partId))
	}
	txnPartitions.reset()
}

//Temporary method. This is used to avoid compile errors on unused variables
//This unused variables mark stuff that isn't being processed yet.
func ignore(any ...interface{}) {

}

func (tm *TransactionManager) applyRemoteClk(request *TMRemoteClk) {
	fmt.Println("[TM]Applying remote clk.")
	//All txns were applied, so it's safe to update the local clock of both TM and materializer.
	if tm.downstreamQueue[request.ReplicaID] == nil {
		tm.localClock.Lock()
		tm.localClock.Timestamp = tm.localClock.Timestamp.UpdatePos(request.ReplicaID, request.StableTs)
		tm.localClock.Unlock()
		tm.mat.SendRequestToAllChannels(MaterializerRequest{MatRequestArgs: MatClkPosUpdArgs{ReplicaID: request.ReplicaID, StableTs: request.StableTs}})
		//Send request
		//TODO: Filter somehow isolated clocks? I.e., clock updates that are sent from time to time without being associated to a txn.
		//fmt.Println("[REMOTE TM]Sent request to NUCRDT TM")

		//This if may be false when we receive a clock but not its transaction (possible situation: clock for bucket we're not replicating, but we still want to update the clock)
		if nPartitions, has := tm.nPartitionsForRemoteTxn[request.ReplicaID]; has {
			tm.downstreamOpsCh <- TMNewRemoteTxn{Timestamp: tm.clockOfRemoteTxn[request.ReplicaID], nPartitions: *nPartitions}
			delete(tm.nPartitionsForRemoteTxn, request.ReplicaID)
			delete(tm.clockOfRemoteTxn, request.ReplicaID)
		}
	} else {
		tm.downstreamQueue[request.ReplicaID] = append(tm.downstreamQueue[request.ReplicaID], request)
	}
}

func (tm *TransactionManager) applyRemoteTxn(request *TMRemotePartTxn) {
	fmt.Println("[TM]Applying remote txn with timestamp.", request.Timestamp.ToString())
	tools.FancyDebugPrint(tools.TM_PRINT, tm.replicaID, "Started applying remote txn. Local clk:", tm.localClock.Timestamp, ". Remote clk:", request.Timestamp)
	nPartitionsSoFar, has := tm.nPartitionsForRemoteTxn[request.ReplicaID]
	if !has {
		value := 1
		tm.nPartitionsForRemoteTxn[request.ReplicaID] = &value
		tm.clockOfRemoteTxn[request.ReplicaID] = request.Timestamp
	} else {
		*nPartitionsSoFar++
	}

	tm.localClock.Lock()
	//TODO: Maybe I should check with Materializer's clock?
	isLowerOrEqual := request.Timestamp.IsLowerOrEqualExceptFor(tm.localClock.Timestamp, tm.replicaID, request.ReplicaID)
	if isLowerOrEqual {
		tm.downstreamRemoteTxn(request.PartitionID, request.ReplicaID, request.Timestamp, request.Upds)
		tools.FancyDebugPrint(tools.TM_PRINT, tm.replicaID, "Finished applying remote txn")
		tm.checkPendingRemoteTxns()
	} else {
		//Queue
		tools.FancyDebugPrint(tools.TM_PRINT, tm.replicaID, "Queuing remote txn")
		tm.downstreamQueue[request.ReplicaID] = append(tm.downstreamQueue[request.ReplicaID], request)
	}
	tm.localClock.Unlock()
}

//Pre-condition: localClock's mutex is hold
func (tm *TransactionManager) downstreamRemoteTxn(partitionID int64, replicaID int16, txnClk clocksi.Timestamp, txnOps []*UpdateObjectParams) {
	tools.FancyDebugPrint(tools.TM_PRINT, tm.replicaID, "Started downstream remote txn")
	currRequest := MaterializerRequest{MatRequestArgs: MatRemoteTxnArgs{
		ReplicaID: replicaID,
		Timestamp: txnClk,
		Upds:      txnOps,
	}}
	tools.FancyDebugPrint(tools.TM_PRINT, tm.replicaID, "Sending remoteTxn request to Materializer")
	if len(txnOps) > 0 {
		tools.FancyDebugPrint(tools.TM_PRINT, tm.replicaID, "Partition", partitionID, "has operations:", txnOps)
	} else {
		tools.FancyDebugPrint(tools.TM_PRINT, tm.replicaID, "Partition", partitionID, "has NO operations:", txnOps)
	}
	tm.mat.SendRequestToChannel(currRequest, uint64(partitionID))
	tools.FancyDebugPrint(tools.TM_PRINT, tm.replicaID, "Finished downstream remote txn")
}

//Pre-condition: localClock's mutex is hold
func (tm *TransactionManager) checkPendingRemoteTxns() {
	tools.FancyDebugPrint(tools.TM_PRINT, tm.replicaID, "Started checking for pending remote txns")
	//AppliedAtLeastOne: to know if there was clock progression. Skip: if we must skip the remaining replica's txns as one wasn't yet ready to be applied
	appliedAtLeastOne, skip := true, false
	var req TMRemoteMsg
	//No txns pending, can return right away
	if len(tm.downstreamQueue) == 0 {
		tools.FancyDebugPrint(tools.TM_PRINT, tm.replicaID, "Finished checking for pending remote txns")
		return
	}
	for appliedAtLeastOne {
		for replicaID, pendingTxns := range tm.downstreamQueue {
			if !appliedAtLeastOne {
				break
			}
			appliedAtLeastOne, skip = false, false
			i := 0
			for ; i < len(pendingTxns) && !skip; i++ {
				req = pendingTxns[i]
				switch typedReq := req.(type) {
				case *TMRemoteClk:
					tm.localClock.UpdatePos(typedReq.ReplicaID, typedReq.StableTs)
					tm.mat.SendRequestToAllChannels(MaterializerRequest{MatRequestArgs: MatClkPosUpdArgs{ReplicaID: typedReq.ReplicaID, StableTs: typedReq.StableTs}})
					fmt.Println(tm.clockOfRemoteTxn[typedReq.ReplicaID], tm.nPartitionsForRemoteTxn[typedReq.ReplicaID])
					tm.downstreamOpsCh <- TMNewRemoteTxn{Timestamp: tm.clockOfRemoteTxn[typedReq.ReplicaID], nPartitions: *tm.nPartitionsForRemoteTxn[typedReq.ReplicaID]}
					delete(tm.nPartitionsForRemoteTxn, typedReq.ReplicaID)
					delete(tm.clockOfRemoteTxn, typedReq.ReplicaID)
					appliedAtLeastOne = true
				case *TMRemotePartTxn:
					isLowerOrEqual := typedReq.Timestamp.IsLowerOrEqualExceptFor(tm.localClock.Timestamp, tm.replicaID, typedReq.ReplicaID)
					if isLowerOrEqual {
						nPartitionsSoFar, has := tm.nPartitionsForRemoteTxn[typedReq.ReplicaID]
						if !has {
							value := 1
							tm.nPartitionsForRemoteTxn[typedReq.ReplicaID] = &value
							tm.clockOfRemoteTxn[typedReq.ReplicaID] = typedReq.Timestamp
						} else {
							*nPartitionsSoFar++
						}
						tm.downstreamRemoteTxn(typedReq.PartitionID, typedReq.ReplicaID, typedReq.Timestamp, typedReq.Upds)
						appliedAtLeastOne = true
					} else {
						i--
						skip = true
					}
				}
			}
			if i == len(pendingTxns) {
				delete(tm.downstreamQueue, replicaID)
			} else {
				tm.downstreamQueue[replicaID] = pendingTxns[i:]
			}
		}
	}
	tools.FancyDebugPrint(tools.TM_PRINT, tm.replicaID, "Finished checking for pending remote txns")
}

//A separate go routine that is responsible for handling ops that are generated when remote downstream ops are applied
//For now, this should only happen with NuCRDTs.
func (tm *TransactionManager) handleDownstreamGeneratedOps() {
	//TODO: Find out why this keeps on receiving msgs with updated timestamps from time to time that come from both TM + materializer
	//If it was only materializer it would be understandable, as it would just be clock updates.
	//Actually it seems to be applying ops so... something is quite slow here.
	/*
		for {
			<-tm.downstreamOpsCh
			fmt.Println("[NUCRDT TM]Debugging, ignored request")
		}
	*/
	partitionsLeft := make(map[clocksi.TimestampKey]*int)                       //Stores the number of partitions that still need to reply
	opsForTxn := make(map[clocksi.TimestampKey]map[int64][]*UpdateObjectParams) //Stores the generated ops separated by partition.
	for {
		//fmt.Println("[NUCRDT TM]Waiting for request...")
		switch typedReq := (<-tm.downstreamOpsCh).(type) {
		case TMNewRemoteTxn:
			//fmt.Println("[NUCRDT TM]Handling TMNewRemoteTxn for timestamp" + typedReq.Timestamp.ToString())
			clkKey := typedReq.Timestamp.GetMapKey()
			nLeft, hasEntry := partitionsLeft[clkKey]
			if !hasEntry {
				partitionsLeft[clkKey] = &typedReq.nPartitions
				opsForTxn[clkKey] = make(map[int64][]*UpdateObjectParams)
			} else {
				*nLeft += typedReq.nPartitions
				if *nLeft == 0 {
					//Already received a reply from all materializers
					ops := opsForTxn[clkKey]
					//Only create a txn if there's actually new ops
					if len(ops) > 0 {
						//fmt.Println("[NUCRDT TM]Received reply from all partitions, need to commit for timestamp", typedReq.Timestamp.ToString())
						go tm.commitOpsForRemote(ops)
					}
					delete(opsForTxn, clkKey)
					delete(partitionsLeft, clkKey)
				}
			}
		case TMDownstreamNewOps:
			//fmt.Printf("[NUCRDT TM]Handling TMDownstreamNewOps for timestamp %s from partition %d.\n", typedReq.Timestamp.ToString(), typedReq.partitionID)
			clkKey := typedReq.Timestamp.GetMapKey()
			nLeft, hasEntry := partitionsLeft[clkKey]
			if !hasEntry {
				value := -1 //We don't know yet how many partitions are involved in this txn
				nLeft = &value
				partitionsLeft[clkKey] = &value
				opsForTxn[clkKey] = make(map[int64][]*UpdateObjectParams)
			} else {
				*nLeft -= 1
			}
			if len(typedReq.newOps) > 0 {
				//Actually has ops
				opsForTxn[clkKey][typedReq.partitionID] = typedReq.newOps
			}
			if *nLeft == 0 {
				//Already received a reply from all materializers
				ops := opsForTxn[clkKey]
				//Only create a txn if there's actually new ops
				if len(ops) > 0 {
					//fmt.Println("[NUCRDT TM]Received reply from all partitions, need to commit for timestamp", typedReq.Timestamp.ToString())
					go tm.commitOpsForRemote(ops)
				}
				delete(opsForTxn, clkKey)
				delete(partitionsLeft, clkKey)
			}
		}
	}
}

func (tm *TransactionManager) handleTMGetSnapshot(snapshot *TMGetSnapshot) {
	buckets, replChan := snapshot.Buckets, snapshot.ReplyChan

	tm.localClock.Lock()
	tsToUse := tm.localClock.Timestamp
	tm.localClock.Unlock()
	nParts := len(tm.mat.channels)

	//Ask to read snapshots based on the localClock.
	replyChans := make([]chan []*proto.ProtoCRDT, nParts)
	for i := range replyChans {
		channel := make(chan []*proto.ProtoCRDT)
		replyChans[i] = channel
		tm.mat.SendRequestToChannel(MaterializerRequest{MatRequestArgs: MatGetSnapshotArgs{
			Timestamp: tsToUse, Buckets: buckets, ReplyChan: channel}}, uint64(i))
	}
	partStates := make([][]*proto.ProtoCRDT, nParts)
	for i, channel := range replyChans {
		partStates[i] = <-channel
		close(channel)
	}

	replChan <- TMGetSnapshotReply{Timestamp: tsToUse, PartStates: partStates}
}

func (tm *TransactionManager) handleTMApplySnapshot(snapshot *TMApplySnapshot) {
	ts, states := snapshot.Timestamp, snapshot.PartStates
	tm.localClock.Lock()
	ts.UpdatePos(tm.replicaID, tm.localClock.GetPos(tm.replicaID))
	tm.localClock.Unlock()

	for i, partState := range states {
		tm.mat.SendRequestToChannel(MaterializerRequest{MatRequestArgs: MatApplySnapshotArgs{
			Timestamp: ts, ProtoCRDTs: partState}}, uint64(i))
	}

	//TODO: Need to update remote entries at the end of this... or right at start?
	tm.localClock.Lock()
	tm.localClock.Timestamp = tm.localClock.Merge(ts)
	tm.localClock.Unlock()
}

//Does a local commit but without applying the operations in the local partitions.
//This is used for operations generated by executing downstream remote ops,
//which need to be sent to other replicas in the same txn, but don't need to be applied locally.
func (tm *TransactionManager) commitOpsForRemote(ops map[int64][]*UpdateObjectParams) {
	//Send special prepare to replicas; wait for reply
	//Send commit
	//Done
	//MatPrepareForRemoteArgs
	fmt.Println("[NUCRDT TM generated goroutine]Starting to commit remote-only ops...")
	newTxnId := TransactionId(rand.Uint64())
	replyChannels := make([]chan clocksi.Timestamp, 0, len(ops))
	var currChan chan clocksi.Timestamp
	var currRequest MaterializerRequest

	//Send prepare
	for partId, partUpdates := range ops {
		//TODO: Is this nil verificatino really necessary?
		if partUpdates != nil {
			currChan = make(chan clocksi.Timestamp)
			currRequest = MaterializerRequest{
				MatRequestArgs: MatPrepareForRemoteArgs{
					TransactionId: newTxnId,
					Updates:       partUpdates,
					ReplyChan:     currChan,
				},
			}
			replyChannels = append(replyChannels, currChan)
			tm.mat.SendRequestToChannel(currRequest, uint64(partId))
		}
	}
	fmt.Println("[NUCRDT TM generated goroutine]Sent special prepare")

	var maxTimestamp *clocksi.Timestamp = &clocksi.DummyTs
	//Wait for reply of each partition
	//TODO: Possibly paralelize?
	for _, channel := range replyChannels {
		replyTs := <-channel
		if replyTs.IsHigherOrEqual(*maxTimestamp) {
			maxTimestamp = &replyTs
		}
	}
	fmt.Println("[NUCRDT TM generated goroutine]Received all replies to special prepare...")

	//Send commit to involved partitions
	//TODO: Should I not assume that the previous phase of commit is fail-safe?
	commitReq := MaterializerRequest{MatRequestArgs: MatCommitArgs{
		TransactionId:   newTxnId,
		CommitTimestamp: *maxTimestamp,
	}}
	for partId, partUpdates := range ops {
		//TODO: Is this nil verificatino really necessary?
		if partUpdates != nil {
			tm.mat.SendRequestToChannel(commitReq, uint64(partId))
		}
	}
	fmt.Println("[NUCRDT TM generated goroutine]Sent all commits")
	tm.localClock.Lock()
	tm.localClock.Timestamp = tm.localClock.Merge(*maxTimestamp)
	tm.localClock.Unlock()
}
