package antidote

//TODO: Read-write lock for the clock? That might help when doing queries-only.

//A circular array. Each instance adds a read clock (in order) to the array and stores its position
//Then, when the read clears, said position is marked as clear and can be rewritten.
//If the array ever gets full, a new one must be created.
//As such we need to keep the start and end position of said circular array
//Thus it is safe to clean up to exactly the start clock.
//Problem: if we start using the max between client's clock and server's clock, it will no longer work.
//This'll have to be dealt it in some special way...

import (
	fmt "fmt"
	"math/rand"
	"net"
	"potionDB/src/clocksi"
	"potionDB/src/crdt"
	"potionDB/src/proto"
	"potionDB/src/tools"
	"sync"
)

/////*****************TYPE DEFINITIONS***********************/////
//TODO: Extract requests types, replies and methods to another file

type KeyParams struct {
	Key      string
	CrdtType proto.CRDTType
	Bucket   string
}

type UpdateObjectParams struct {
	KeyParams
	UpdateArgs *crdt.UpdateArguments
}

type ReadObjectParams struct {
	KeyParams
	ReadArgs crdt.ReadArguments
}

type TransactionManagerRequest struct {
	TransactionId //TODO: Remove this, as most requests don't need it (iirc, only staticWrite, commit and abort use it)
	Timestamp     clocksi.Timestamp
	Args          TMRequestArgs
}

type TMRequestArgs interface {
	getRequestType() (requestType TMRequestType)
}

type TMReadArgs struct {
	ReadParams []ReadObjectParams
	ReplyChan  chan []crdt.State
}

type TMUpdateArgs struct {
	UpdateParams []*UpdateObjectParams
	ReplyChan    chan TMUpdateReply
}

type TMStaticUpdateArgs struct {
	UpdateParams []*UpdateObjectParams
	ReplyChan    chan TMStaticUpdateReply
}

type TMStaticReadArgs struct {
	ReadParams []ReadObjectParams
	ReplyChan  chan TMStaticReadReply
}

type TMConnLostArgs struct {
}

type TMStartTxnArgs struct {
	ReplyChan chan TMStartTxnReply
}

type TMCommitArgs struct {
	ReplyChan chan TMCommitReply
}

type TMAbortArgs struct {
}

type TMNewTriggerArgs struct {
	Source, Target Link
	IsGeneric      bool
	ReplyChan      chan bool
}

type TMGetTriggersArgs struct {
	WaitFor   chan bool
	ReplyChan chan *TriggerDB
}

type TMServerConn struct{}

/*****Remote/Replicator interaction structs*****/

//Used by the Replication Layer. Use a different thread to handle this
/*
type TMRemoteTxn struct {
	ReplicaID int16
	Upds      []NewRemoteTxns
	StableTs  int64
}
*/

type TMRemoteMsg interface {
	getReplicaID() int16
}

type TMRemoteClk struct {
	ReplicaID int16
	StableTs  int64
}

type TMRemotePartTxn struct {
	PartitionID int64
	ReplicaID   int16
	clocksi.Timestamp
	Upds []*UpdateObjectParams
}

type TMGetSnapshot struct {
	Buckets   map[string]struct{}
	ReplyChan chan TMGetSnapshotReply
}

type TMApplySnapshot struct {
	clocksi.Timestamp
	PartStates [][]*proto.ProtoCRDT
}

type TMReplicaID struct {
	ReplicaID int16
	IP        string
	Buckets   []string
}

type TMRemoteTrigger struct {
	AutoUpdate
	IsGeneric bool
}

type TMStart struct {
}

/*****Msgs for handling ops generated by downstream remotes*****/

type TMDownstreamRemoteMsg interface {
}

type TMNewRemoteTxn struct {
	clocksi.Timestamp
	nPartitions int
}

type TMDownstreamNewOps struct {
	clocksi.Timestamp
	partitionID int64
	newOps      []*UpdateObjectParams
}

/***** *****/

type TMStaticReadReply struct {
	States    []crdt.State
	Timestamp clocksi.Timestamp
}

type TMStaticUpdateReply struct {
	TransactionId
	Timestamp clocksi.Timestamp
	Err       error
}

type TMUpdateReply struct {
	Success bool
	Err     error
}

type TMStartTxnReply struct {
	TransactionId
	Timestamp clocksi.Timestamp
}

type TMCommitReply struct {
	Timestamp clocksi.Timestamp
	Err       error
}

type TMGetSnapshotReply struct {
	Timestamp  clocksi.Timestamp
	PartStates [][]*proto.ProtoCRDT
}

type TMNewTriggerReply struct{}

type TMGetTriggersReply struct {
}

type TMRequestType int

type ClientId uint64

/*
type TransactionId struct {
	ClientId  ClientId
	Timestamp clocksi.Timestamp
}
*/
type TransactionId uint64

type ongoingRemote struct {
	originalClk clocksi.Timestamp
	conns       []net.Conn
	//nConns       int //when nConns = len(conns), can skip checking to start connections
	txnDataToUse [][]byte
	nTxnsStarted int //Number of connections with a txn started (txnDataToUse[i] != nil). When it's equal to len(conns), can skip a check to start txn.
}

type ongoingTxn struct {
	TransactionId
	//partSet
	partitions []bool     //true: partition participates; false: partition doesn't participate.
	debugID    int        //random ID just for debbuging purposes
	conns      []net.Conn //connection to other replicas that have been created by this transaction
	ongoingRemote
}

//type partSet map[uint64]struct{}

type ProtectedClock struct {
	clocksi.Timestamp
	sync.Mutex
}

type ProtectedTriggerDB struct {
	TriggerDB
	sync.RWMutex
}

type TransactionManager struct {
	mat                     *Materializer
	remoteChan              chan TMRemoteMsg
	localClock              ProtectedClock
	txnsSinceCompact        int //Number of txns done since the last time history was compacted. Also protected by the above mutex.
	downstreamQueue         map[int16][]TMRemoteMsg
	replicator              *Replicator
	replicaID               int16
	downstreamOpsCh         chan TMDownstreamRemoteMsg  //Channel for handling ops that are generated when applying remote downstreams.
	nPartitionsForRemoteTxn map[int16]*int              //Number of partitions that are involved in the current remote txn for a given replicaID.
	clockOfRemoteTxn        map[int16]clocksi.Timestamp //Stores the timestamps for the remote txns referred in the map above
	waitStartChan           chan bool                   //Channel for notifying ProtoServer when is TM ready to start processing requests
	triggerDB               ProtectedTriggerDB
	//Only used if doCompactHistory=true
	ongoingReads map[TransactionId]int //TxnID -> position in circular array
	clocksArray  *tools.CircularArray
	RemoteInfo
}

type RemoteInfo struct {
	remoteBks     [][]string //Note: This gets turned to nil after all replicas are known
	remoteIPs     []string
	bucketToIndex map[string][]int
	ownBuckets    []string
	hasAll        bool //In case server is replicating "*"
}

/////*****************CONSTANTS AND VARIABLES***********************/////

const (
	readStaticTMRequest   TMRequestType = 0
	updateStaticTMRequest TMRequestType = 1
	readTMRequest         TMRequestType = 2
	updateTMRequest       TMRequestType = 3
	startTxnTMRequest     TMRequestType = 4
	commitTMRequest       TMRequestType = 5
	abortTMRequest        TMRequestType = 6
	newTriggerTMRequest   TMRequestType = 7
	getTriggersTMRequest  TMRequestType = 8
	serverConnRequest     TMRequestType = 80
	lostConnRequest       TMRequestType = 255

	downstreamOpsChBufferSize int = 100 //Default buffer size for the downstreamOpsCh
)

//Both are filled from configs
var (
	doCompactHistory         = false
	historyCompactInterval   = 60
	historyCompactTargetTxns = 1000
	circularArraySize        = 1000 //Array grows as needed
)

/////*****************TYPE METHODS***********************/////

//TransactionManagerRequest

func (args TMStaticReadArgs) getRequestType() (requestType TMRequestType) {
	return readStaticTMRequest
}

func (args TMStaticUpdateArgs) getRequestType() (requestType TMRequestType) {
	return updateStaticTMRequest
}

func (args TMReadArgs) getRequestType() (requestType TMRequestType) {
	return readTMRequest
}

func (args TMUpdateArgs) getRequestType() (requestType TMRequestType) {
	return updateTMRequest
}

func (args TMConnLostArgs) getRequestType() (requestType TMRequestType) {
	return lostConnRequest
}

func (args TMStartTxnArgs) getRequestType() (requestType TMRequestType) {
	return startTxnTMRequest
}

func (args TMCommitArgs) getRequestType() (requestType TMRequestType) {
	return commitTMRequest
}

func (args TMAbortArgs) getRequestType() (requestType TMRequestType) {
	return abortTMRequest
}

func (args TMNewTriggerArgs) getRequestType() (requestType TMRequestType) {
	return newTriggerTMRequest
}

func (args TMGetTriggersArgs) getRequestType() (requestType TMRequestType) {
	return getTriggersTMRequest
}

func (args TMServerConn) getRequestType() (requestType TMRequestType) {
	return serverConnRequest
}

//TMRemoteMsg

func (req TMRemoteClk) getReplicaID() (id int16) {
	return req.ReplicaID
}

func (req TMRemotePartTxn) getReplicaID() (id int16) {
	return req.ReplicaID
}

func (args TMGetSnapshot) getReplicaID() (id int16) {
	return 0 //Irrelevant
}

func (args TMApplySnapshot) getReplicaID() (id int16) {
	return 0 //Irrelevant
}

func (args TMStart) getReplicaID() (id int16) {
	return 0 //Irrelevant
}

func (args TMReplicaID) getReplicaID() (id int16) {
	return args.ReplicaID
}

//ReplicaID is irrelevant. It's only here for interface
func (args TMRemoteTrigger) getReplicaID() (id int16) {
	return 0
}

//Others

/*
func makePartSet() (set partSet) {
	set = partSet(make(map[uint64]struct{}))
	return
}

func (set partSet) add(partId uint64) {
	set[partId] = struct{}{}
}
*/

func (txnPartitions *ongoingTxn) reset() {
	txnPartitions.TransactionId = 0
	//txnPartitions.partSet = nil
	txnPartitions.partitions = make([]bool, nGoRoutines)
	//txnPartitions.ongoingRemote = ongoingRemote{}
	txnPartitions.ongoingRemote.reset()
}

func (remote *ongoingRemote) initializeConnections(remoteIPs []string) {
	remote.conns = make([]net.Conn, len(remoteIPs))
	var err error
	//Sending reqs
	//fmt.Println("[remote]Initializing connections")
	for i := range remote.conns {
		remote.conns[i], err = net.Dial("tcp", remoteIPs[i])
		tools.CheckErr("Network connection establishment err on ongoingRemote.initializeConnections()", err)
		SendProto(ServerConn, CreateServerConn(), remote.conns[i])
	}
}

func (remote *ongoingRemote) closeConnections() {
	for _, conn := range remote.conns {
		conn.Close()
	}
}

func (remote *ongoingRemote) reset() {
	remote.originalClk, remote.txnDataToUse, remote.nTxnsStarted = nil, nil, 0
}

/////*****************TRANSACTION MANAGER CODE***********************/////

func (tm *TransactionManager) ResetServer() {
	matChan := make(chan bool, len(tm.mat.channels))
	tm.mat.SendRequestToAllChannels(MaterializerRequest{MatRequestArgs: MatResetArgs{ReplyChan: matChan}})
	tm.replicator.Reset()
	tm.remoteChan = make(chan TMRemoteMsg)
	tm.localClock = ProtectedClock{Mutex: sync.Mutex{}, Timestamp: clocksi.NewClockSiTimestamp()}
	tm.downstreamQueue = make(map[int16][]TMRemoteMsg)
	tm.nPartitionsForRemoteTxn = make(map[int16]*int)
	tm.clockOfRemoteTxn = make(map[int16]clocksi.Timestamp)
	for i := 0; i < len(tm.mat.channels); i++ {
		<-matChan
	}
	fmt.Println("[TM]Reset complete.")
}

func Initialize(replicaID int16) (tm *TransactionManager) {
	clocksi.AddNewID(replicaID)
	downstreamOpsCh := make(chan TMDownstreamRemoteMsg, downstreamOpsChBufferSize)
	mat, loggers := InitializeMaterializer(replicaID, downstreamOpsCh)
	//mat, _, _ := InitializeMaterializer(replicaID)
	tm = &TransactionManager{
		mat:                     mat,
		remoteChan:              make(chan TMRemoteMsg),
		localClock:              ProtectedClock{Mutex: sync.Mutex{}, Timestamp: clocksi.NewClockSiTimestamp()},
		txnsSinceCompact:        0,
		downstreamQueue:         make(map[int16][]TMRemoteMsg),
		replicator:              &Replicator{},
		replicaID:               replicaID,
		downstreamOpsCh:         downstreamOpsCh,
		nPartitionsForRemoteTxn: make(map[int16]*int),
		clockOfRemoteTxn:        make(map[int16]clocksi.Timestamp),
		waitStartChan:           make(chan bool, 1),
		triggerDB:               ProtectedTriggerDB{RWMutex: sync.RWMutex{}, TriggerDB: InitializeTriggerDB()},
		RemoteInfo: RemoteInfo{
			bucketToIndex: make(map[string][]int),
			hasAll:        false,
		},
	}
	tm.replicator.Initialize(tm, loggers, replicaID)
	tm.ownBuckets = tm.replicator.buckets
	//Check if server replicates all buckets
	for _, bkt := range tm.ownBuckets {
		if bkt == "*" {
			tm.hasAll = true
			break
		}
	}
	go tm.handleRemoteMsgs()
	go tm.handleDownstreamGeneratedOps()
	if doCompactHistory {
		tm.ongoingReads, tm.clocksArray = make(map[TransactionId]int), &tools.CircularArray{}
		tm.clocksArray.Initialize(circularArraySize)
		go tm.doHistoryCompact()
	}
	return tm
}

func CreateKeyParams(key string, crdtType proto.CRDTType, bucket string) (keyParams KeyParams) {
	return KeyParams{Key: key, CrdtType: crdtType, Bucket: bucket}
}

func (tm *TransactionManager) WaitUntilReady() {
	<-tm.waitStartChan
}

//Starts a goroutine to handle the client requests. Returns a channel to communicate with that goroutine
func (tm *TransactionManager) CreateClientHandler() (channel chan TransactionManagerRequest) {
	channel = make(chan TransactionManagerRequest)
	go tm.listenForProtobufRequests(channel)
	return
}

func (tm *TransactionManager) SendRemoteMsg(msg TMRemoteMsg) {
	tm.remoteChan <- msg
}

func (tm *TransactionManager) listenForProtobufRequests(channel chan TransactionManagerRequest) {
	stop := false
	var txnPartitions *ongoingTxn = &ongoingTxn{}
	txnPartitions.partitions = make([]bool, nGoRoutines)
	txnPartitions.debugID = rand.Intn(10)
	txnPartitions.ongoingRemote = ongoingRemote{}

	firstReq := <-channel
	if firstReq.Args.getRequestType() != serverConnRequest {
		txnPartitions.initializeConnections(tm.remoteIPs)
		stop = tm.handleTMRequest(firstReq, txnPartitions)
	}
	for !stop {
		request := <-channel
		stop = tm.handleTMRequest(request, txnPartitions)
	}

	tools.FancyDebugPrint(tools.TM_PRINT, tm.replicaID, "connection lost, shutting down goroutine for client.")
}

func (tm *TransactionManager) handleTMRequest(request TransactionManagerRequest,
	txnPartitions *ongoingTxn) (shouldStop bool) {
	shouldStop = false

	switch request.Args.getRequestType() {
	case readStaticTMRequest:
		tm.handleStaticTMRead(request, &txnPartitions.ongoingRemote)
	case updateStaticTMRequest:
		tm.handleStaticTMUpdate(request, &txnPartitions.ongoingRemote)
	case readTMRequest:
		tm.handleTMRead(request, txnPartitions)
	case updateTMRequest:
		tm.handleTMUpdate(request, txnPartitions)
	case startTxnTMRequest:
		tm.handleTMStartTxn(request, txnPartitions)
	case commitTMRequest:
		tm.handleTMCommit(request, txnPartitions)
	case abortTMRequest:
		tm.handleTMAbort(request, txnPartitions)
	case newTriggerTMRequest:
		tm.handleNewTrigger(request)
	case getTriggersTMRequest:
		tm.handleGetTriggers(request)
	case lostConnRequest:
		shouldStop = true
		txnPartitions.closeConnections()
		txnPartitions = nil
	}
	//remoteTxnRequest is handled separatelly

	return
}

func (tm *TransactionManager) handleRemoteMsgs() {
	for {
		request := <-tm.remoteChan
		switch typedReq := request.(type) {
		case TMRemoteClk:
			tm.applyRemoteClk(&typedReq)
		case TMRemotePartTxn:
			tm.applyRemoteTxn(&typedReq)
		case TMGetSnapshot:
			tm.handleTMGetSnapshot(&typedReq)
		case TMApplySnapshot:
			tm.handleTMApplySnapshot(&typedReq)
		case TMReplicaID:
			tm.handleReplicaID(&typedReq)
		case TMRemoteTrigger:
			tm.handleRemoteTrigger(&typedReq)
		case TMStart:
			tm.handleTMStart(&typedReq)
		}
	}
}

func (tm *TransactionManager) handleStaticTMRead(request TransactionManagerRequest, ongoingRemote *ongoingRemote) {
	readArgs := request.Args.(TMStaticReadArgs)
	//tsToUse := request.Timestamp
	tm.localClock.Lock()
	tsToUse := tm.localClock.Timestamp.Copy()
	tm.localClock.Unlock()

	var historyPos int
	if doCompactHistory {
		historyPos = tm.clocksArray.Write(tsToUse)
	}

	/*
		var currReadChan chan crdt.State = nil
		var currRequest MaterializerRequest
		states := make([]crdt.State, len(readArgs.ReadParams))

		//Now, ask to read the client requested version.
		for i, currRead := range readArgs.ReadParams {
			currReadChan = make(chan crdt.State)
			//currReadChan = make(chan crdt.State, 1)

			currRequest = MaterializerRequest{
				MatRequestArgs: MatStaticReadArgs{MatReadCommonArgs: MatReadCommonArgs{
					Timestamp:        tsToUse,
					ReadObjectParams: currRead,
					ReplyChan:        currReadChan,
				}},
			}
			tm.mat.SendRequest(currRequest)
			//TODO: Wait for reply in different for
			states[i] = <-currReadChan
			close(currReadChan)*/
	/*
		select {
		case states[i] = <-currReadChan:
			close(currReadChan)
		case <-time.After(15 * time.Second):
			fmt.Printf("[TM]Read with timestamp %s (%s) has blocked for more than 15s. Returning CounterState{0} response. Partition at fault: %d. "+
				"TxnId/ClientId: %d. Key: %s %s %d\n",
				tsToUse.ToString(), tsToUse.GetMapKey(), GetChannelKey(readArgs.ReadParams[i].KeyParams),
				request.TransactionId,
				readArgs.ReadParams[i].Key, readArgs.ReadParams[i].Bucket, readArgs.ReadParams[i].CrdtType)
			states[i] = crdt.CounterState{Value: 0}
		}
	*/ /*
		}
	*/

	//TODO: Go back

	var currRequest MaterializerRequest
	readChans := make([]chan crdt.State, len(readArgs.ReadParams))
	states := make([]crdt.State, len(readArgs.ReadParams))
	//remoteArgs := make([]ReadObjectParams, len(readArgs.ReadParams))

	reqsPerServer := make([][]ReadObjectParams, len(tm.remoteIPs))
	remoteReqsToChan := make([][]int, len(tm.remoteIPs))
	isRemote, serverIndex, hasRemote := true, 0, false

	for i, currRead := range readArgs.ReadParams {
		readChans[i] = make(chan crdt.State, 1)
		isRemote, serverIndex = tm.getReadLocation(currRead.Bucket)

		if !isRemote {
			currRequest = MaterializerRequest{
				MatRequestArgs: MatStaticReadArgs{MatReadCommonArgs: MatReadCommonArgs{
					Timestamp:        tsToUse,
					ReadObjectParams: currRead,
					ReplyChan:        readChans[i],
				}},
			}
			tm.mat.SendRequest(currRequest)
		} else {
			reqsPerServer[serverIndex] = append(reqsPerServer[serverIndex], currRead)
			remoteReqsToChan[serverIndex] = append(remoteReqsToChan[serverIndex], i)
			hasRemote = true
		}
	}

	if hasRemote {
		go tm.handleRemoteStaticReads(tsToUse, reqsPerServer, remoteReqsToChan, readChans, ongoingRemote)
	}

	/*
		for i, readChan := range readChans {
			select {
			case states[i] = <-readChan:
				close(readChan)
			case <-time.After(8 * time.Second):
				fmt.Printf("[TM]Read with timestamp %s (%s) has blocked for more than 15s. Returning CounterState{0} response. Partition at fault: %d. "+
					"TxnId/ClientId: %d. Key: %s %s %d\n",
					tsToUse.ToString(), tsToUse.GetMapKey(), GetChannelKey(readArgs.ReadParams[i].KeyParams),
					request.TransactionId,
					readArgs.ReadParams[i].Key, readArgs.ReadParams[i].Bucket, readArgs.ReadParams[i].CrdtType)
				states[i] = crdt.CounterState{Value: 0}
			}
		}
	*/

	for i, readChan := range readChans {
		states[i] = <-readChan
		close(readChan)
	}

	if doCompactHistory {
		tm.clocksArray.Delete(historyPos)
	}

	//fmt.Println("[TM]Static read with clk: ", tsToUse)
	readArgs.ReplyChan <- TMStaticReadReply{
		States:    states,
		Timestamp: tsToUse,
	}
}

//TODO: Separate in parts?
func (tm *TransactionManager) handleStaticTMUpdate(request TransactionManagerRequest, ongoingRemote *ongoingRemote) {
	updateArgs := request.Args.(TMStaticUpdateArgs)

	newTxnId := TransactionId(rand.Uint64())
	//1st step: discover involved partitions and group updates
	updsPerPartition, reqsPerServer, hasRemote := tm.groupWrites(updateArgs.UpdateParams)

	replyChannels := make([]chan TimestampErrorPair, 0, len(updsPerPartition))
	var currChan chan TimestampErrorPair
	var currRequest MaterializerRequest

	//2nd step: send update operations to each involved partition
	for partId, partUpdates := range updsPerPartition {
		if partUpdates != nil {
			currChan = make(chan TimestampErrorPair, 1) //To avoid materializers blocking while trying to reply
			currRequest = MaterializerRequest{
				MatRequestArgs: MatStaticUpdateArgs{
					TransactionId: newTxnId,
					Updates:       partUpdates,
					ReplyChan:     currChan,
				},
			}
			replyChannels = append(replyChannels, currChan)
			tm.mat.SendRequestToChannel(currRequest, uint64(partId))
		}
	}

	var maxTimestamp *clocksi.Timestamp = &clocksi.DummyTs
	//Also 2nd step: wait for reply of each partition
	//TODO: Possibly paralelize? What if errors occour?
	//TODO: Doesn't this block certain partitions forever if an error occours?
	for _, channel := range replyChannels {
		reply := <-channel
		if reply.Timestamp == nil {
			updateArgs.ReplyChan <- TMStaticUpdateReply{
				Timestamp: nil,
				Err:       reply.error,
			}
			fmt.Println("[TM]Write returned nil, thus partitions will block forever!!!")
			fmt.Println("[TM]Reported error: ", reply.error)
			return
		}
		if reply.Timestamp.IsHigherOrEqual(*maxTimestamp) {
			maxTimestamp = &reply.Timestamp
			//tmp := reply.Timestamp.Copy()
			//maxTimestamp = &tmp
		}
	}

	//3rd step: send commit to involved partitions
	//TODO: Should I not assume that the 2nd phase of commit is fail-safe?
	commitReq := MaterializerRequest{MatRequestArgs: MatCommitArgs{
		TransactionId:   newTxnId,
		CommitTimestamp: *maxTimestamp,
	}}
	for partId, partUpdates := range updsPerPartition {
		if partUpdates != nil {
			tm.mat.SendRequestToChannel(commitReq, uint64(partId))
		}
	}

	//TODO: Maybe TM's clock should only be updated AFTER all partitions apply the txn?
	//This should reduce the number of reads that get queued for later (and even reduce to 0 if clients are sticky), albeit it might raise the number of reads
	//in the past.
	tm.localClock.Lock()
	copyClk := tm.localClock.Copy()
	tm.localClock.Timestamp = tm.localClock.Merge(*maxTimestamp)
	tm.txnsSinceCompact++
	tm.localClock.Unlock()

	//Request for remote operations. Wait for remote commit
	if hasRemote {
		tm.handleRemoteStaticUpds(copyClk, reqsPerServer, ongoingRemote)
	}

	//4th step: send ok to client
	updateArgs.ReplyChan <- TMStaticUpdateReply{
		TransactionId: newTxnId,
		Timestamp:     *maxTimestamp,
		Err:           nil,
	}

	/*
		Algorithm:
			1st step: discover involved partitions and group writes
				- for update in writeRequest.UpdateParams
					- getPartitionKey
					- add update to list
			2nd step: send update operations to each involved partition and collect proposed timestamp
				- for each partition involved
					- send list of updates
					- wait for proposed timestamp
					- if proposed timestamp > highest proposed timestamp so far
						highest timestamp = proposed timestamp
			3rd step: send commit to involved partitions
				- for each partition
					- commit(highest timestamp)
			4th step: send ok to client
	*/
}

//TODO: Group reads that go for the same partition
func (tm *TransactionManager) handleTMRead(request TransactionManagerRequest, txnPartitions *ongoingTxn) {
	readArgs := request.Args.(TMReadArgs)
	tsToUse := request.Timestamp

	/*
		var currReadChan chan crdt.State = nil
		var currRequest MaterializerRequest
		states := make([]crdt.State, len(readArgs.ReadParams))

		//Now, ask to read the client requested version.
		for i, currRead := range readArgs.ReadParams {
			currReadChan = make(chan crdt.State, 1)

			currRequest = MaterializerRequest{
				MatRequestArgs: MatReadArgs{MatReadCommonArgs: MatReadCommonArgs{
					Timestamp:        tsToUse,
					ReadObjectParams: currRead,
					ReplyChan:        currReadChan,
				}, TransactionId: request.TransactionId},
			}
			tm.mat.SendRequest(currRequest)
			states[i] = <-currReadChan
			close(currReadChan)
		}

		readArgs.ReplyChan <- states
		//++fmt.Println(tm.replicaID, "TM - finished handling read.")
	*/

	var currRequest MaterializerRequest
	readChans := make([]chan crdt.State, len(readArgs.ReadParams))
	states := make([]crdt.State, len(readArgs.ReadParams))

	reqsPerServer := make([][]ReadObjectParams, len(tm.remoteIPs))
	remoteReqsToChan := make([][]int, len(tm.remoteIPs))
	isRemote, serverIndex, hasRemote := true, 0, false

	for i, currRead := range readArgs.ReadParams {
		readChans[i] = make(chan crdt.State, 1)
		isRemote, serverIndex = tm.getReadLocation(currRead.Bucket)

		if !isRemote {
			currRequest = MaterializerRequest{
				MatRequestArgs: MatReadArgs{MatReadCommonArgs: MatReadCommonArgs{
					Timestamp:        tsToUse,
					ReadObjectParams: currRead,
					ReplyChan:        readChans[i],
				}, TransactionId: request.TransactionId},
			}
			tm.mat.SendRequest(currRequest)
		} else {
			reqsPerServer[serverIndex] = append(reqsPerServer[serverIndex], currRead)
			remoteReqsToChan[serverIndex] = append(remoteReqsToChan[serverIndex], i)
			hasRemote = true
		}
	}

	if hasRemote {
		go tm.handleRemoteReads(txnPartitions, reqsPerServer, remoteReqsToChan, readChans)
	}

	for i, readChan := range readChans {
		states[i] = <-readChan
		close(readChan)
	}

	readArgs.ReplyChan <- states
	//++fmt.Println(tm.replicaID, "TM - finished handling read.")
}

func (tm *TransactionManager) handleTMUpdate(request TransactionManagerRequest, txnPartitions *ongoingTxn) {
	//++fmt.Printf("%d TM%d - Started handling update.\n", tm.replicaID, txnPartitions.debugID)
	updateArgs := request.Args.(TMUpdateArgs)

	updsPerPartition, reqsPerServer, hasRemote := tm.groupWrites(updateArgs.UpdateParams)

	replyChannels := make([]chan BoolErrorPair, 0, len(updsPerPartition))
	var currChan chan BoolErrorPair
	var currRequest MaterializerRequest
	var partId uint64

	for id, partUpdates := range updsPerPartition {
		if partUpdates != nil {
			partId = uint64(id)
			currChan = make(chan BoolErrorPair, 1) //To avoid materializers from blocking when trying to reply
			currRequest = MaterializerRequest{
				MatRequestArgs: MatUpdateArgs{
					TransactionId: request.TransactionId,
					Updates:       partUpdates,
					ReplyChan:     currChan,
				},
			}
			replyChannels = append(replyChannels, currChan)
			//Mark this partition as one of the involved in this txnId
			/*
				if _, hasPart := txnPartitions.partSet[partId]; !hasPart {
					txnPartitions.add(partId)
				}
			*/
			txnPartitions.partitions[partId] = true
			//++fmt.Printf("%d TM%d - Trying to send upds list.\n", tm.replicaID, txnPartitions.debugID)
			tm.mat.SendRequestToChannel(currRequest, partId)
			//++fmt.Printf("%d TM%d - Upds list sent.\n", tm.replicaID, txnPartitions.debugID)
		}
	}

	if hasRemote {
		tm.handleRemoteUpds(txnPartitions, reqsPerServer)
	}

	//++fmt.Printf("%d TM%d - Listening to upd replies.\n", tm.replicaID, txnPartitions.debugID)
	var errString = ""
	//TODO: Possibly paralelize? What if errors occour?
	for _, channel := range replyChannels {
		reply := <-channel
		if reply.error != nil {
			errString += reply.Error()
		}
	}

	if errString == "" {
		updateArgs.ReplyChan <- TMUpdateReply{
			Success: true,
			Err:     nil,
		}
	} else {
		//TODO: Send abort on error?
		updateArgs.ReplyChan <- TMUpdateReply{
			Success: false,
			Err:     fmt.Errorf(errString),
		}
	}
	//++fmt.Printf("%d TM%d - Finished handling update.\n", tm.replicaID, txnPartitions.debugID)
}

func (tm *TransactionManager) handleNewTrigger(request TransactionManagerRequest) {
	args := request.Args.(TMNewTriggerArgs)
	src, target := args.Source, args.Target

	tm.triggerDB.Lock()
	fmt.Printf("Handling client trigger: %v\n", args)
	if args.IsGeneric {
		matchParams := tm.triggerDB.GetMatchableKeyParams(src.Key, src.Bucket, src.CrdtType)
		tm.triggerDB.AddGenericLink(matchParams, src, target)
	} else {
		tm.triggerDB.AddObject(src.KeyParams)
		tm.triggerDB.AddLink(src, target)
	}
	args.ReplyChan <- true
	tm.triggerDB.DebugPrint("[TM@NewT]")
	tm.triggerDB.Unlock()
	tm.replicator.remoteConn.SendTrigger(AutoUpdate{Trigger: src, Target: target}, args.IsGeneric)
}

func (tm *TransactionManager) handleGetTriggers(request TransactionManagerRequest) {
	args := request.Args.(TMGetTriggersArgs)

	tm.triggerDB.RLock()
	tm.triggerDB.DebugPrint("[TM@GetT]")
	args.ReplyChan <- &tm.triggerDB.TriggerDB
	<-args.WaitFor
	tm.triggerDB.RUnlock()
}

/*
	Returns an array in which each index corresponds to one partition.
	Associated to each index is the list of reads that belong to the referred partition
*/
func groupReads(reads []KeyParams) (readsPerPartition [][]KeyParams) {
	readsPerPartition = make([][]KeyParams, nGoRoutines)
	var currChanKey uint64

	for _, read := range reads {
		currChanKey = GetChannelKey(read)
		if readsPerPartition[currChanKey] == nil {
			readsPerPartition[currChanKey] = make([]KeyParams, 0, len(reads)*2/int(nGoRoutines))
		}
		readsPerPartition[currChanKey] = append(readsPerPartition[currChanKey], read)
	}

	return
}

/*
	Returns an array in which each index corresponds to one partition.
	Associated to each index is the list of writes that belong to the referred partition
	It also separates local updates from updates for objects non-locally replicated
*/
func (tm TransactionManager) groupWrites(updates []*UpdateObjectParams) (updsPerPartition [][]*UpdateObjectParams, reqsPerServer [][]UpdateObjectParams, hasRemote bool) {
	updsPerPartition, reqsPerServer = make([][]*UpdateObjectParams, nGoRoutines), make([][]UpdateObjectParams, len(tm.remoteIPs))
	var currChanKey uint64
	isRemote, serverIndex, hasRemote := true, 0, false

	for _, upd := range updates {
		isRemote, serverIndex = tm.getReadLocation(upd.Bucket)

		if !isRemote {
			currChanKey = GetChannelKey(upd.KeyParams)
			if updsPerPartition[currChanKey] == nil {
				updsPerPartition[currChanKey] = make([]*UpdateObjectParams, 0, len(updates)*2/int(nGoRoutines))
			}
			updsPerPartition[currChanKey] = append(updsPerPartition[currChanKey], upd)
		} else {
			reqsPerServer[serverIndex] = append(reqsPerServer[serverIndex], *upd)
			hasRemote = true
		}
	}

	return
}

func (tm *TransactionManager) handleTMStartTxn(request TransactionManagerRequest, txnPartitions *ongoingTxn) {
	//++fmt.Printf("%d TM%d - Started handling startTxn.\n", tm.replicaID, txnPartitions.debugID)
	startTxnArgs := request.Args.(TMStartTxnArgs)

	//TODO: Ensure that the new clock is higher than the one received from the client. Also, take in consideration txn properties?
	tm.localClock.Lock()
	//Two txns may get the same clock...
	newClock := tm.localClock.NextTimestamp(tm.replicaID)
	txnPartitions.originalClk = tm.localClock.Copy()
	tm.localClock.Unlock()
	txnPartitions.TransactionId = TransactionId(rand.Uint64())

	//Remote data
	//txnPartitions.conns = make([]net.Conn, len(tm.remoteIPs))
	txnPartitions.txnDataToUse = make([][]byte, len(tm.remoteIPs))

	if doCompactHistory {
		tm.ongoingReads[txnPartitions.TransactionId] = tm.clocksArray.Write(newClock)
	}
	//txnPartitions.partSet = makePartSet()
	//txnPartitions.partitions = make([]bool, nGoRoutines)
	//It's already initialized.

	startTxnArgs.ReplyChan <- TMStartTxnReply{TransactionId: txnPartitions.TransactionId, Timestamp: newClock}
	//++fmt.Printf("%d TM%d - Finished handling finishTxn.\n", tm.replicaID, txnPartitions.debugID)
}

func (tm *TransactionManager) handleTMCommit(request TransactionManagerRequest, txnPartitions *ongoingTxn) {
	//++fmt.Printf("%d TM%d - Started handling commit.\n", tm.replicaID, txnPartitions.debugID)
	commitArgs := request.Args.(TMCommitArgs)

	//PREPARE
	//involvedPartitions := txnPartitions.partSet
	involvedPartitions := txnPartitions.partitions
	replyChannels := make([]chan clocksi.Timestamp, 0, len(involvedPartitions))
	var currRequest MaterializerRequest
	//TODO: Use bounded channels and send the same channel to every partition?
	var currChan chan clocksi.Timestamp
	remoteChan := make(chan bool, 1)
	//if txnPartitions.nConns > 0 {
	if txnPartitions.nTxnsStarted > 0 {
		go tm.handleRemoteCommit(txnPartitions, remoteChan)
	}

	//Send prepare to each partition involved
	//++fmt.Printf("%d TM%d - Sending prepares to Materializers for id %d.\n", tm.replicaID, txnPartitions.debugID, request.TransactionId)
	//for partId, _ := range involvedPartitions {
	for partId, isOn := range involvedPartitions {
		if isOn {
			currChan = make(chan clocksi.Timestamp, 1)
			currRequest = MaterializerRequest{MatRequestArgs: MatPrepareArgs{TransactionId: request.TransactionId, ReplyChan: currChan}}
			replyChannels = append(replyChannels, currChan)
			//++fmt.Printf("%d TM%d - Sending prepare to Materializer %d for id %d.\n", tm.replicaID, txnPartitions.debugID, partId, request.TransactionId)
			tm.mat.SendRequestToChannel(currRequest, uint64(partId))
		}
	}
	//}

	//Collect proposed timestamps and accept the maximum one
	//var maxTimestamp clocksi.Timestamp = nil
	var maxTimestamp clocksi.Timestamp = clocksi.DummyTs
	//TODO: Possibly paralelize?
	//++fmt.Printf("%d TM%d - Waiting for prepares from Materializers.\n", tm.replicaID, txnPartitions.debugID)
	for _, channel := range replyChannels {
		replyTs := <-channel
		if replyTs.IsHigherOrEqual(maxTimestamp) {
			maxTimestamp = replyTs
		}
	}

	//COMMIT
	//Send commit to involved partitions
	//TODO: Should I not assume that the 2nd phase of commit is fail-safe?

	//++fmt.Printf("%d TM%d - Sending commits to Materializers.\n", tm.replicaID, txnPartitions.debugID)
	for partId, isOn := range involvedPartitions {
		//for partId, _ := range involvedPartitions {
		if isOn {
			currRequest = MaterializerRequest{MatRequestArgs: MatCommitArgs{
				TransactionId:   request.TransactionId,
				CommitTimestamp: maxTimestamp,
			}}
			tm.mat.SendRequestToChannel(currRequest, uint64(partId))
		}
	}

	tm.localClock.Lock()
	tm.localClock.Timestamp = tm.localClock.Merge(maxTimestamp)
	tm.txnsSinceCompact++
	tm.localClock.Unlock()

	if doCompactHistory {
		tm.clocksArray.Delete(tm.ongoingReads[txnPartitions.TransactionId])
		delete(tm.ongoingReads, txnPartitions.TransactionId)
	}

	//if txnPartitions.nConns > 0 {
	if txnPartitions.nTxnsStarted > 0 {
		<-remoteChan //Wait for remote commits to be confirmed.
	}

	txnPartitions.reset()

	//Send ok to client
	commitArgs.ReplyChan <- TMCommitReply{
		Timestamp: maxTimestamp,
		Err:       nil,
	}
	//++fmt.Printf("%d TM%d - Finished handling commit.\n", tm.replicaID, txnPartitions.debugID)
}

func (tm *TransactionManager) handleTMAbort(request TransactionManagerRequest, txnPartitions *ongoingTxn) {
	abortReq := MaterializerRequest{MatRequestArgs: MatAbortArgs{TransactionId: request.TransactionId}}
	//for partId, _ := range txnPartitions.partSet {
	for partId, isOn := range txnPartitions.partitions {
		if isOn {
			tm.mat.SendRequestToChannel(abortReq, uint64(partId))
		}
	}
	if doCompactHistory {
		tm.clocksArray.Delete(tm.ongoingReads[txnPartitions.TransactionId])
	}

	//if txnPartitions.nConns > 0 {
	if txnPartitions.nTxnsStarted > 0 {
		remoteChan := make(chan bool, 1)
		tm.handleRemoteAbort(txnPartitions, remoteChan)
		<-remoteChan
	}
	txnPartitions.reset()
}

/*
type RemoteInfo struct {
	idToBuckets map[int16][]string
	idToIp      map[int16]string
	bucketToIPs map[string][]string
	ownBuckets  []string
}
*/

func (tm *TransactionManager) getReadLocation(bkt string) (isRemote bool, remoteIndex int) {
	if tm.hasAll || bkt == "" {
		//fmt.Println("[ReadLoc]Read for", bkt, "this replica has all or bkt is empty.")
		return false, 0
	}
	for _, localBkt := range tm.ownBuckets {
		if bkt == localBkt {
			//fmt.Println("[ReadLoc]Read for", bkt, "found local matching bucket", localBkt)
			return false, 0
		}
	}
	serverMap, has := tm.bucketToIndex[bkt]
	if !has {
		//Must use a server with *

		//fmt.Println("[ReadLoc]Read for", bkt, "unknown bucket, trying to match with '*'")
		serverMap = tm.bucketToIndex["*"]
	}

	//fmt.Println("[ReadLoc]Read for", bkt, "sending to first server of this bucket", serverMap)
	return true, serverMap[0]
}

//Temporary method. This is used to avoid compile errors on unused variables
//This unused variables mark stuff that isn't being processed yet.
func ignore(any ...interface{}) {

}

func (tm *TransactionManager) applyRemoteClk(request *TMRemoteClk) {
	//All txns were applied, so it's safe to update the local clock of both TM and materializer.
	if tm.downstreamQueue[request.ReplicaID] == nil {
		tm.localClock.Lock()
		tm.localClock.Timestamp = tm.localClock.Timestamp.UpdatePos(request.ReplicaID, request.StableTs)
		tm.txnsSinceCompact++ //Note: Isolated clocks (i.e. without txns associated) are also counted in
		tm.localClock.Unlock()
		tm.mat.SendRequestToAllChannels(MaterializerRequest{MatRequestArgs: MatClkPosUpdArgs{ReplicaID: request.ReplicaID, StableTs: request.StableTs}})
		//Send request
		//TODO: Filter somehow isolated clocks? I.e., clock updates that are sent from time to time without being associated to a txn.
		//fmt.Println("[REMOTE TM]Sent request to NUCRDT TM")

		//This if may be false when we receive a clock but not its transaction (possible situation: clock for bucket we're not replicating, but we still want to update the clock)
		if nPartitions, has := tm.nPartitionsForRemoteTxn[request.ReplicaID]; has {
			tm.downstreamOpsCh <- TMNewRemoteTxn{Timestamp: tm.clockOfRemoteTxn[request.ReplicaID], nPartitions: *nPartitions}
			delete(tm.nPartitionsForRemoteTxn, request.ReplicaID)
			delete(tm.clockOfRemoteTxn, request.ReplicaID)
		}
		tm.checkPendingRemoteTxns()
	} else {
		tm.downstreamQueue[request.ReplicaID] = append(tm.downstreamQueue[request.ReplicaID], request)
	}
}

func (tm *TransactionManager) applyRemoteTxn(request *TMRemotePartTxn) {
	tools.FancyDebugPrint(tools.TM_PRINT, tm.replicaID, "Started applying remote txn. Local clk:", tm.localClock.Timestamp, ". Remote clk:", request.Timestamp)
	nPartitionsSoFar, has := tm.nPartitionsForRemoteTxn[request.ReplicaID]
	if !has {
		value := 1
		tm.nPartitionsForRemoteTxn[request.ReplicaID] = &value
		tm.clockOfRemoteTxn[request.ReplicaID] = request.Timestamp
	} else {
		*nPartitionsSoFar++
	}

	tm.localClock.Lock()
	//TODO: Maybe I should check with Materializer's clock?
	isLowerOrEqual := request.Timestamp.IsLowerOrEqualExceptFor(tm.localClock.Timestamp, tm.replicaID, request.ReplicaID)
	tm.localClock.Unlock()
	if isLowerOrEqual {
		tm.downstreamRemoteTxn(request.PartitionID, request.ReplicaID, request.Timestamp, request.Upds)
		tools.FancyDebugPrint(tools.TM_PRINT, tm.replicaID, "Finished applying remote txn")
		//Doesn't make sense to check pending txns since the TM's clock isn't updated.
		//tm.checkPendingRemoteTxns()
	} else {
		//Queue
		tools.FancyDebugPrint(tools.TM_PRINT, tm.replicaID, "Queuing remote txn")
		tm.downstreamQueue[request.ReplicaID] = append(tm.downstreamQueue[request.ReplicaID], request)
	}
	//tm.localClock.Unlock()
}

//Pre-condition: localClock's mutex is hold
func (tm *TransactionManager) downstreamRemoteTxn(partitionID int64, replicaID int16, txnClk clocksi.Timestamp, txnOps []*UpdateObjectParams) {
	tools.FancyDebugPrint(tools.TM_PRINT, tm.replicaID, "Started downstream remote txn")
	currRequest := MaterializerRequest{MatRequestArgs: MatRemoteTxnArgs{
		ReplicaID: replicaID,
		Timestamp: txnClk,
		Upds:      txnOps,
	}}
	tools.FancyDebugPrint(tools.TM_PRINT, tm.replicaID, "Sending remoteTxn request to Materializer")
	if len(txnOps) > 0 {
		tools.FancyDebugPrint(tools.TM_PRINT, tm.replicaID, "Partition", partitionID, "has operations:", txnOps)
	} else {
		tools.FancyDebugPrint(tools.TM_PRINT, tm.replicaID, "Partition", partitionID, "has NO operations:", txnOps)
	}
	tm.mat.SendRequestToChannel(currRequest, uint64(partitionID))
	tools.FancyDebugPrint(tools.TM_PRINT, tm.replicaID, "Finished downstream remote txn")
}

//Note: new version no longer assumes localClock's mutex is hold.
func (tm *TransactionManager) checkPendingRemoteTxns() {
	tools.FancyDebugPrint(tools.TM_PRINT, tm.replicaID, "Started checking for pending remote txns")
	//No txns pending, can return right away
	if len(tm.downstreamQueue) == 0 {
		tools.FancyDebugPrint(tools.TM_PRINT, tm.replicaID, "Finished checking for pending remote txns")
		return
	}

	//AppliedAtLeastOne: to know if there was clock progression. Skip: if we must skip the remaining replica's txns as one wasn't yet ready to be applied
	//ClockAlreadyTested: true if the previous typedReq was a TMRemotePartTxn (and thus, clock didn't change so no need to test it again)
	//CanApply: clockAlreadyTested || remoteClk < localClock (except for remote entry). ClockChanged: true if any txn was applied at all.
	appliedAtLeastOne, skip, clockAlreadyTested, canApply, clockChanged := true, false, false, false, false
	var req TMRemoteMsg
	//Obtain copy of TM's clock and use it for further comparisions until

	//Idea: read clock once and create a copy.
	//If txns can be applied, keep applying and comparing the copied clock. Maybe even update the copied clock?
	//When a txn is found that can't be applied, force a updatePos/merge of the copied clock (no use in checking the TM's clock since at best it'll have advanced only in the local entry,
	//which is irrelevant).
	//If it can be applied, repeat the process from the start. Otherwise, stop.
	//At the end, merge copy and TM's clock.

	tm.localClock.Lock()
	copyClk := tm.localClock.Copy()
	tm.localClock.Unlock()
	nApplied := 0

	//TODO: Somewhere around here I should be calling downstreamRemoteTxn...
	for appliedAtLeastOne {
		appliedAtLeastOne = false
		for replicaID, pendingTxns := range tm.downstreamQueue {
			skip, clockAlreadyTested = false, false
			i := 0
			for ; i < len(pendingTxns) && !skip; i++ {
				req = pendingTxns[i]
				switch typedReq := req.(type) {
				case *TMRemoteClk:
					copyClk.UpdatePos(typedReq.ReplicaID, typedReq.StableTs)
					tm.mat.SendRequestToAllChannels(MaterializerRequest{MatRequestArgs: MatClkPosUpdArgs{ReplicaID: typedReq.ReplicaID, StableTs: typedReq.StableTs}})
					nApplied++
					remoteTxnClk, remoteNPartitions := tm.clockOfRemoteTxn[typedReq.ReplicaID], tm.nPartitionsForRemoteTxn[typedReq.ReplicaID]
					//TODO: Remove this if once the bug of sending multiple TMRemoteClk is fixed.
					if remoteTxnClk == nil {
						//fmt.Printf("[TM]Remote clk on hold with ts %d from replicaID %d but without remoteTxns!\n", typedReq.StableTs, typedReq.ReplicaID)
						appliedAtLeastOne, clockAlreadyTested, clockChanged = true, false, true
						continue
					}
					//fmt.Printf("[TM]Remote clk on hold with ts %d from replicaID %d.\n", typedReq.StableTs, typedReq.ReplicaID)
					//fmt.Println("[TM]Clock of remote txn:", remoteTxnClk)
					//fmt.Println("[TM]Number of partitions for remote txn:", remoteNPartitions, *remoteNPartitions)
					//TODO: If this was sent BEFORE the tm.mat.SendRequestToAllChannels, wouldn't it be guaranteed that TMNewRemoteTxn is the first to arrive?
					//This would simplify the code for the goroutine that handles downstreamOpsCh.
					tm.downstreamOpsCh <- TMNewRemoteTxn{Timestamp: remoteTxnClk, nPartitions: *remoteNPartitions}
					delete(tm.nPartitionsForRemoteTxn, typedReq.ReplicaID)
					delete(tm.clockOfRemoteTxn, typedReq.ReplicaID)
					appliedAtLeastOne, clockAlreadyTested, clockChanged = true, false, true
				case *TMRemotePartTxn:
					//fmt.Printf("[TM]Remote part txn (partitionID, replicaID: %d, %d) on hold.\n", typedReq.PartitionID, typedReq.ReplicaID)
					//GO uses short circuit testing, so if the first passes the 2nd isn't even tested.
					canApply = clockAlreadyTested || typedReq.Timestamp.IsLowerOrEqualExceptFor(copyClk, tm.replicaID, typedReq.ReplicaID)
					if canApply && !clockAlreadyTested {
						//fmt.Println("[TM]canApply && !clockAlreadyTested")
						//First remote txn with this clock.
						value := 1
						tm.nPartitionsForRemoteTxn[typedReq.ReplicaID], tm.clockOfRemoteTxn[typedReq.ReplicaID] = &value, typedReq.Timestamp
					}
					if canApply {
						if clockAlreadyTested {
							//fmt.Println("[TM]canApply && clockAlreadyTested")
							*tm.nPartitionsForRemoteTxn[typedReq.ReplicaID]++
						} else {
							//Clock was tested in this iteration.
							clockAlreadyTested = true
						}
						appliedAtLeastOne = true
						//TODO: downstream txn
					} else {
						//fmt.Println("[TM]Skip.")
						i--
						skip = true
					}
				}
			}
			if i == len(pendingTxns) {
				delete(tm.downstreamQueue, replicaID)
			} else {
				tm.downstreamQueue[replicaID] = pendingTxns[i:]
			}
		}
	}

	if clockChanged {
		//Update TM's clock bcs at least one txn was applied
		tm.localClock.Lock()
		tm.localClock.Timestamp = tm.localClock.Merge(copyClk)
		tm.txnsSinceCompact += nApplied
		tm.localClock.Unlock()
	}

	/*
		for appliedAtLeastOne {
			appliedAtLeastOne = false
			for replicaID, pendingTxns := range tm.downstreamQueue {
				skip = false
				i := 0
				for ; i < len(pendingTxns) && !skip; i++ {
					req = pendingTxns[i]
					switch typedReq := req.(type) {
					case *TMRemoteClk:
						tm.localClock.UpdatePos(typedReq.ReplicaID, typedReq.StableTs)
						tm.mat.SendRequestToAllChannels(MaterializerRequest{MatRequestArgs: MatClkPosUpdArgs{ReplicaID: typedReq.ReplicaID, StableTs: typedReq.StableTs}})
						fmt.Println("[TM]Remote txns on hold.", tm.clockOfRemoteTxn[typedReq.ReplicaID], *tm.nPartitionsForRemoteTxn[typedReq.ReplicaID])
						tm.downstreamOpsCh <- TMNewRemoteTxn{Timestamp: tm.clockOfRemoteTxn[typedReq.ReplicaID], nPartitions: *tm.nPartitionsForRemoteTxn[typedReq.ReplicaID]}
						delete(tm.nPartitionsForRemoteTxn, typedReq.ReplicaID)
						delete(tm.clockOfRemoteTxn, typedReq.ReplicaID)
						appliedAtLeastOne, clockAlreadyTested = true, false
					case *TMRemotePartTxn:
						//GO uses short circuit testing, so if the first passes the 2nd isn't even tested.
						canApply = clockAlreadyTested || typedReq.Timestamp.IsLowerOrEqualExceptFor(tm.localClock.Timestamp, tm.replicaID, typedReq.ReplicaID)
						if canApply && !clockAlreadyTested {
							//First remote txn with this clock.
							value := 1
							tm.nPartitionsForRemoteTxn[typedReq.ReplicaID], tm.clockOfRemoteTxn[typedReq.ReplicaID] = &value, typedReq.Timestamp
						}
						if canApply {
							if clockAlreadyTested {
								*tm.nPartitionsForRemoteTxn[typedReq.ReplicaID]++
							}
							appliedAtLeastOne = true
						} else {
							i--
							skip = true
						}*/
	/*
		isLowerOrEqual := typedReq.Timestamp.IsLowerOrEqualExceptFor(tm.localClock.Timestamp, tm.replicaID, typedReq.ReplicaID)
		if isLowerOrEqual {
			nPartitionsSoFar, has := tm.nPartitionsForRemoteTxn[typedReq.ReplicaID]
			if !has {
				value := 1
				tm.nPartitionsForRemoteTxn[typedReq.ReplicaID] = &value
				tm.clockOfRemoteTxn[typedReq.ReplicaID] = typedReq.Timestamp
			} else {
				*nPartitionsSoFar++
			}
			tm.downstreamRemoteTxn(typedReq.PartitionID, typedReq.ReplicaID, typedReq.Timestamp, typedReq.Upds)
			appliedAtLeastOne = true
		} else {
			i--
			skip = true
		}
	*/ /*
					}
				}
				if i == len(pendingTxns) {
					delete(tm.downstreamQueue, replicaID)
				} else {
					tm.downstreamQueue[replicaID] = pendingTxns[i:]
				}
			}
		}
	*/
	tools.FancyDebugPrint(tools.TM_PRINT, tm.replicaID, "Finished checking for pending remote txns")
}

func (tm *TransactionManager) handleTMGetSnapshot(snapshot *TMGetSnapshot) {
	buckets, replChan := snapshot.Buckets, snapshot.ReplyChan

	tm.localClock.Lock()
	tsToUse := tm.localClock.Timestamp
	tm.localClock.Unlock()
	nParts := len(tm.mat.channels)

	//Ask to read snapshots based on the localClock.
	replyChans := make([]chan []*proto.ProtoCRDT, nParts)
	for i := range replyChans {
		channel := make(chan []*proto.ProtoCRDT)
		replyChans[i] = channel
		tm.mat.SendRequestToChannel(MaterializerRequest{MatRequestArgs: MatGetSnapshotArgs{
			Timestamp: tsToUse, Buckets: buckets, ReplyChan: channel}}, uint64(i))
	}
	partStates := make([][]*proto.ProtoCRDT, nParts)
	for i, channel := range replyChans {
		partStates[i] = <-channel
		close(channel)
	}

	replChan <- TMGetSnapshotReply{Timestamp: tsToUse, PartStates: partStates}
}

func (tm *TransactionManager) handleTMApplySnapshot(snapshot *TMApplySnapshot) {
	ts, states := snapshot.Timestamp, snapshot.PartStates
	tm.localClock.Lock()
	ts.UpdatePos(tm.replicaID, tm.localClock.GetPos(tm.replicaID))
	tm.localClock.Unlock()

	for i, partState := range states {
		tm.mat.SendRequestToChannel(MaterializerRequest{MatRequestArgs: MatApplySnapshotArgs{
			Timestamp: ts, ProtoCRDTs: partState}}, uint64(i))
	}

	//TODO: Need to update remote entries at the end of this... or right at start?
	tm.localClock.Lock()
	tm.localClock.Timestamp = tm.localClock.Merge(ts)
	tm.localClock.Unlock()
}

func (tm *TransactionManager) handleReplicaID(replica *TMReplicaID) {
	remoteID := replica.ReplicaID
	fmt.Println("Adding ID", remoteID)
	clocksi.AddNewID(remoteID)
	tm.RemoteInfo.remoteBks = append(tm.RemoteInfo.remoteBks, replica.Buckets)
	tm.RemoteInfo.remoteIPs = append(tm.RemoteInfo.remoteIPs, replica.IP)
}

func (tm *TransactionManager) handleRemoteTrigger(trigger *TMRemoteTrigger) {
	tm.triggerDB.Lock()
	fmt.Printf("[TM]Handling remote trigger: %v\n", *trigger)
	if trigger.IsGeneric {
		src := trigger.Trigger
		matchable := tm.triggerDB.GetMatchableKeyParams(src.Key, src.Bucket, src.CrdtType)
		tm.triggerDB.AddGenericLink(matchable, src, trigger.Target)
	} else {
		tm.triggerDB.AddLink(trigger.AutoUpdate.Trigger, trigger.AutoUpdate.Target)
	}
	tm.triggerDB.Unlock()
}

//This code is run before the server starts accepting client requests, so it doesn't need to be efficient.
func (tm *TransactionManager) handleTMStart(start *TMStart) {
	tm.mat.SendRequestToAllChannels(MaterializerRequest{MatRequestArgs: MatWaitForReplicasArgs{}})
	for i, buckets := range tm.remoteBks {
		for _, bkt := range buckets {
			tm.bucketToIndex[bkt] = append(tm.bucketToIndex[bkt], i)
		}
	}
	tm.remoteBks = nil
	tm.waitStartChan <- true
}

//A separate go routine that is responsible for handling ops that are generated when remote downstream ops are applied
//For now, this should only happen with NuCRDTs.
func (tm *TransactionManager) handleDownstreamGeneratedOps() {
	partitionsLeft := make(map[clocksi.TimestampKey]*int)                       //Stores the number of partitions that still need to reply
	opsForTxn := make(map[clocksi.TimestampKey]map[int64][]*UpdateObjectParams) //Stores the generated ops separated by partition.
	for {
		//fmt.Println("[NUCRDT TM]Waiting for request...")
		switch typedReq := (<-tm.downstreamOpsCh).(type) {
		case TMNewRemoteTxn:
			//fmt.Println("[NUCRDT TM]Handling TMNewRemoteTxn for timestamp" + typedReq.Timestamp.ToString())
			clkKey := typedReq.Timestamp.GetMapKey()
			nLeft, hasEntry := partitionsLeft[clkKey]
			if !hasEntry {
				partitionsLeft[clkKey] = &typedReq.nPartitions
				opsForTxn[clkKey] = make(map[int64][]*UpdateObjectParams)
			} else {
				*nLeft += typedReq.nPartitions
				if *nLeft == 0 {
					//Already received a reply from all materializers
					ops := opsForTxn[clkKey]
					//Only create a txn if there's actually new ops
					if len(ops) > 0 {
						//fmt.Println("[NUCRDT TM]Received reply from all partitions, need to commit for timestamp", typedReq.Timestamp.ToString())
						go tm.commitOpsForRemote(ops)
					}
					delete(opsForTxn, clkKey)
					delete(partitionsLeft, clkKey)
				}
			}
		case TMDownstreamNewOps:
			//fmt.Printf("[NUCRDT TM]Handling TMDownstreamNewOps for timestamp %s from partition %d.\n", typedReq.Timestamp.ToString(), typedReq.partitionID)
			clkKey := typedReq.Timestamp.GetMapKey()
			nLeft, hasEntry := partitionsLeft[clkKey]
			if !hasEntry {
				value := -1 //We don't know yet how many partitions are involved in this txn
				nLeft = &value
				partitionsLeft[clkKey] = &value
				opsForTxn[clkKey] = make(map[int64][]*UpdateObjectParams)
			} else {
				*nLeft -= 1
			}
			if len(typedReq.newOps) > 0 {
				//Actually has ops
				opsForTxn[clkKey][typedReq.partitionID] = typedReq.newOps
			}
			if *nLeft == 0 {
				//Already received a reply from all materializers
				ops := opsForTxn[clkKey]
				//Only create a txn if there's actually new ops
				if len(ops) > 0 {
					//fmt.Println("[NUCRDT TM]Received reply from all partitions, need to commit for timestamp", typedReq.Timestamp.ToString())
					go tm.commitOpsForRemote(ops)
				}
				delete(opsForTxn, clkKey)
				delete(partitionsLeft, clkKey)
			}
		}

	}
}

//Does a local commit but without applying the operations in the local partitions.
//This is used for operations generated by executing downstream remote ops,
//which need to be sent to other replicas in the same txn, but don't need to be applied locally.
func (tm *TransactionManager) commitOpsForRemote(ops map[int64][]*UpdateObjectParams) {
	//Send special prepare to replicas; wait for reply
	//Send commit
	//Done
	//MatPrepareForRemoteArgs
	//fmt.Println("[NUCRDT TM generated goroutine]Starting to commit remote-only ops...")
	newTxnId := TransactionId(rand.Uint64())
	replyChannels := make([]chan clocksi.Timestamp, 0, len(ops))
	var currChan chan clocksi.Timestamp
	var currRequest MaterializerRequest

	//Send prepare
	for partId, partUpdates := range ops {
		//TODO: Is this nil verificatino really necessary?
		if partUpdates != nil {
			currChan = make(chan clocksi.Timestamp, 1)
			currRequest = MaterializerRequest{
				MatRequestArgs: MatPrepareForRemoteArgs{
					TransactionId: newTxnId,
					Updates:       partUpdates,
					ReplyChan:     currChan,
				},
			}
			replyChannels = append(replyChannels, currChan)
			tm.mat.SendRequestToChannel(currRequest, uint64(partId))
		}
	}
	//fmt.Println("[NUCRDT TM generated goroutine]Sent special prepare")

	var maxTimestamp *clocksi.Timestamp = &clocksi.DummyTs
	//Wait for reply of each partition
	//TODO: Possibly paralelize?
	for _, channel := range replyChannels {
		replyTs := <-channel
		if replyTs.IsHigherOrEqual(*maxTimestamp) {
			maxTimestamp = &replyTs
		}
	}
	//fmt.Println("[NUCRDT TM generated goroutine]Received all replies to special prepare...")

	//Send commit to involved partitions
	//TODO: Should I not assume that the previous phase of commit is fail-safe?
	commitReq := MaterializerRequest{MatRequestArgs: MatCommitArgs{
		TransactionId:   newTxnId,
		CommitTimestamp: *maxTimestamp,
	}}
	for partId, partUpdates := range ops {
		//TODO: Is this nil verificatino really necessary?
		if partUpdates != nil {
			tm.mat.SendRequestToChannel(commitReq, uint64(partId))
		}
	}
	//fmt.Println("[NUCRDT TM generated goroutine]Sent all commits")
	tm.localClock.Lock()
	tm.localClock.Timestamp = tm.localClock.Merge(*maxTimestamp)
	tm.txnsSinceCompact++
	tm.localClock.Unlock()
}

func (tm *TransactionManager) doHistoryCompact() {
	oldestRead := tm.clocksArray.ReadFirst()
	if oldestRead == nil {
		//TODO: Clean everything until last commited clock
	} else {
		//oldestTxnClk := oldestRead.(clocksi.Timestamp)
		//TODO: Send requests to clean history
	}
}

//Remote handling functions

//Only for non-static transactions
func (tm *TransactionManager) startTxnForRemote(txnPartitions *ongoingTxn, toContact []bool) {
	indexesToWait := make([]int, 0, len(toContact))
	for i, hasTo := range toContact {
		if hasTo && txnPartitions.conns[i] == nil {
			/*
					conn, err := net.Dial("tcp", tm.remoteIPs[i])
					tools.CheckErr("Network connection establishment err on remote read", err)
					txnPartitions.conns[i] = conn
				SendProto(StartTrans, CreateStartTransaction(txnPartitions.originalClk.ToBytes()), conn)
			*/
			SendProto(StartTrans, CreateStartTransaction(txnPartitions.originalClk.ToBytes()), txnPartitions.conns[i])
			indexesToWait = append(indexesToWait, i)
			//txnPartitions.nConns++
			txnPartitions.nTxnsStarted++
		}
	}

	for _, index := range indexesToWait {
		_, replyProto, _ := ReceiveProto(txnPartitions.conns[index])
		txnPartitions.txnDataToUse[index] = replyProto.(*proto.ApbStartTransactionResp).GetTransactionDescriptor()
	}

}

//Note: Done by a separate goroutine
func (tm *TransactionManager) handleRemoteReads(txnPartitions *ongoingTxn, reqsPerServer [][]ReadObjectParams,
	remoteReqsToChan [][]int, readChans []chan crdt.State) {

	//if txnPartitions.nConns < len(reqsPerServer) {
	if txnPartitions.nTxnsStarted < len(reqsPerServer) {
		toContact, has := make([]bool, len(reqsPerServer)), false
		for i, reqs := range reqsPerServer {
			if len(reqs) > 0 {
				toContact[i], has = true, true
			}
		}
		if has {
			tm.startTxnForRemote(txnPartitions, toContact)
		}
	}

	for i, reqs := range reqsPerServer {
		if len(reqs) > 0 {
			SendProto(ReadObjs, CreateReadObjs(txnPartitions.txnDataToUse[i], reqs), txnPartitions.conns[i])
		}
	}

	var readParams ReadObjectParams
	var currReqs []ReadObjectParams
	var currIndexes []int
	//Receiving replies and redirecting to state
	for i, conn := range txnPartitions.conns {
		currReqs, currIndexes = reqsPerServer[i], remoteReqsToChan[i]
		if len(currReqs) > 0 {
			_, protobuf, _ := ReceiveProto(conn)
			readReply := protobuf.(*proto.ApbReadObjectsResp).GetObjects()
			for j, obj := range readReply {
				readParams = currReqs[j]
				readChans[currIndexes[j]] <- crdt.ReadRespProtoToAntidoteState(obj, readParams.CrdtType, readParams.ReadArgs.GetREADType())
			}
		}
	}
}

func (tm *TransactionManager) handleRemoteStaticReads(ts clocksi.Timestamp, reqsPerServer [][]ReadObjectParams,
	remoteReqsToChan [][]int, readChans []chan crdt.State, ongoingRemote *ongoingRemote) {

	//conns := make([]net.Conn, len(reqsPerServer))
	//Sending reqs
	for i, reqs := range reqsPerServer {
		if len(reqs) > 0 {
			/*
				conn, err := net.Dial("tcp", tm.remoteIPs[i])
				tools.CheckErr("Network connection establishment err on remote read", err)
				conns[i] = conn
				SendProto(StaticReadObjs, CreateStaticReadObjs(ts.ToBytes(), reqs), conn)
			*/
			SendProto(StaticReadObjs, CreateStaticReadObjs(ts.ToBytes(), reqs), ongoingRemote.conns[i])
		}
	}

	var readParams ReadObjectParams
	var currReqs []ReadObjectParams
	var currIndexes []int
	//Receiving replies and redirecting to state
	//for i, conn := range conns {
	for i, conn := range ongoingRemote.conns {
		currReqs, currIndexes = reqsPerServer[i], remoteReqsToChan[i]
		if len(currReqs) > 0 {
			_, protobuf, _ := ReceiveProto(conn)
			readReply := protobuf.(*proto.ApbStaticReadObjectsResp).GetObjects().GetObjects()
			for j, obj := range readReply {
				readParams = currReqs[j]
				readChans[currIndexes[j]] <- crdt.ReadRespProtoToAntidoteState(obj, readParams.CrdtType, readParams.ReadArgs.GetREADType())
			}
			//conns[i].Close()
		}
	}
}

func (tm *TransactionManager) handleRemoteUpds(txnPartitions *ongoingTxn, reqsPerServer [][]UpdateObjectParams) {

	//if txnPartitions.nConns < len(reqsPerServer) {
	if txnPartitions.nTxnsStarted < len(reqsPerServer) {
		toContact, has := make([]bool, len(reqsPerServer)), false
		for i, reqs := range reqsPerServer {
			if len(reqs) > 0 {
				toContact[i], has = true, true
			}
		}
		if has {
			tm.startTxnForRemote(txnPartitions, toContact)
		}
	}

	//Sending reqs
	for i, reqs := range reqsPerServer {
		if len(reqs) > 0 {
			SendProto(UpdateObjs, CreateUpdateObjs(txnPartitions.txnDataToUse[i], reqs), txnPartitions.conns[i])
		}
	}

	var currReqs []UpdateObjectParams
	for i, conn := range txnPartitions.conns {
		currReqs = reqsPerServer[i]
		if len(currReqs) > 0 {
			ReceiveProto(conn) //Waits until the other server acks the write
		}
	}
}

//Note: done by a separate goroutine
func (tm *TransactionManager) handleRemoteCommit(txnPartitions *ongoingTxn, remoteChan chan bool) {
	for i, conn := range txnPartitions.conns {
		//if conn != nil {
		if txnPartitions.txnDataToUse[i] != nil {
			SendProto(CommitTrans, CreateCommitTransaction(txnPartitions.txnDataToUse[i]), conn)
		}
	}

	for i, conn := range txnPartitions.conns {
		//if conn != nil {
		if txnPartitions.txnDataToUse[i] != nil {
			ReceiveProto(conn) //Waits until the other server acks the commit
			conn.Close()
		}
	}

	remoteChan <- true
}

func (tm *TransactionManager) handleRemoteAbort(txnPartitions *ongoingTxn, remoteChan chan bool) {
	for i, conn := range txnPartitions.conns {
		//if conn != nil {
		if txnPartitions.txnDataToUse[i] != nil {
			SendProto(CommitTrans, CreateCommitTransaction(txnPartitions.txnDataToUse[i]), conn)
		}
	}

	for i, conn := range txnPartitions.conns {
		//if conn != nil {
		if txnPartitions.txnDataToUse[i] != nil {
			ReceiveProto(conn) //Waits until the other server acks the abort
			conn.Close()
		}
	}
	remoteChan <- true
}

func (tm *TransactionManager) handleRemoteStaticUpds(ts clocksi.Timestamp, reqsPerServer [][]UpdateObjectParams, ongoingRemote *ongoingRemote) {
	//conns := make([]net.Conn, len(reqsPerServer))
	//Sending reqs
	for i, reqs := range reqsPerServer {
		if len(reqs) > 0 {
			/*
				conn, err := net.Dial("tcp", tm.remoteIPs[i])
				tools.CheckErr("Network connection establishment err on remote read", err)
				conns[i] = conn
				SendProto(StaticUpdateObjs, CreateStaticUpdateObjs(ts.ToBytes(), reqs), conn)
			*/
			SendProto(StaticUpdateObjs, CreateStaticUpdateObjs(ts.ToBytes(), reqs), ongoingRemote.conns[i])
		}
	}

	var currReqs []UpdateObjectParams
	//for i, conn := range conns {
	for i, conn := range ongoingRemote.conns {
		currReqs = reqsPerServer[i]
		if len(currReqs) > 0 {
			ReceiveProto(conn) //Waits until the other server acks the write
			//conns[i].Close()
		}
	}

}
