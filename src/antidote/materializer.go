package antidote

import (
	"container/heap"
	fmt "fmt"
	"math"
	"potionDB/src/clocksi"
	"potionDB/src/crdt"
	"potionDB/src/proto"
	"potionDB/src/shared"
	"potionDB/src/tools"
	"sort"
	"strings"
	"time"

	hashFunc "github.com/twmb/murmur3"
)

//TODO: A good way to "softcheck" things would be to:
//Step1: do a TPC-H test
//Step2: check some time after the updates finished if every hold/prepare is empty
//Step3: check the max sizes of those (i.e., keep track with counters)
//The idea is that, with a test as big as TPC-H, it should cover a lot of situations.

//TODO: When updating TM/Replicator, search for "Pre-condition"
//TODO: Possible improvement: storing the pending updates associated to each object, instead of grouped per txn.
//Advantage is that the access is instant, disadvantage is a lot more maps
//TODO: Group reads of the same partition?

// /////*****************TYPE DEFINITIONS***********************/////
//////////************Requests**************************//////////

type MaterializerRequest struct {
	MatRequestArgs
}

type MatRequestArgs interface {
	getRequestType() (requestType MatRequestType)
	getChannel() (channelId uint64)
}

type MatStaticReadArgs struct {
	MatReadCommonArgs
}

type MatReadArgs struct {
	MatReadCommonArgs
	TransactionId
}

//Args for update request. Note that unlike with MatReadArgs, a MatUpdateArgs represents multiple updates, but all for the same partition
type MatUpdateArgs struct {
	Updates []*UpdateObjectParams
	TransactionId
}

//These arguments are common to StaticReads and normal Reads
type MatReadCommonArgs struct {
	ReadObjectParams
	clocksi.Timestamp
	ReplyChan chan crdt.State
}

type MatStaticUpdateArgs struct {
	Updates []*UpdateObjectParams
	TransactionId
	ReplyChan chan TsClkPair
}

type MatCommitArgs struct {
	TransactionId TransactionId
	ReadClk       clocksi.Timestamp
	CommitTs      int64
}

type MatAbortArgs struct {
	TransactionId TransactionId
}

type MatPrepareArgs struct {
	TransactionId TransactionId
	ReplyChan     chan int64
}

//Used when multiple remote txns may be commited in a row.
//Note: This may include transactions from multiple servers - TM may group them together
type MatRemoteGroupTxnArgs struct {
	Txns      []MatRemoteTxn
	ReplyChan chan []*UpdateObjectParams
}

type MatRemoteTxnArgs struct {
	MatRemoteTxn
	ReplyChan chan []*UpdateObjectParams
}

type MatRemoteTxn struct {
	ReplicaID int16
	TMClock   clocksi.Timestamp //Note: as of now, this corresponds to the ReadClock used in the origin server.
	CommitTs  int64
	Upds      []*UpdateObjectParams
}

type MatClkPosUpdArgs struct {
	ReplicaID int16
	StableTs  int64
	ReplyChan chan bool
}

//Used for preparing operations generated by applying downstream remotes
type MatPrepareForRemoteArgs struct {
	Updates []*UpdateObjectParams
	TransactionId
	ReplyChan chan TsClkPair
}

//Used by the logging layer (to then pass to the replication layer) to know the most recent clock that is "safe", i.e., for which there will be no commit for sure.
type MatSafeClkArgs struct{}

/*
type MatSafeClkArgs struct {
	ReplyChan chan clocksi.Timestamp
}*/

//Used by the replicator's join to know the latest commited timestamp
type MatCommitedClkArgs struct {
	ReplyChan chan TimestampPartIdPair
}

type MatResetArgs struct {
	ReplyChan chan bool
}

//Used to get a snapshot of CRDTs, which is needed by the joining mechanism
type MatGetSnapshotArgs struct {
	clocksi.Timestamp
	Buckets   map[string]struct{}
	ReplyChan chan []*proto.ProtoCRDT
}

//Used to apply a snapshot of CRDTs send by existing replicas in the system
type MatApplySnapshotArgs struct {
	clocksi.Timestamp
	ProtoCRDTs []*proto.ProtoCRDT
}

//Processed separately. This is used to know when the partition can start and the initial clock be generated.
type MatWaitForReplicasArgs struct{}

type MatGCArgs struct {
	SafeClk clocksi.Timestamp //Can clean anything "before" (<) this clock.
}

type TsClkPair struct {
	Clk clocksi.Timestamp
	Ts  int64
}

type MatRequestType byte

type Materializer struct {
	channels []chan MaterializerRequest
}

type partition struct {
	id int64 //partitionID
	partitionClock
	log              Logger            //logs the operations of txns that were commited in this partition
	commitChan       chan TMCommitInfo //channel used to inform the TM of a commit ending.
	db               map[uint64]VersionManager
	pendingOps       map[TransactionId][]*UpdateObjectParams //prepared transactions waiting for a commit to be issued
	pendingOpsRemote map[TransactionId][]*UpdateObjectParams //prepared NuCRDT transactions for other replicas (not to be applied locally)
	commitsOnHold    commitHeap                              //commited transactions that couldn't be applied yet due to other prepared (not commited) clocks
	debugPart
	testState crdt.State //TODO: Undo
}

//Manages the multiple clocks of the partition
type partitionClock struct {
	replicaID  int16
	lastUsedTs int64             //Last value generated by this partition for this replica with time.Now().UnixNano(). It helps ensure there are no two prepares with same clock.
	stableClk  clocksi.Timestamp //The latest clock for all the objects. Reads can be applied on this version or any prior. Do not pass this clock outside of the partition.
	prepClks   prepClockHeap     //Sorted (smallest to highest) list of prepared clocks.
}

type TxnIdTsPair struct {
	ts    int64
	txnId TransactionId
}

type TimestampPartIdPair struct {
	clocksi.Timestamp
	partID int64
}

type prepClockHeap struct {
	entries  []TxnIdTsPair
	nEntries *int
	innerMap map[TransactionId]int //Helper map; for efficiency keeps a mapping of txnId -> pos in heap
}

//Pop() must give the lowest clock.
type commitHeap struct {
	entries  []MatCommitArgs
	nEntries *int
}

//Holds data used in debug mode
type debugPart struct {
	hashToParams map[uint64]KeyParams //Keeps a track of all hash -> KeyParams seen by this partition
}

/////*****************TYPE METHODS***********************/////

//Data structures
//PrepClockHeap

func (c prepClockHeap) Len() int { return *c.nEntries }

//We want the heap's Pop() to return the lowest element, so we use < on "less". The smallest element is on h[0].
func (c prepClockHeap) Less(i, j int) bool {
	return c.entries[j].ts > c.entries[i].ts
}

func (c prepClockHeap) Swap(i, j int) {
	c.entries[i], c.entries[j] = c.entries[j], c.entries[i]
	c.innerMap[c.entries[i].txnId], c.innerMap[c.entries[j].txnId] = i, j
}

func (c *prepClockHeap) Push(value interface{}) {
	convValue := value.(TxnIdTsPair)
	c.innerMap[convValue.txnId] = *c.nEntries
	if *c.nEntries == cap(c.entries) {
		c.entries = append(c.entries, convValue)
		c.entries = c.entries[:cap(c.entries)] //Extending to capacity
		*c.nEntries += 1
	} else {
		c.entries[*c.nEntries], *c.nEntries = convValue, *c.nEntries+1
	}

}

func (c prepClockHeap) Pop() interface{} {
	if *c.nEntries == 0 {
		return nil
	}
	old := c.entries[*c.nEntries-1]
	c.entries[*c.nEntries-1] = TxnIdTsPair{}
	*c.nEntries--
	delete(c.innerMap, old.txnId)
	return old
}

//Returns the lowest value, but does not remove it.
func (c prepClockHeap) PeekMin() TxnIdTsPair {
	if *c.nEntries == 0 {
		return TxnIdTsPair{}
	}
	return c.entries[0]
}

//Returns the highest value, but does not remove it.
func (c prepClockHeap) PeekMax() TxnIdTsPair {
	if *c.nEntries == 0 {
		return TxnIdTsPair{}
	}
	return c.entries[*c.nEntries-1]
}

//CommitHeap

func (c commitHeap) Len() int { return *c.nEntries }

//We want the heap's Pop() to return the lowest element, so we use < on "less". The smallest element is on h[0].
func (c commitHeap) Less(i, j int) bool {
	return c.entries[j].CommitTs > c.entries[i].CommitTs
}

func (c commitHeap) Swap(i, j int) {
	c.entries[i], c.entries[j] = c.entries[j], c.entries[i]
	//c.entries[i].pos, c.entries[j].pos = i, j
}

func (c *commitHeap) Push(value interface{}) {
	convValue := value.(MatCommitArgs)
	//convValue.pos = *c.nEntries
	if *c.nEntries == cap(c.entries) {
		c.entries = append(c.entries, convValue)
		c.entries = c.entries[:cap(c.entries)] //Extending to capacity
		*c.nEntries += 1
	} else {
		c.entries[*c.nEntries], *c.nEntries = convValue, *c.nEntries+1
	}

}

func (c commitHeap) Pop() interface{} {
	if *c.nEntries == 0 {
		return nil
	}
	old := c.entries[*c.nEntries-1]
	//Cleaning the clock for GC purposes
	//c.entries[*c.nEntries-1].CommitTs = nil
	*c.nEntries--
	return old
}

//Returns the lowest value, but does not remove it.
func (c commitHeap) PeekMin() MatCommitArgs {
	if *c.nEntries == 0 {
		return MatCommitArgs{}
	}
	return c.entries[0]
}

//Returns the highest value, but does not remove it.
func (c commitHeap) PeekMax() MatCommitArgs {
	if *c.nEntries == 0 {
		return MatCommitArgs{}
	}
	return c.entries[*c.nEntries-1]
}

//

func (args MatStaticReadArgs) getRequestType() (requestType MatRequestType) {
	return readStaticMatRequest
}

func (args MatStaticReadArgs) getChannel() (channelId uint64) {
	return GetChannelKey(args.KeyParams)
}

func (args MatStaticUpdateArgs) getRequestType() (requestType MatRequestType) {
	return writeStaticMatRequest
}
func (args MatStaticUpdateArgs) getChannel() (channelId uint64) {
	return GetChannelKey(args.Updates[0].KeyParams)
}

func (args MatReadArgs) getRequestType() (requestType MatRequestType) {
	return readMatRequest
}
func (args MatReadArgs) getChannel() (channelId uint64) {
	return GetChannelKey(args.KeyParams)
}

func (args MatUpdateArgs) getRequestType() (requestType MatRequestType) {
	return writeMatRequest
}
func (args MatUpdateArgs) getChannel() (channelId uint64) {
	return GetChannelKey(args.Updates[0].KeyParams)
}

// /*
// func (args MatVersionArgs) getRequestType() (requestType MatRequestType) {
// 	return versionMatRequest
// }
// func (args MatVersionArgs) getChannel() (channelId uint64) {
// 	return args.ChannelId
// }*/

func (args MatCommitArgs) getRequestType() (requestType MatRequestType) {
	return commitMatRequest
}

func (args MatCommitArgs) getChannel() (channelId uint64) {
	return 0 //When sending the commit the TM already knows the channel to send the request
}

func (args MatAbortArgs) getRequestType() (requestType MatRequestType) {
	return abortMatRequest
}

func (args MatAbortArgs) getChannel() (channelId uint64) {
	return 0 //When sending an abort the TM already knows the channel to send the request
}

func (args MatPrepareArgs) getRequestType() (requestType MatRequestType) {
	return prepareMatRequest
}

func (args MatPrepareArgs) getChannel() (channelId uint64) {
	return 0 //When sending a prepare the TM already knows the channel to send the request
}

func (args MatRemoteTxnArgs) getRequestType() (requestType MatRequestType) {
	return remoteTxnMatRequest
}

func (args MatRemoteTxnArgs) getChannel() (channelId uint64) {
	return 0 //When sending a remoteTxn request the TM sends directly to the correct materializer
}

func (args MatRemoteGroupTxnArgs) getRequestType() (requestType MatRequestType) {
	return remoteGroupTxnMatRequest
}

func (args MatRemoteGroupTxnArgs) getChannel() (channelId uint64) {
	return 0 //When sending a remoteTxn request the TM sends directly to the correct materializer
}

func (args MatClkPosUpdArgs) getRequestType() (requestType MatRequestType) {
	return clkPosUpdMatRequest
}

func (args MatClkPosUpdArgs) getChannel() (channelId uint64) {
	return 0 //matClkPosUpdRequest is always sent to all partitions
}

func (args MatSafeClkArgs) getRequestType() (requestType MatRequestType) {
	return safeClkMatRequest
}

func (args MatSafeClkArgs) getChannel() (channelId uint64) {
	return 0 //When sending a safeClk request the logger sends directly to the correct materializer
}

func (args MatPrepareForRemoteArgs) getRequestType() (requestType MatRequestType) {
	return prepareForRemoteRequest
}

func (args MatPrepareForRemoteArgs) getChannel() (channelId uint64) {
	return GetChannelKey(args.Updates[0].KeyParams)
}

func (args MatCommitedClkArgs) getRequestType() (requestType MatRequestType) {
	return commitedClkMatRequest
}

func (args MatCommitedClkArgs) getChannel() (channelId uint64) {
	return 0 //commitedClkMatRequest is always sent to all partitions
}

func (args MatGetSnapshotArgs) getRequestType() (requestType MatRequestType) {
	return getSnapshotMatRequest
}

func (args MatGetSnapshotArgs) getChannel() (channelId uint64) {
	return 0 //getSnapshotArgs is always sent to all partitions
}

func (args MatApplySnapshotArgs) getRequestType() (requestType MatRequestType) {
	return applySnapshotMatRequest
}

func (args MatApplySnapshotArgs) getChannel() (channelId uint64) {
	return 0 //ApplySnapshotArgs is sent directly to the right partition
}

func (args MatResetArgs) getRequestType() (requestType MatRequestType) {
	return resetMatRequest
}

func (args MatResetArgs) getChannel() (channelId uint64) {
	return 0 //MatReset is always sent to all partitions
}

func (args MatWaitForReplicasArgs) getRequestType() (requestType MatRequestType) {
	return waitForReplicasRequest
}

func (args MatWaitForReplicasArgs) getChannel() (channelId uint64) {
	return 0 //MatWaitForReplicas is always sent to all partitions
}

func (args MatGCArgs) getRequestType() (requestType MatRequestType) {
	return gcMatRequest
}

func (args MatGCArgs) getChannel() (channelId uint64) {
	return 0 //MatGCArgs is always sent to all partitions
}

// /////*****************CONSTANTS AND VARIABLES***********************/////

const (
	//Types of requests
	readStaticMatRequest     MatRequestType = 0
	writeStaticMatRequest    MatRequestType = 1
	readMatRequest           MatRequestType = 2
	writeMatRequest          MatRequestType = 3
	commitMatRequest         MatRequestType = 4
	abortMatRequest          MatRequestType = 5
	prepareMatRequest        MatRequestType = 6
	safeClkMatRequest        MatRequestType = 7
	remoteTxnMatRequest      MatRequestType = 8
	clkPosUpdMatRequest      MatRequestType = 9
	prepareForRemoteRequest  MatRequestType = 10
	commitedClkMatRequest    MatRequestType = 11
	getSnapshotMatRequest    MatRequestType = 12
	applySnapshotMatRequest  MatRequestType = 13
	resetMatRequest          MatRequestType = 14
	waitForReplicasRequest   MatRequestType = 15
	remoteGroupTxnMatRequest MatRequestType = 16
	gcMatRequest             MatRequestType = 17
	versionMatRequest        MatRequestType = 255
	//TODO: At the end, delete unused requests from above

	initialPrepSize = 100 //Initial size for the prepClocks heap
)

var (
	nGoRoutines               uint64 //Number of partitions of the materializer. Each partition has one goroutine
	requestQueueSize          int    //Size for the channel that handles requests
	heapSize                  = 50   //Initial size for the prepare and commit buffer heaps
	expectedNewDownstreamSize int    //Initial size for new downstream upds generated by non-uniform CRDTs when applying remote ops
	keyRangeSize              uint64 //Number of keys that each partition is responsible, except for the last one which might have a bit more.

	debugMode = false //In debug mode, extra variables are kept track of and sanity checks will be done periodically.
)

func InitializeMaterializer(replicaID int16, commitCh chan TMCommitInfo) (mat *Materializer,
	loggers []Logger) {

	//Loading configs
	nGoRoutines = uint64(tools.SharedConfig.GetIntConfig("nPartitions", 1))
	requestQueueSize = tools.SharedConfig.GetIntConfig("requestChannelSize", 50)
	expectedNewDownstreamSize = tools.SharedConfig.GetIntConfig("newDownstreamSize", 10)
	keyRangeSize = math.MaxUint64 / nGoRoutines

	mat = &Materializer{channels: make([]chan MaterializerRequest, nGoRoutines)}
	loggers = make([]Logger, nGoRoutines)
	var i uint64
	for i = 0; i < nGoRoutines; i++ {
		loggers[i] = &MemLogger{}
		loggers[i].Initialize(mat, i)
		//Channel to receive requests from TM. Has a buffer since requests may be received from multiple sources.
		mat.channels[i] = make(chan MaterializerRequest, requestQueueSize)
		go listenToTM(int64(i), loggers[i], replicaID, commitCh, mat)
	}

	return
}

func listenToTM(id int64, logger Logger, replicaID int16, commitCh chan TMCommitInfo, mat *Materializer) {
	//Each goroutine is responsible for the range of keys [keyRangeSize * id, keyRangeSize * (id + 1)[
	//Where keyRangeSize = math.MaxUint64 / number of goroutines

	part := &partition{
		id:               id,
		log:              logger,
		commitChan:       commitCh,
		db:               make(map[uint64]VersionManager),
		pendingOps:       make(map[TransactionId][]*UpdateObjectParams),
		pendingOpsRemote: make(map[TransactionId][]*UpdateObjectParams),
		commitsOnHold:    commitHeap{entries: make([]MatCommitArgs, heapSize), nEntries: new(int)},
	}
	channel := mat.channels[id]

	waitForReplicas(channel)
	part.initializePartitionClock(replicaID)

	if debugMode {
		part.hashToParams = make(map[uint64]KeyParams)
		go part.sanityCheck()
	}

	var request MaterializerRequest
	for {
		request = <-channel
		part.handleRequest(request)
	}
}

//Makes the partition wait for the rest of the system to start before processing requests.
func waitForReplicas(channel chan MaterializerRequest) {
	for {
		msg := <-channel
		if msg.getRequestType() == waitForReplicasRequest {
			break
		}
	}
}

func (part *partition) initializePartitionClock(replicaID int16) {
	part.partitionClock = partitionClock{
		replicaID:  replicaID,
		lastUsedTs: 0,
		stableClk:  clocksi.NewClockSiTimestamp(),
		prepClks:   prepClockHeap{entries: make([]TxnIdTsPair, heapSize), nEntries: new(int), innerMap: make(map[TransactionId]int)},
	}
}

func (part *partition) handleRequest(request MaterializerRequest) {
	switch request.getRequestType() {
	case readStaticMatRequest:
		part.handleMatStaticRead(request.MatRequestArgs.(MatStaticReadArgs))
	case readMatRequest:
		part.handleMatRead(request.MatRequestArgs.(MatReadArgs))
	case writeStaticMatRequest:
		part.handleMatStaticWrite(request.MatRequestArgs.(MatStaticUpdateArgs))
	case writeMatRequest:
		part.handleMatWrite(request.MatRequestArgs.(MatUpdateArgs))
	case commitMatRequest:
		part.handleMatCommit(request.MatRequestArgs.(MatCommitArgs))
	case prepareMatRequest:
		part.handleMatPrepare(request.MatRequestArgs.(MatPrepareArgs))
	case abortMatRequest:
		part.handleMatAbort(request.MatRequestArgs.(MatAbortArgs))
	case safeClkMatRequest:
		part.handleMatSafeClk(request.MatRequestArgs.(MatSafeClkArgs))
	case remoteTxnMatRequest:
		part.handleMatRemoteTxn(request.MatRequestArgs.(MatRemoteTxnArgs))
	case clkPosUpdMatRequest:
		part.handleMatClkPosUpd(request.MatRequestArgs.(MatClkPosUpdArgs))
	case remoteGroupTxnMatRequest:
		part.handleMatRemoteGroupTxn(request.MatRequestArgs.(MatRemoteGroupTxnArgs))
	case prepareForRemoteRequest:
		part.handleMatPrepareRemote(request.MatRequestArgs.(MatPrepareForRemoteArgs))
		//For Replicator's join
	case commitedClkMatRequest:
		part.handleMatCommitedClk(request.MatRequestArgs.(MatCommitedClkArgs))
	case resetMatRequest:
		part.handleMatReset(request.MatRequestArgs.(MatResetArgs))
		//TODO: snapshots, version, etc.
	case gcMatRequest:
		part.handleMatGC(request.MatRequestArgs.(MatGCArgs))
	default:
		fmt.Println("[WARNING][MAT]Unknown request type: ", request.getRequestType())
	}
}

func (part *partition) handleMatRead(args MatReadArgs) {
	readLatest := part.canReadLatest(args.Timestamp)
	obj := part.getObject(args.KeyParams, args.Timestamp)
	objPending := part.getObjPendingOps(args.KeyParams, args.TransactionId)
	args.ReplyChan <- part.getState(readLatest, obj, args.ReadArgs, args.Timestamp, objPending)
}

func (part *partition) handleMatStaticRead(args MatStaticReadArgs) {
	//Static read does not need to check for pending ops
	readLatest := part.canReadLatest(args.Timestamp)
	obj := part.getObject(args.KeyParams, args.Timestamp)
	args.ReplyChan <- part.getState(readLatest, obj, args.ReadArgs, args.Timestamp, nil)
}

//Pre-condition: Assumes a read is not for the future - either present or past.
//This implies the TM must ensure a read is not sent to materializer before all partition's clock has caught up
//This should be easy to guarantee by only updating the TM clock after all partitions have confirmed the clock.
//A possible problem may be when applying remote transactions - this may be slow when many remote transactions are received.
func (part *partition) canReadLatest(readTs clocksi.Timestamp) bool {
	if shared.IsReadWaitingDisabled {
		return true
	}
	//If it is lower, a read in the past is issued (i.e., not readLatest)
	return !readTs.IsLower(part.stableClk)
}

func (part *partition) getState(readLatest bool, obj VersionManager, readArgs crdt.ReadArguments,
	clk clocksi.Timestamp, objPending []*crdt.UpdateArguments) crdt.State {
	if readLatest {
		//fmt.Println("[MAT]Reading latest version", readTs)
		return obj.ReadLatest(readArgs, objPending)
	} else {
		//fmt.Println("[MAT]Reading old version", readTs)
		return obj.ReadOld(readArgs, clk, objPending)
	}
}

func (part *partition) handleMatPrepare(args MatPrepareArgs) {
	args.ReplyChan <- part.prepareTs(args.TransactionId)
}

func (part *partition) handleMatWrite(args MatUpdateArgs) {
	part.pendingOps[args.TransactionId] = append(part.pendingOps[args.TransactionId], args.Updates...)
	part.debugLogUpdParams(args.Updates)
}

func (part *partition) handleMatStaticWrite(args MatStaticUpdateArgs) {
	newTs := part.prepareTs(args.TransactionId)
	part.pendingOps[args.TransactionId] = args.Updates
	args.ReplyChan <- TsClkPair{Clk: part.partitionClock.stableClk.Copy(), Ts: newTs}
	part.debugLogUpdParams(args.Updates)
}

func (part *partition) handleMatAbort(args MatAbortArgs) {
	delete(part.pendingOps, args.TransactionId)
}

func (part *partition) handleMatCommit(args MatCommitArgs) {
	lowestPrep := part.getLowestPrepare()
	part.removePrepare(args.TransactionId) //We always remove this prepare, even if the commit is put on hold.
	if part.canCommit(args) {
		//Safe to commit
		//fmt.Println("[MAT]Apply commit")
		part.applyCommit(args)
	} else {
		//fmt.Println("[MAT]Hold commit", args)
		part.holdCommit(args)
	}
	if lowestPrep.txnId == args.TransactionId {
		//Was the lowest prepare, so some commit on hold may be appliable now - even if this transaction's commit was put on hold.
		part.checkHoldCommits()
	}
}

func (part *partition) applyCommit(args MatCommitArgs) {
	//fmt.Println("[MAT]Commit")
	ops, has := part.pendingOps[args.TransactionId]
	var downstreamUpds []*UpdateObjectParams
	if has { //Normal commit
		//fmt.Println("[MAT]Commiting")
		downstreamUpds = part.applyUpdates(ops, args.ReadClk, args.CommitTs)
	} else { //Commit only for remote (NuCRDTs)
		//fmt.Println("[MAT]Commiting for remote", args)
		downstreamUpds = part.pendingOpsRemote[args.TransactionId]
	}
	part.updatePartitionWithCommit(args, downstreamUpds)
	part.commitChan <- TMPartCommitReply{txnId: args.TransactionId}
}

func (part *partition) holdCommit(args MatCommitArgs) {
	heap.Push(&part.commitsOnHold, args)
	//fmt.Println("[MAT]Heap min after pushing:", part.commitsOnHold.PeekMin(), ". Len:", part.commitsOnHold.Len())
}

//Note: this function also removes args.TxnId from the list of prepares
func (part *partition) canCommit(args MatCommitArgs) (canCommit bool) {
	if part.prepClks.Len() == 0 {
		if part.commitsOnHold.Len() == 0 {
			return true //No prep or commit on hold, can commit for sure.
		}
		//No prep. Need to still check the lowest commit however.
		return part.commitsOnHold.PeekMin().CommitTs > args.CommitTs //if minCommitHold > args.CommitTs -> safe to commit. Else, can't.
	}
	//If the lowest prepare is lower than args.CommitTs, we can't commit for sure.
	if part.prepClks.PeekMin().ts < args.CommitTs {
		return false
	}
	//The lowest prepare is higher than args.CommitTs. If the lowest commit on hold is also higher (or no commit on hold), then we can commit. Otherwise, can't.
	return part.commitsOnHold.Len() == 0 || part.commitsOnHold.PeekMin().CommitTs > args.CommitTs
}

//downstreamUpds: key, CRDTType, bucket + downstream arguments of each update. This is what is sent to other replicas for them to apply the txn.
func (part *partition) applyUpdates(updates []*UpdateObjectParams, readClk clocksi.Timestamp, commitTs int64) (downstreamUpds []*UpdateObjectParams) {
	downstreamUpds = make([]*UpdateObjectParams, len(updates))
	i := 0
	var hashKey uint64
	var obj VersionManager

	commitClk := readClk.Copy().UpdatePos(part.replicaID, commitTs)
	for _, upd := range updates {
		hashKey = getHash(getCombinedKey(upd.KeyParams))

		//If it's a reset, delete the CRDT, generate the downstream and continue to the next upd.
		//Note that reads may recreate the CRDT, but with an empty state.
		if (*upd.UpdateArgs == crdt.ResetOp{}) {
			downstreamUpds[i] = upd
			i++
			delete(part.db, hashKey)
			//fmt.Println("Reset Op, continuing.")
			continue
		}

		obj = part.getObjectForUpd(hashKey, upd.CrdtType, commitClk)
		//Due to non-uniform CRDTs, the downstream args might be noop (op with no effect/doesn't need to be propagated yet)
		//Some "normal" CRDTs have also be optimized to do the same (e.g., setAWCrdt when removing element that doesn't exist)
		downArgs := obj.Update(*upd.UpdateArgs)
		if (downArgs != nil && downArgs != crdt.NoOp{}) {
			//fmt.Printf("[MAT]Applying downstream update to %v with args %T %+v\n", upd.KeyParams, *upd.UpdateArgs, *upd.UpdateArgs)
			obj.Downstream(commitClk, downArgs)
			if downArgs.MustReplicate() {
				updArgs := downArgs.(crdt.UpdateArguments)
				downstreamUpds[i] = &UpdateObjectParams{
					KeyParams:  upd.KeyParams,
					UpdateArgs: &updArgs,
				}
				i++
			}
		}
	}
	//Hides positions that are empty due to non-uniform CRDTs
	return downstreamUpds[:i]
}

func (part *partition) updatePartitionWithCommit(args MatCommitArgs, downUpds []*UpdateObjectParams) {
	part.updateClockWithCommit(args.CommitTs)

	/*downUpdsS := "["
	for _, upd := range part.pendingOps[args.TransactionId] {
		downUpdsS += fmt.Sprintf("(%s,%s,%s,%T):%+v; ", upd.KeyParams.Key, upd.KeyParams.Bucket,
			proto.CRDTType_name[int32(upd.KeyParams.CrdtType)], *upd.UpdateArgs, *upd.UpdateArgs)
	}
	downUpdsS += "]"*/
	//No need to log if there's no downstream effects (can happen due to NuCRDTs or, e.g., set remove without element existing)
	if len(downUpds) > 0 {
		//fmt.Printf("[MAT]Sending logger request. (original upds: %s)\n", downUpdsS)
		part.log.SendLoggerRequest(LoggerRequest{
			LogRequestArgs: LogCommitArgs{
				ReadClk:  args.ReadClk,
				CommitTs: args.CommitTs,
				Upds:     downUpds,
			},
		})
	} else {
		//fmt.Printf("[MAT]Not sending logger request: no downUpds!!! (original upds: %s)\n", downUpdsS)
	}
	delete(part.pendingOps, args.TransactionId)
	delete(part.pendingOpsRemote, args.TransactionId)
}

//Cyclic. It will keep commiting operations until none are left.
func (part *partition) checkHoldCommits() {
	if part.commitsOnHold.Len() == 0 {
		//fmt.Println("[MAT][CHC]Commit hold empty, returning")
		return
	}
	if part.prepClks.Len() == 0 {
		//fmt.Println("[MAT][CHC]Prep clocks empty, applying all commits")
		//A commit got applied, no prepare left. Can safely apply all hold commits.
		for part.commitsOnHold.Len() > 0 {
			part.applyCommit(heap.Pop(&part.commitsOnHold).(MatCommitArgs))
		}
		return
	}
	//fmt.Println("[MAT]Checking commits, neither are empty")
	minCommit := part.commitsOnHold.PeekMin()
	minPrep := part.prepClks.PeekMin().ts
	//fmt.Printf("[MAT][CHC]Hold size: %d. MinCommitTs: %s. MinPrepTs: %s\n",
	//part.commitsOnHold.Len(), minCommit.CommitTs.ToSortedString(), minPrep.ToSortedString())
	for minCommit.CommitTs < minPrep {
		//fmt.Println("[MAT][CHC]Applying commits")
		//Can commit. Can keep commiting as long as < minPrep
		heap.Pop(&part.commitsOnHold)
		part.applyCommit(minCommit)
		if part.commitsOnHold.Len() == 0 {
			//All commits applied, stop
			break
		}
		minCommit = part.commitsOnHold.PeekMin()
	}
	//fmt.Printf("[MAT][CHC]Hold size after attempting to apply commits on hold: %d\n", part.commitsOnHold.Len())
}

func (part *partition) handleMatRemoteTxn(args MatRemoteTxnArgs) {
	//fmt.Println("[MAT]RemoteTxn. Len: ", len(args.Upds))
	newDown := make([]*UpdateObjectParams, 0, tools.MinInt(expectedNewDownstreamSize, len(args.Upds)))
	newDown = part.applyRemoteUpds(args.MatRemoteTxn, newDown)
	part.updateClockWithRemoteTS(args.ReplicaID, args.CommitTs)
	//part.updateClockWithCommit(args.Timestamp) //In theory an UpdatePos should be enough?
	args.ReplyChan <- newDown
	//fmt.Println("[MAT]Replied from remoteTxn")
}

func (part *partition) handleMatClkPosUpd(args MatClkPosUpdArgs) {
	part.updateClockWithRemoteTS(args.ReplicaID, args.StableTs)
	args.ReplyChan <- true
}

func (part *partition) handleMatRemoteGroupTxn(args MatRemoteGroupTxnArgs) {
	//fmt.Println("[MAT]RemoteGroupTxn")
	newDown := make([]*UpdateObjectParams, 0, expectedNewDownstreamSize*(1+len(args.Txns)/5))
	for _, txn := range args.Txns {
		newDown = part.applyRemoteUpds(txn, newDown)
		//Need to update clock always as a group may contain txns from multiple servers.
		part.updateClockWithRemoteTS(txn.ReplicaID, txn.CommitTs)
	}
	//part.updateClockWithCommit(args.FinalClk)
	//fmt.Printf("[MAT]RemoteGroupTxnEnd (nTxns: %d)\n", len(args.Txns))
	args.ReplyChan <- newDown
}

func (part *partition) applyRemoteUpds(args MatRemoteTxn, newDownstream []*UpdateObjectParams) (newDown []*UpdateObjectParams) {
	//newDownstream = make([]*UpdateObjectParams, 0, tools.MinInt(expectedNewDownstreamSize, len(args.Upds)))
	var hashKey uint64
	var obj VersionManager
	//Should here the original read clock be used or the current partition's clock?
	commitClk := args.TMClock.Copy().UpdatePos(args.ReplicaID, args.CommitTs)
	for _, upd := range args.Upds {
		hashKey = getHash(getCombinedKey(upd.KeyParams))
		obj = part.getObjectForUpd(hashKey, upd.CrdtType, commitClk)
		//TODO: Update this in order to avoid forced cast
		var generatedDownstream crdt.UpdateArguments = obj.Downstream(commitClk, (*upd.UpdateArgs).(crdt.DownstreamArguments))
		//non-uniform CRDTs may generate downstream updates when applying remote ops. We'll "commit" those upds with the stable clock
		if generatedDownstream != nil {
			newDownstream = append(newDownstream, &UpdateObjectParams{KeyParams: upd.KeyParams, UpdateArgs: &generatedDownstream})
		}
	}
	part.debugLogUpdParams(args.Upds)
	return newDownstream
}

func (part *partition) handleMatPrepareRemote(args MatPrepareForRemoteArgs) {
	part.pendingOpsRemote[args.TransactionId] = args.Updates
	newTs := part.prepareTs(args.TransactionId)
	args.ReplyChan <- TsClkPair{Clk: part.partitionClock.stableClk.Copy(), Ts: newTs}
}

//Tells the partition's logger what is the latest safe clock that can be sent.
func (part *partition) handleMatSafeClk(args MatSafeClkArgs) {
	if part.prepClks.Len() == 0 {
		//args.ReplyChan <- part.stableClk.NextTimestamp(part.replicaID) //Will send the latest timestamp
		part.log.SendLoggerRequest(LoggerRequest{LogRequestArgs: LogClockArgs{SafeTs: clocksi.GetSystemCurrTime()}})
	} else {
		//safeClk := part.prepClks.PeekMin().clk.Copy()
		//safeClk.UpdateForcedPos(part.replicaID, safeClk.GetPos(part.replicaID)-1) //Subtracting 1 for safety: no commit will be < than minimum prepare.
		//args.ReplyChan <- safeClk
		part.log.SendLoggerRequest(LoggerRequest{LogRequestArgs: LogClockArgs{SafeTs: part.prepClks.PeekMin().ts - 1}}) //Subtracting 1 for safety: no commit will be < than minimum prepare.
	}
}

//For replicator's join.
func (part *partition) handleMatCommitedClk(args MatCommitedClkArgs) {
	args.ReplyChan <- TimestampPartIdPair{Timestamp: part.stableClk.Copy(), partID: part.id}
}

func (part *partition) handleMatReset(args MatResetArgs) {
	part.log.Reset()
	part.partitionClock.reset()
	part.db = make(map[uint64]VersionManager)
	part.pendingOps = make(map[TransactionId][]*UpdateObjectParams)
	part.commitsOnHold = commitHeap{entries: make([]MatCommitArgs, heapSize), nEntries: new(int)}
}

func (part *partition) handleMatGC(args MatGCArgs) {
	clkKey := args.SafeClk.GetMapKey()
	for _, obj := range part.db {
		obj.GC(args.SafeClk, clkKey)
	}
}

//Generic helper functions

func (part *partition) getObject(keyParams KeyParams, clk clocksi.Timestamp) VersionManager {
	hashKey := getHash(getCombinedKey(keyParams))
	obj, hasKey := part.db[hashKey]
	if !hasKey {
		obj = part.initializeVersionManager(keyParams.CrdtType, clk)
	}
	return obj
}

//Also stores the object if non-existent
func (part *partition) getObjectForUpd(hashKey uint64, crdtType proto.CRDTType, clk clocksi.Timestamp) VersionManager {
	obj, hasKey := part.db[hashKey]
	if !hasKey {
		obj = part.initializeVersionManager(crdtType, clk)
		part.db[hashKey] = obj
	}
	return obj
}

//Will be nil if there is no pending ops
func (part *partition) getObjPendingOps(keyParams KeyParams, txnId TransactionId) (objPending []*crdt.UpdateArguments) {
	pendingOps, hasPending := part.pendingOps[txnId]
	if hasPending {
		objPending = make([]*crdt.UpdateArguments, 0, len(pendingOps))
		for _, upd := range pendingOps {
			if upd.Key == keyParams.Key && upd.Bucket == keyParams.Bucket && upd.CrdtType == keyParams.CrdtType {
				objPending = append(objPending, upd.UpdateArgs)
			}
		}
	}
	//It is okay to return nil if there is no pending
	return
}

func (part *partition) initializeVersionManager(crdtType proto.CRDTType, clk clocksi.Timestamp) (newVM VersionManager) {
	crdt := part.initializeCrdt(crdtType)
	//Variable defined in versionManager.go
	return BaseVM.Initialize(crdt, clk)
}

func (part *partition) initializeCrdt(crdtType proto.CRDTType) (newCrdt crdt.CRDT) {
	return crdt.InitializeCrdt(crdtType, part.replicaID)
}

/*
Called by the TransactionManager or any other entity that may want to communicate with the Materializer
and doesn't yet know the appropriate channel
*/
func (mat *Materializer) SendRequest(request MaterializerRequest) {
	mat.channels[request.getChannel()] <- request
}

/*
Called by the TransactionManager or any other entity that may want to communicate with the Materializer
when they know the appropriate channel. Avoids computing an extra hash.
*/
func (mat *Materializer) SendRequestToChannel(request MaterializerRequest, channelKey uint64) {
	mat.channels[channelKey] <- request
}

func (mat *Materializer) SendRequestToChannels(request MaterializerRequest, channelsToSend ...chan MaterializerRequest) {
	for _, channel := range channelsToSend {
		//Copy the request to avoid concurrent access problems
		newReq := request
		channel <- newReq
	}
}

func (mat *Materializer) SendRequestToAllChannels(request MaterializerRequest) {
	for _, channel := range mat.channels {
		//Copy the request to avoid concurrent access problems
		newReq := request
		channel <- newReq
	}
}

func GetChannelKey(keyParams KeyParams) (key uint64) {
	hash := getHash(getCombinedKey(keyParams))
	key = hash / keyRangeSize
	//Overflow, which might happen due to rounding
	if key == nGoRoutines {
		key -= 1
	}
	return
}

func getCombinedKey(keyParams KeyParams) (combKey string) {
	combKey = keyParams.Bucket + keyParams.CrdtType.String() + keyParams.Key
	return
}

func getHash(combKey string) (hash uint64) {
	hash = hashFunc.StringSum64(combKey)
	return
}

//PartitionClock functions

/*
func (partClock *partitionClock) prepareClk(txnId TransactionId) (preparedClk clocksi.Timestamp) {
	//Clocks are monotonically increasing (they are based on real time clock). Can safely use get next timestamp from stableClk
	newClk := partClock.getNextClk(partClock.stableClk)
	//fmt.Println("[MAT]Pushing clock:", TxnIdTsPair{clk: newClk, txnId: txnId})
	heap.Push(&partClock.prepClks, TxnIdTsPair{clk: newClk, txnId: txnId})
	//partClock.prepClks.innerMap[txnId] = partClock.prepClks.Len() - 1 //It always goes to the last position
	//min := partClock.prepClks.PeekMin()
	//fmt.Println("[MAT][PrepareClock]Min clk", min)
	//fmt.Println()
	return newClk
}

func (partClock *partitionClock) getNextClk(baseClk clocksi.Timestamp) (newClk clocksi.Timestamp) {
	newClk = baseClk.NextTimestamp(partClock.replicaID)
	//Ensure the clock is bigger. It may be the same as some previously generated clk if two are generated too quickly
	//Note: the clocksi implementation provides a solution for this, but only works when generating a new TS using as base the last one used.
	newReplicaTS := newClk.GetPos(partClock.replicaID)
	if newReplicaTS <= partClock.lastUsedTs { //<=, in the (extremely rare) occasion three timestamps in a row are generated with same value
		partClock.lastUsedTs++
		newClk.UpdatePos(partClock.replicaID, partClock.lastUsedTs)
	} else {
		partClock.lastUsedTs = newReplicaTS
	}
	return
}
*/

func (partClock *partitionClock) prepareTs(txnId TransactionId) (preparedTs int64) {
	newTs := partClock.getNextTs()
	heap.Push(&partClock.prepClks, TxnIdTsPair{ts: newTs, txnId: txnId})
	return newTs
}

func (partClock *partitionClock) getNextTs() (newTs int64) {
	newTs = time.Now().UTC().UnixNano()
	//It's possible the new TS is the same as the previous one in case two TS are generated too quickly
	if newTs <= partClock.lastUsedTs { //<=, in the (extremely rare) occasion three timestamps in a row are generated with same value
		partClock.lastUsedTs++
		newTs = partClock.lastUsedTs
	} else {
		partClock.lastUsedTs = newTs
	}
	return
}

func (partClock *partitionClock) getLowestPrepare() (pair TxnIdTsPair) {
	return partClock.prepClks.PeekMin()
}

func (partClock *partitionClock) removePrepare(txnId TransactionId) {
	pos := partClock.prepClks.innerMap[txnId]
	//fmt.Println("[MAT]Removing prepare at pos:", pos, ", prepClks has length:", partClock.prepClks.Len())
	heap.Remove(&partClock.prepClks, pos)
	//delete(partClock.prepClks.innerMap, txnId)
}

func (partClock *partitionClock) removeLowestPrepare() {
	heap.Pop(&partClock.prepClks)
	//txnId := heap.Pop(&partClock.prepClks).(TxnIdTsPair).txnId
	//delete(partClock.prepClks.innerMap, txnId)
}

func (partClock *partitionClock) updateClockWithCommit(ts int64) {
	partClock.stableClk = partClock.stableClk.UpdatePos(partClock.replicaID, ts)
}

func (partClock *partitionClock) updateClockWithRemoteTS(remoteID int16, ts int64) {
	partClock.stableClk.UpdatePos(remoteID, ts)
}

func (partClock *partitionClock) reset() {
	partClock.lastUsedTs = 0
	partClock.stableClk = clocksi.NewClockSiTimestamp()
	partClock.prepClks = prepClockHeap{entries: make([]TxnIdTsPair, heapSize), nEntries: new(int), innerMap: make(map[TransactionId]int)}
}

//Debug methods

func (part *partition) debugLogUpdParams(upds []*UpdateObjectParams) {
	if debugMode {
		for _, upd := range upds {
			part.hashToParams[getHash(getCombinedKey(upd.KeyParams))] = upd.KeyParams
		}
	}
}

//Checks if all the queues are empty. The output is only relevant if the partition is inactive (i.e., not actively processing requests)
func (part partition) sanityCheck() {
	time.Sleep(50 * time.Second)
	fmt.Println("[MAT]Clock:", part.stableClk.ToSortedString())
	err := false
	if part.prepClks.Len() > 0 {
		fmt.Printf("[MAT]Prep clocks is not empty!!! PrepClocks: %v\n", part.prepClks.entries)
		err = true
	}
	if part.commitsOnHold.Len() > 0 {
		fmt.Printf("[MAT]Commits on hold is not empty!!! CommitsOnHold: %v\n", part.commitsOnHold.entries)
		err = true
	}
	if len(part.pendingOps) > 0 {
		fmt.Printf("[MAT]PendingOps is not empty!!! PendingOps: %v\n", part.pendingOps)
		err = true
	}
	if err {
		return
	}

	//Check the bucket of "INDEX"
	var sb strings.Builder
	//TODO: Sort keys
	sortedKeys := make([]uint64, len(part.hashToParams))
	i := 0
	for hash, keyParams := range part.hashToParams {
		if keyParams.Bucket == "INDEX" {
			sortedKeys[i] = hash
			i++
		}
	}
	sortedKeys = sortedKeys[:i]
	sort.Slice(sortedKeys, func(i, j int) bool { return sortedKeys[i] < sortedKeys[j] })
	for _, hash := range sortedKeys {
		obj, keyParams := part.db[hash], part.hashToParams[hash]
		fmt.Fprintf(&sb, "[%s,%s,%s]:", keyParams.Key, keyParams.Bucket, proto.CRDTType_name[int32(keyParams.CrdtType)])
		fmt.Fprintf(&sb, "%v\n", obj.ReadLatest(crdt.StateReadArguments{}, nil))
	}
	fmt.Print(sb.String())

}
