package antidote

import (
	"clocksi"
	"crdt"
	fmt "fmt"
	math "math"
	"os"
	"proto"
	"tools"

	hashFunc "github.com/twmb/murmur3"
)

/////*****************TYPE DEFINITIONS***********************/////
//////////************Requests**************************//////////

type MaterializerRequest struct {
	MatRequestArgs
}

type MatRequestArgs interface {
	getRequestType() (requestType MatRequestType)
	getChannel() (channelId uint64)
}

//Args for latest stable version request. This won't be necessary if we remove findCommonTimestamp from transactionManager
type MatVersionArgs struct {
	ChannelId uint64
	ReplyChan chan clocksi.Timestamp
}

type MatReadCommonArgs struct {
	ReadObjectParams
	clocksi.Timestamp
	ReplyChan chan crdt.State
}

//Args for read request
type MatReadArgs struct {
	MatReadCommonArgs
	TransactionId
}

//Args for update request. Note that unlike with MatReadArgs, a MatUpdateArgs represents multiple updates, but all for the same partition
type MatUpdateArgs struct {
	Updates []*UpdateObjectParams
	TransactionId
	ReplyChan chan BoolErrorPair
}

type MatStaticReadArgs struct {
	MatReadCommonArgs
}

type MatStaticUpdateArgs struct {
	Updates []*UpdateObjectParams
	TransactionId
	ReplyChan chan TimestampErrorPair
}

type MatCommitArgs struct {
	TransactionId   TransactionId
	CommitTimestamp clocksi.Timestamp
}

type MatAbortArgs struct {
	TransactionId TransactionId
}

type MatPrepareArgs struct {
	TransactionId TransactionId
	ReplyChan     chan clocksi.Timestamp
}

type MatRemoteTxnArgs struct {
	ReplicaID int16
	clocksi.Timestamp
	Upds []*UpdateObjectParams
}

//Used by the logging layer (to then pass to the replication layer) to know the most recent clock that is "safe", i.e., for which there will be no commit for sure.
type MatSafeClkArgs struct {
	ReplyChan chan clocksi.Timestamp
}

type MatClkPosUpdArgs struct {
	ReplicaID int16
	StableTs  int64
}

//Used for preparing operations generated by applying downstream remotes
type MatPrepareForRemoteArgs struct {
	Updates []*UpdateObjectParams
	TransactionId
	ReplyChan chan clocksi.Timestamp
}

type PendingReads struct {
	TransactionId
	clocksi.Timestamp
	Reads []*MatReadCommonArgs
}

type MatRequestType byte

//////////********************Other types************************//////////
//Struct that represents local data to each goroutine/partition
type partitionData struct {
	//db                    map[uint64]crdt.CRDT //CRDT database of this partition
	db                    map[uint64]VersionManager
	stableVersion         clocksi.Timestamp //latest commited timestamp.
	twoSmallestPendingTxn [2]*TransactionId //Contains the two transactionIds that have been prepared with the smallest timestamps.
	//Idea: avoids the issue of the txn we're verying being the one with the lowest proposed timestamp (in this case, check the 2nd entry)
	highestPendingTs    clocksi.Timestamp                       //Contains the highest timestamp that was prepared. Used to check if a read can be executed or not.
	pendingOps          map[TransactionId][]*UpdateObjectParams //pending transactions waiting for commit
	suggestedTimestamps map[TransactionId]clocksi.Timestamp     //map of transactionId -> timestamp suggested on first write request for transactionId
	commitedWaitToApply map[TransactionId]clocksi.Timestamp     //map of transactionId -> commit timestamp of commited transactions that couldn't be applied due to pending versions
	//TODO: Choose a better option to hold pending reads? Checking the whole map takes a long time...
	pendingReads        map[clocksi.TimestampKey]*PendingReads //pending reads that require a more recent version than stableVersion
	log                 Logger                                 //logs the operations of txns that were commited in this partition
	remoteWaiting       map[int16][]PairClockUpdates           //remote transactions that are waiting for other remote transactions to be applied. Int: replicaID
	replicaID           int16
	partitionID         int64 //Useful for debugging
	downstreamOpsCh     chan TMDownstreamRemoteMsg
	pendingOpsForRemote map[TransactionId][]*UpdateObjectParams //pending transactions that are only to be applied in remote replicas
}

type BoolErrorPair struct {
	bool
	error
}

type TimestampErrorPair struct {
	clocksi.Timestamp
	error
}

type Materializer struct {
	//Each goroutine is responsible for a certain range of keys (with no intersection between ranges)
	//More precisely, a goroutine is responsible from its id * keyRangeSize (inclusive) to (id + 1) * keyRangeSize (exclusive)
	channels []chan MaterializerRequest
}

//////////*******************Error types***********************//////////
//TODO: Move this to crdt.go...?
type UnknownCrdtTypeError struct {
	proto.CRDTType
}

/////*****************CONSTANTS AND VARIABLES***********************/////

const (
	//Types of requests
	readStaticMatRequest    MatRequestType = 0
	writeStaticMatRequest   MatRequestType = 1
	readMatRequest          MatRequestType = 2
	writeMatRequest         MatRequestType = 3
	commitMatRequest        MatRequestType = 4
	abortMatRequest         MatRequestType = 5
	prepareMatRequest       MatRequestType = 6
	safeClkMatRequest       MatRequestType = 7
	remoteTxnMatRequest     MatRequestType = 8
	clkPosUpdMatRequest     MatRequestType = 9
	prepareForRemoteRequest MatRequestType = 10
	versionMatRequest       MatRequestType = 255

	//Number of goroutines in the pool to access the database. Each goroutine has a (automatically assigned) range of keys that it can access.
	//nGoRoutines               uint64 = 8
	nGoRoutines               uint64 = 1
	readQueueSize                    = 10 //Initial size of the read queue for pending reads (partitionData.pendingReads)
	requestQueueSize                 = 10 //Size for the channel that handles requests
	expectedNewDownstreamSize        = 10 //Initial size for new downstream upds generated by non-uniform CRDTs when applying remote ops
)

var (
	//uint64: result returned by the hash function
	keyRangeSize uint64 = math.MaxUint64 / nGoRoutines //Number of keys that each goroutine is responsible, except for the last one which might have a bit more.
)

/////*****************TYPE METHODS***********************/////

func (args MatStaticReadArgs) getRequestType() (requestType MatRequestType) {
	return readStaticMatRequest
}

func (args MatStaticReadArgs) getChannel() (channelId uint64) {
	return GetChannelKey(args.KeyParams)
}

func (args MatStaticUpdateArgs) getRequestType() (requestType MatRequestType) {
	return writeStaticMatRequest
}
func (args MatStaticUpdateArgs) getChannel() (channelId uint64) {
	return GetChannelKey(args.Updates[0].KeyParams)
}

func (args MatReadArgs) getRequestType() (requestType MatRequestType) {
	return readMatRequest
}
func (args MatReadArgs) getChannel() (channelId uint64) {
	return GetChannelKey(args.KeyParams)
}

func (args MatUpdateArgs) getRequestType() (requestType MatRequestType) {
	return writeMatRequest
}
func (args MatUpdateArgs) getChannel() (channelId uint64) {
	return GetChannelKey(args.Updates[0].KeyParams)
}

func (args MatVersionArgs) getRequestType() (requestType MatRequestType) {
	return versionMatRequest
}
func (args MatVersionArgs) getChannel() (channelId uint64) {
	return args.ChannelId
}

func (args MatCommitArgs) getRequestType() (requestType MatRequestType) {
	return commitMatRequest
}

func (args MatCommitArgs) getChannel() (channelId uint64) {
	return 0 //When sending the commit the TM already knows the channel to send the request
}

func (args MatAbortArgs) getRequestType() (requestType MatRequestType) {
	return abortMatRequest
}

func (args MatAbortArgs) getChannel() (channelId uint64) {
	return 0 //When sending an abort the TM already knows the channel to send the request
}

func (args MatPrepareArgs) getRequestType() (requestType MatRequestType) {
	return prepareMatRequest
}

func (args MatPrepareArgs) getChannel() (channelId uint64) {
	return 0 //When sending a prepare the TM already knows the channel to send the request
}

func (args MatSafeClkArgs) getRequestType() (requestType MatRequestType) {
	return safeClkMatRequest
}

func (args MatSafeClkArgs) getChannel() (channelId uint64) {
	return 0 //When sending a safeClk request the logger sends directly to the correct materializer
}

func (args MatRemoteTxnArgs) getRequestType() (requestType MatRequestType) {
	return remoteTxnMatRequest
}

func (args MatRemoteTxnArgs) getChannel() (channelId uint64) {
	return 0 //When sending a remoteTxn request the TM sends directly to the correct materializer
}

func (args MatClkPosUpdArgs) getRequestType() (requestType MatRequestType) {
	return clkPosUpdMatRequest
}

func (args MatClkPosUpdArgs) getChannel() (channelId uint64) {
	return 0 //matClkPosUpdRequest is always sent to all partitions
}

func (args MatPrepareForRemoteArgs) getRequestType() (requestType MatRequestType) {
	return prepareForRemoteRequest
}

func (args MatPrepareForRemoteArgs) getChannel() (channelId uint64) {
	return GetChannelKey(args.Updates[0].KeyParams)
}

func (err UnknownCrdtTypeError) Error() (errString string) {
	return fmt.Sprint("Unknown/unsupported CRDT type:", err.CRDTType)
}

/////*****************MATERIALIZER CODE***********************/////

//Starts listening goroutines and channels. Also starts each partition's logger and returns it
func InitializeMaterializer(replicaID int16, downstreamOpsCh chan TMDownstreamRemoteMsg) (mat *Materializer,
	loggers []Logger) {

	mat = &Materializer{
		channels: make([]chan MaterializerRequest, nGoRoutines),
	}
	loggers = make([]Logger, nGoRoutines)
	var i uint64
	for i = 0; i < nGoRoutines; i++ {
		loggers[i] = &MemLogger{}
		loggers[i].Initialize(mat, i)
		go listenForTransactionManagerRequests(i, loggers[i], replicaID, downstreamOpsCh, mat)
	}

	return
}

func listenForTransactionManagerRequests(id uint64, logger Logger, replicaID int16, downstreamOpsCh chan TMDownstreamRemoteMsg, materializer *Materializer) {
	//Each goroutine is responsible for the range of keys [keyRangeSize * id, keyRangeSize * (id + 1)[
	//Where keyRangeSize = math.MaxUint64 / number of goroutines

	partitionData := partitionData{
		db:                  make(map[uint64]VersionManager),
		stableVersion:       clocksi.ClockSiTimestamp{}.NewTimestamp(replicaID),
		highestPendingTs:    nil,
		pendingOps:          make(map[TransactionId][]*UpdateObjectParams),
		suggestedTimestamps: make(map[TransactionId]clocksi.Timestamp),
		commitedWaitToApply: make(map[TransactionId]clocksi.Timestamp),
		pendingReads:        make(map[clocksi.TimestampKey]*PendingReads),
		log:                 logger,
		remoteWaiting:       make(map[int16][]PairClockUpdates),
		replicaID:           replicaID,
		partitionID:         int64(id),
		downstreamOpsCh:     downstreamOpsCh,
		pendingOpsForRemote: make(map[TransactionId][]*UpdateObjectParams),
	}

	//TODO: Delete this
	/*
		go func() {
			time.Sleep(80000 * time.Millisecond)
			fmt.Println("Deleting CRDTs!")
			partitionData.db = nil
		}()
	*/

	//Listens to the channel and processes requests. Has a buffer since requests may be received from multiple sources.
	channel := make(chan MaterializerRequest, requestQueueSize)
	materializer.channels[id] = channel
	for {
		request := <-channel
		handleMatRequest(request, &partitionData)
	}
}

func handleMatRequest(request MaterializerRequest, partitionData *partitionData) {
	switch request.getRequestType() {
	case readStaticMatRequest:
		handleMatStaticRead(request, partitionData)
	case readMatRequest:
		handleMatRead(request, partitionData)
	case writeStaticMatRequest:
		handleMatStaticWrite(request, partitionData)
	case writeMatRequest:
		handleMatWrite(request, partitionData)
	case commitMatRequest:
		handleMatCommit(request, partitionData)
	case abortMatRequest:
		handleMatAbort(request, partitionData)
	case prepareMatRequest:
		handleMatPrepare(request, partitionData)
	case versionMatRequest:
		handleMatVersion(request, partitionData)
	case safeClkMatRequest:
		handleMatSafeClk(request, partitionData)
	case remoteTxnMatRequest:
		handleMatRemoteTxn(request, partitionData)
	case clkPosUpdMatRequest:
		handleMatClkPosUpd(request, partitionData)
	case prepareForRemoteRequest:
		handleMatPrepareRemote(request, partitionData)
	}
}

func handleMatStaticRead(request MaterializerRequest, partitionData *partitionData) {
	auxiliaryRead(request.MatRequestArgs.(MatStaticReadArgs).MatReadCommonArgs, math.MaxInt64, partitionData)
}

func handleMatRead(request MaterializerRequest, partitionData *partitionData) {
	matReadArgs := request.MatRequestArgs.(MatReadArgs)
	auxiliaryRead(matReadArgs.MatReadCommonArgs, matReadArgs.TransactionId, partitionData)
}

func auxiliaryRead(readArgs MatReadCommonArgs, txnId TransactionId, partitionData *partitionData) {
	if canRead, readLatest := canRead(readArgs.Timestamp, partitionData); canRead {
		applyReadAndReply(&readArgs, readLatest, readArgs.Timestamp, txnId, partitionData)
	} else {
		//Queue the request.
		//##fmt.Println(partitionData.partitionID, "Warning - queuing read")
		tools.FancyDebugPrint(tools.MAT_PRINT, partitionData.replicaID, "Warning - Queuing read")
		queue, exists := partitionData.pendingReads[readArgs.Timestamp.GetMapKey()]
		if !exists {
			queue = &PendingReads{
				Timestamp:     readArgs.Timestamp,
				TransactionId: txnId,
				Reads:         make([]*MatReadCommonArgs, 0, readQueueSize),
			}
			partitionData.pendingReads[readArgs.Timestamp.GetMapKey()] = queue
		}
		queue.Reads = append(queue.Reads, &readArgs)
	}
}

func canRead(readTs clocksi.Timestamp, partitionData *partitionData) (canRead bool, readLatest bool) {
	compResult := readTs.Compare(partitionData.stableVersion)
	if compResult == clocksi.EqualTs {
		//fmt.Printf("canRead - %s and %s are equal, thus canRead and readLatest.\n", readTs.ToString(), partitionData.stableVersion.ToString())
		canRead, readLatest = true, true
	} else if compResult == clocksi.LowerTs {
		//fmt.Printf("canRead - %s is lower than %s, thus canRead and !readLatest.\n", readTs.ToString(), partitionData.stableVersion.ToString())
		canRead, readLatest = true, false
	} else if partitionData.twoSmallestPendingTxn[0] != nil &&
		partitionData.suggestedTimestamps[*partitionData.twoSmallestPendingTxn[0]].IsLower(readTs) {
		//fmt.Printf("canRead - pending ts %s is lower than read ts %s, thus !canRead and !readLatest.\n", partitionData.suggestedTimestamps[*partitionData.twoSmallestPendingTxn[0]].ToString(), readTs.ToString())
		//There's a commit prepared with a timestamp lower than read's
		canRead, readLatest = false, false
	} else {
		localTs := partitionData.stableVersion.NextTimestamp(partitionData.replicaID)
		//fmt.Printf("canRead - else case, asking for next timestamp. New ts is %s, previous read is %s, stable is %s\n", localTs.ToString(), readTs.ToString(), partitionData.stableVersion.ToString())
		//TODO: Can this be false when we consider concurrent replicas...?
		if localTs.IsHigherOrEqual(readTs) {
			//fmt.Println("canRead - generated timestamp is higher, thus canRead and readLatest\n")
			canRead, readLatest = true, true
		}
	}
	return
}

func applyReadAndReply(readArgs *MatReadCommonArgs, readLatest bool, readTs clocksi.Timestamp, txnId TransactionId, partitionData *partitionData) {
	hashKey := getHash(getCombinedKey(readArgs.KeyParams))
	obj, hasKey := partitionData.db[hashKey]
	var state crdt.State
	if !hasKey {
		obj = initializeVersionManager(readArgs.CrdtType, partitionData)
		//TODO: Handle error as antidote does (check what it does? I think it just returns the object with the initial state)
	}
	pendingOps, hasPending := partitionData.pendingOps[txnId]
	var pendingObjOps []*crdt.UpdateArguments = nil
	if hasPending {
		pendingObjOps = getObjectPendingOps(readArgs.KeyParams, pendingOps)
	}
	if readLatest {
		//fmt.Println("[MAT]Reading latest version", readTs)
		state = obj.ReadLatest(readArgs.ReadArgs, pendingObjOps)
	} else {
		//fmt.Println("[MAT]Reading old version", readTs)
		state = obj.ReadOld(readArgs.ReadArgs, readTs, pendingObjOps)
	}

	readArgs.ReplyChan <- state
}

func getObjectPendingOps(keyParams KeyParams, allPending []*UpdateObjectParams) (objPending []*crdt.UpdateArguments) {
	objPending = make([]*crdt.UpdateArguments, 0, len(allPending))
	for _, upd := range allPending {
		if upd.Key == keyParams.Key && upd.Bucket == keyParams.Bucket && upd.CrdtType == keyParams.CrdtType {
			objPending = append(objPending, upd.UpdateArgs)
		}
	}
	return
}

//Contains code shared between prepare and staticWrite
func auxiliaryStartTransaction(transactionId TransactionId, partitionData *partitionData) {
	var newTimestamp clocksi.Timestamp
	if partitionData.highestPendingTs == nil {
		newTimestamp = partitionData.stableVersion.NextTimestamp(partitionData.replicaID)
	} else {
		newTimestamp = partitionData.highestPendingTs.NextTimestamp(partitionData.replicaID)
	}
	partitionData.highestPendingTs = newTimestamp
	partitionData.suggestedTimestamps[transactionId] = newTimestamp
	if partitionData.twoSmallestPendingTxn[0] == nil {
		partitionData.twoSmallestPendingTxn[0] = &transactionId
	} else if partitionData.twoSmallestPendingTxn[1] == nil {
		partitionData.twoSmallestPendingTxn[1] = &transactionId
	}
}

func handleMatStaticWrite(request MaterializerRequest, partitionData *partitionData) {
	writeArgs := request.MatRequestArgs.(MatStaticUpdateArgs)

	ok, err := typecheckWrites(writeArgs.Updates, partitionData)
	if !ok {
		writeArgs.ReplyChan <- TimestampErrorPair{
			Timestamp: nil,
			error:     err,
		}
	} else {
		auxiliaryStartTransaction(writeArgs.TransactionId, partitionData)
		partitionData.pendingOps[writeArgs.TransactionId] = writeArgs.Updates
		writeArgs.ReplyChan <- TimestampErrorPair{
			Timestamp: partitionData.suggestedTimestamps[writeArgs.TransactionId],
			error:     nil,
		}
	}

}

func handleMatPrepareRemote(request MaterializerRequest, partitionData *partitionData) {
	//These operations don't need to be applied locally. We still need to send them to the logger though.
	args := request.MatRequestArgs.(MatPrepareForRemoteArgs)
	auxiliaryStartTransaction(args.TransactionId, partitionData)
	partitionData.pendingOpsForRemote[args.TransactionId] = args.Updates
	args.ReplyChan <- partitionData.suggestedTimestamps[args.TransactionId]
}

func handleMatWrite(request MaterializerRequest, partitionData *partitionData) {
	writeArgs := request.MatRequestArgs.(MatUpdateArgs)

	ok, err := typecheckWrites(writeArgs.Updates, partitionData)
	if ok {
		partitionData.pendingOps[writeArgs.TransactionId] = append(partitionData.pendingOps[writeArgs.TransactionId], writeArgs.Updates...)
	}
	writeArgs.ReplyChan <- BoolErrorPair{
		bool:  ok,
		error: err,
	}
}

func typecheckWrites(updates []*UpdateObjectParams, partitionData *partitionData) (ok bool, err error) {
	//Typechecking
	for _, upd := range updates {
		tmpCrdt := initializeCrdt(upd.CrdtType, partitionData)
		if tmpCrdt == nil {
			return false, UnknownCrdtTypeError{CRDTType: upd.CrdtType}
		}
		ok, err = tmpCrdt.IsOperationWellTyped(*upd.UpdateArgs)
		if !ok {
			return
		}
	}

	return
}

func handleMatPrepare(request MaterializerRequest, partitionData *partitionData) {
	prepareArgs := request.MatRequestArgs.(MatPrepareArgs)
	auxiliaryStartTransaction(prepareArgs.TransactionId, partitionData)
	//fmt.Println(partitionData.partitionID, "Prepared txn", prepareArgs.TransactionId, "with clock", partitionData.suggestedTimestamps[prepareArgs.TransactionId].ToString())
	prepareArgs.ReplyChan <- partitionData.suggestedTimestamps[prepareArgs.TransactionId]
}

func handleMatVersion(request MaterializerRequest, partitionData *partitionData) {
	request.MatRequestArgs.(MatVersionArgs).ReplyChan <- partitionData.stableVersion
}

func handleMatAbort(request MaterializerRequest, partitionData *partitionData) {
	delete(partitionData.pendingOps, request.MatRequestArgs.(MatAbortArgs).TransactionId)
	//TODO: What about reads that were pending? We don't know which ones belong to this transaction and which belong to other transactions with the same TS...
}

func handleMatCommit(request MaterializerRequest, partitionData *partitionData) {
	commitArgs := request.MatRequestArgs.(MatCommitArgs)

	if canCommit(commitArgs, partitionData) {
		//Safe to commit
		applyCommit(&commitArgs.TransactionId, &commitArgs.CommitTimestamp, partitionData)
	} else {
		//A transaction with smaller version is pending, thus we queue this commit
		partitionData.suggestedTimestamps[commitArgs.TransactionId] = commitArgs.CommitTimestamp
		partitionData.commitedWaitToApply[commitArgs.TransactionId] = commitArgs.CommitTimestamp
		//This might now be the highest pending timestamp
		if commitArgs.CommitTimestamp.IsHigher(partitionData.highestPendingTs) {
			partitionData.highestPendingTs = commitArgs.CommitTimestamp
		}
		//##fmt.Println(partitionData.partitionID, "Queue commit")
		tools.FancyWarnPrint(tools.MAT_PRINT, partitionData.replicaID, "Warning - Queuing commit")

		//Update smallestPendingTxns
		//If this txn ID corresponds with twoSmallestPendingTxn[0], then it no longer is the smallest.
		//In that case we need to update both entries. Also, we might be able to apply a pending commit.
		//If this txn ID corresponds with twoSmallestPendingTxn[1], we only need to update the 2nd entry.
		//Otherwise, no update is needed
		if *partitionData.twoSmallestPendingTxn[0] == commitArgs.TransactionId {
			partitionData.twoSmallestPendingTxn[0] = partitionData.twoSmallestPendingTxn[1]
			updateSecondSmallestPendingTxn(partitionData)
			//Check if now the transaction with smaller version can be executed.
			//This may happen if the commit clock is higher than the suggested clock.
			checkAndApplyPendingCommit(partitionData)
		} else if *partitionData.twoSmallestPendingTxn[1] == commitArgs.TransactionId {
			updateSecondSmallestPendingTxn(partitionData)
		}

	}
}

func canCommit(commitArgs MatCommitArgs, partitionData *partitionData) (canCommit bool) {
	if commitArgs.TransactionId != *partitionData.twoSmallestPendingTxn[0] {
		//Is this ever false...?
		/*
			canCommit = commitArgs.CommitTimestamp.IsLower(partitionData.suggestedTimestamps[*partitionData.twoSmallestPendingTxn[0]])

			if !canCommit {
				if partitionData.suggestedTimestamps[*partitionData.twoSmallestPendingTxn[0]] == nil {
					fmt.Println(partitionData.partitionID, "suggestedTimestamps is null for entry twoSmallestPendingTxn[0]!")
				}
				fmt.Printf("%d Can't commit because commit timestamp %s with txnID %d is higher than smallestPendingTxn's timestamp %s.\n", partitionData.partitionID,
					commitArgs.CommitTimestamp.ToString(), commitArgs.TransactionId, partitionData.suggestedTimestamps[*partitionData.twoSmallestPendingTxn[0]].ToString())
			}
		*/
		//##canCommit = commitArgs.CommitTimestamp.IsLower(partitionData.suggestedTimestamps[*partitionData.twoSmallestPendingTxn[0]])
		//##if canCommit {
		//##fmt.Printf("%d I hope to never see this.\n", partitionData.partitionID)
		//##}
		//##fmt.Printf("%d Can't commit because commit timestamp %s with txnID %d is higher than smallestPendingTxn's timestamp %s with txnID %d.\n", partitionData.partitionID,
		//##commitArgs.CommitTimestamp.ToString(), commitArgs.TransactionId, partitionData.suggestedTimestamps[*partitionData.twoSmallestPendingTxn[0]].ToString(),
		//##*partitionData.twoSmallestPendingTxn[0])
		canCommit = false
	} else {
		//The txn we're verying is the one for which we proposed the lowest value. Check the 2nd lowest.
		//TODO: Possible bug - what if the clock is <= twoSmallestPendingTxn[1] but > than other pending txns?
		canCommit = partitionData.twoSmallestPendingTxn[1] == nil || commitArgs.CommitTimestamp.IsLower(partitionData.suggestedTimestamps[*partitionData.twoSmallestPendingTxn[1]])

		//##if !canCommit {
		//##fmt.Printf("%d Can't commit because commit timestamp %s with TxnID %d is higher than the 2nd smallestPendingTxn's timestamp %s with txnID %d.\n", partitionData.partitionID,
		//##commitArgs.CommitTimestamp.ToString(), commitArgs.TransactionId, partitionData.suggestedTimestamps[*partitionData.twoSmallestPendingTxn[1]].ToString(),
		//##*partitionData.twoSmallestPendingTxn[1])
		//##fmt.Printf("twoSmallestPendingTxnIDs: 1st - %d, 2nd - %d.\n", *partitionData.twoSmallestPendingTxn[0], *partitionData.twoSmallestPendingTxn[1])
		//##}

	}
	return
}

func applyCommit(transactionId *TransactionId, commitTimestamp *clocksi.Timestamp, partitionData *partitionData) {
	ops, has := partitionData.pendingOps[*transactionId]
	var downstreamUpds []*UpdateObjectParams
	if has {
		downstreamUpds = applyUpdates(ops, commitTimestamp, partitionData)
	} else {
		//This commit is to be applied only in remote replicas
		downstreamUpds = partitionData.pendingOpsForRemote[*transactionId]
	}
	updatePartitionDataWithCommit(transactionId, commitTimestamp, downstreamUpds, partitionData)
	//fmt.Println(partitionData.partitionID, "Finished applying commit with ID", *transactionId)
}

func updatePartitionDataWithCommit(transactionId *TransactionId, commitTimestamp *clocksi.Timestamp, downstreamUpds []*UpdateObjectParams, partitionData *partitionData) {
	deleteTransactionMetadata(transactionId, partitionData)
	//To avoid concurrency conflicts between partitions we need to do a deep copy of the timestamp.
	//partitionData.stableVersion = (*commitTimestamp).Copy()
	partitionData.stableVersion = (*commitTimestamp).Merge(partitionData.stableVersion)

	//No need to log if there's no downstream effects (can happen with NuCRDTs and, in the future, possibly others? E.g., remove of an element that doesn't exist.)
	if len(downstreamUpds) > 0 {
		partitionData.log.SendLoggerRequest(LoggerRequest{
			LogRequestArgs: LogCommitArgs{
				TxnClk: commitTimestamp,
				Upds:   downstreamUpds,
			},
		})
	}

	handlePendingCommits(partitionData)
}

func deleteTransactionMetadata(transactionId *TransactionId, partitionData *partitionData) {
	delete(partitionData.pendingOps, *transactionId)
	delete(partitionData.suggestedTimestamps, *transactionId)
	delete(partitionData.commitedWaitToApply, *transactionId)
	delete(partitionData.pendingOpsForRemote, *transactionId)
}

func handlePendingCommits(partitionData *partitionData) {
	//First step: update twoSmallestPendingTxn
	//Note that since commits are in order, the twoSmallestPendingTxn[0] was always the latest commit applied.
	partitionData.twoSmallestPendingTxn[0] = partitionData.twoSmallestPendingTxn[1]
	partitionData.twoSmallestPendingTxn[1] = nil
	//tools.FancyDebugPrint(tools.MAT_PRINT, partitionData.replicaID, "Checking for pending commits")
	//No further commits pending
	if len(partitionData.suggestedTimestamps) == 0 {
		//tools.FancyDebugPrint(tools.MAT_PRINT, partitionData.replicaID, "Nothing pending")
		partitionData.highestPendingTs = nil
	} else if len(partitionData.suggestedTimestamps) > 1 {
		//Search for the now second smallest pending timestamp
		updateSecondSmallestPendingTxn(partitionData)
	}
	//Added for, delete
	/*
		for id, clk := range partitionData.commitedWaitToApply {
			tools.FancyDebugPrint(tools.MAT_PRINT, partitionData.replicaID, "Pending id:", id, ". Pending clk:", clk.ToString())
		}
	*/
	checkAndApplyPendingCommit(partitionData)
	//Third step: apply pending reads
	if len(partitionData.pendingReads) > 0 {
		applyPendingReads(partitionData)
	}
	return
}

func updateSecondSmallestPendingTxn(partitionData *partitionData) {
	//##fmt.Println(partitionData.partitionID, "Called updateSecondSmallestPendingTxn")
	var smallestTs clocksi.Timestamp = partitionData.highestPendingTs
	smallestFirst := partitionData.suggestedTimestamps[*partitionData.twoSmallestPendingTxn[0]]
	smallestFirstId := *partitionData.twoSmallestPendingTxn[0]
	var smallestId TransactionId = 0
	for transId, ts := range partitionData.suggestedTimestamps {
		//Actually this would be the same as <= && !=, since no ts can be smaller than smallestFirst.
		//if ts.IsLowerOrEqual(smallestTs) && ts.IsHigherOrEqual(smallestFirst) && smallestFirstId != transId {
		if ts.IsLowerOrEqual(smallestTs) && smallestFirstId != transId {
			//##fmt.Println(partitionData.partitionID, "Updating smallestTs")
			smallestId = transId
			smallestTs = ts
		}
	}
	if smallestId != 0 {
		partitionData.twoSmallestPendingTxn[1] = &smallestId
	} else {
		//No other txn is pending
		partitionData.twoSmallestPendingTxn[1] = nil
		//Debug
		if len(partitionData.suggestedTimestamps) > 1 {
			fmt.Println(partitionData.partitionID, "ERROR - len of suggested timestamps is > 1 yet the twoSmallestPendingTxn[1] couldn't be found!")
			fmt.Println(partitionData.partitionID, "SmallestSuggestedTimestamp:", smallestFirst.ToString())
			fmt.Println(partitionData.partitionID, "Value of smallestTs:", smallestTs.ToString())
			for _, ts := range partitionData.suggestedTimestamps {
				fmt.Println(partitionData.partitionID, "Found this suggestedTimestamp:", ts.ToString())
				if ts.IsLowerOrEqual(smallestTs) {
					fmt.Println(partitionData.partitionID, "It is lower than the smallestTs.")
				}
				if ts.IsHigher(smallestFirst) {
					fmt.Println(partitionData.partitionID, "It is higher than the SmallestSuggestedTimestamp.")
				}
			}
			os.Exit(1)
		}
	}
}

//Checks if the next smallest clock corresponds to a pending commit and, if it does, applies it
func checkAndApplyPendingCommit(partitionData *partitionData) {
	//Second step: check if the smallest version corresponds to a commited transaction that wasn't yet applied
	if partitionData.twoSmallestPendingTxn[0] != nil {
		nextCommitTs, isWaitingToApply := partitionData.commitedWaitToApply[*partitionData.twoSmallestPendingTxn[0]]
		if isWaitingToApply {
			//Third step: apply that transaction
			//##fmt.Println(partitionData.partitionID, "Dequeued commit")
			tools.FancyDebugPrint(tools.MAT_PRINT, partitionData.replicaID, "Dequeued commit")
			applyCommit(partitionData.twoSmallestPendingTxn[0], &nextCommitTs, partitionData)
		}
	}
}

//downstreamUpds: key, CRDTType, bucket + downstream arguments of each update. This is what is sent to other replicas for them to apply the txn.
func applyUpdates(updates []*UpdateObjectParams, commitTimestamp *clocksi.Timestamp, partitionData *partitionData) (downstreamUpds []*UpdateObjectParams) {
	tmp := make([]*UpdateObjectParams, len(updates))
	//downstreamUpds = &tmp
	//Copy of timestamp to pass to downstream, in order to avoid conflits when updating the timestamp
	copyTs := (*commitTimestamp).Copy()
	i := 0
	for _, upd := range updates {
		hashKey := getHash(getCombinedKey(upd.KeyParams))
		obj, hasKey := partitionData.db[hashKey]
		if !hasKey {
			obj = initializeVersionManager(upd.CrdtType, partitionData)
			partitionData.db[hashKey] = obj
		}

		//Due to non-uniform CRDTs, the downstream args might be noop (op with no effect/doesn't need to be propagated yet)
		//Some "normal" CRDTs have also be optimized to do the same (e.g., setAWCrdt when removing element that doesn't exist)
		downArgs := obj.Update(*upd.UpdateArgs)
		if (downArgs != crdt.NoOp{}) {
			obj.Downstream(copyTs, downArgs)
			if downArgs.MustReplicate() {
				updArgs := downArgs.(crdt.UpdateArguments)
				tmp[i] = &UpdateObjectParams{
					KeyParams:  upd.KeyParams,
					UpdateArgs: &updArgs,
				}
				i++
			}
		}
	}
	//Hides positions that are empty due to non-uniform CRDTs
	tmp = tmp[:i]
	downstreamUpds = tmp
	return
}

func applyPendingReads(partitionData *partitionData) {
	for tsKey, pendingReads := range partitionData.pendingReads {
		if canRead, readLatest := canRead(pendingReads.Timestamp, partitionData); canRead {
			//Apply all reads of that transaction
			//##fmt.Printf("%d Applying pending reads for txn with key %s. Number of reads: %d.\n", partitionData.partitionID, tsKey, len(pendingReads.Reads))
			for _, readArgs := range pendingReads.Reads {
				//##fmt.Println(partitionData.partitionID, "Applying pending read.")
				applyReadAndReply(readArgs, readLatest, pendingReads.Timestamp, pendingReads.TransactionId, partitionData)
				//##fmt.Println(partitionData.partitionID, "Finished applying pending read.")
			}
			delete(partitionData.pendingReads, tsKey)
		}
	}
}

func handleMatSafeClk(request MaterializerRequest, partitionData *partitionData) {
	replyChan := request.MatRequestArgs.(MatSafeClkArgs).ReplyChan
	if partitionData.twoSmallestPendingTxn[0] != nil {
		replyChan <- partitionData.suggestedTimestamps[*partitionData.twoSmallestPendingTxn[0]]
	} else {
		replyChan <- partitionData.stableVersion.NextTimestamp(partitionData.replicaID)
	}
}

func handleMatRemoteTxn(request MaterializerRequest, partitionData *partitionData) {
	tools.FancyDebugPrint(tools.MAT_PRINT, partitionData.replicaID, "Starting to handle remoteTxn")
	remoteTxnArgs := request.MatRequestArgs.(MatRemoteTxnArgs)
	copyTs := remoteTxnArgs.Timestamp.Copy()
	if copyTs.IsLowerOrEqualExceptFor(partitionData.stableVersion, partitionData.replicaID, remoteTxnArgs.ReplicaID) {
		//Downstream upds that may be generated by non-uniform crdts
		newDownstream := make([]*UpdateObjectParams, 0, expectedNewDownstreamSize)

		tools.FancyDebugPrint(tools.MAT_PRINT, partitionData.replicaID, "Remote txn clock is lower or equal. Upds:", remoteTxnArgs.Upds)
		for _, upd := range remoteTxnArgs.Upds {
			tools.FancyDebugPrint(tools.MAT_PRINT, partitionData.replicaID, "Downstreaming remote op", upd, "Args:", upd.UpdateArgs)
			tools.FancyWarnPrint(tools.MAT_PRINT, partitionData.replicaID, "Remote Downstream args:", upd.UpdateArgs)
			hashKey := getHash(getCombinedKey(upd.KeyParams))

			obj, hasKey := partitionData.db[hashKey]
			if !hasKey {
				obj = initializeVersionManager(upd.CrdtType, partitionData)
				partitionData.db[hashKey] = obj
			}
			//TODO: Update this in order to avoid forced cast
			var generatedDownstream crdt.UpdateArguments = obj.Downstream(copyTs, (*upd.UpdateArgs).(crdt.DownstreamArguments))
			//non-uniform CRDTs may generate downstream updates when applying remote ops. We'll "commit" those upds with the stable clock
			if generatedDownstream != nil {
				newDownstream = append(newDownstream, &UpdateObjectParams{KeyParams: upd.KeyParams, UpdateArgs: &generatedDownstream})
			}
			tools.FancyDebugPrint(tools.MAT_PRINT, partitionData.replicaID, "Object after downstream:", obj, "hashkey:", hashKey)
		}
		tools.FancyDebugPrint(tools.MAT_PRINT, partitionData.replicaID, "Stable clk before:", partitionData.stableVersion)
		partitionData.stableVersion = partitionData.stableVersion.UpdatePos(remoteTxnArgs.ReplicaID, remoteTxnArgs.Timestamp.GetPos(remoteTxnArgs.ReplicaID))

		//Need to "commit" some operations for non-uniform CRDTs
		/*
			if len(newDownstream) > 0 {
				copyClk := partitionData.stableVersion.Copy()
				partitionData.log.SendLoggerRequest(LoggerRequest{LogRequestArgs: LogCommitArgs{TxnClk: &copyClk, Upds: &newDownstream}})
			}
		*/
		partitionData.downstreamOpsCh <- TMDownstreamNewOps{Timestamp: copyTs, partitionID: partitionData.partitionID, newOps: newDownstream}
		tools.FancyDebugPrint(tools.MAT_PRINT, partitionData.replicaID, "Stable clk after:", partitionData.stableVersion)
	} else {
		tools.FancyDebugPrint(tools.MAT_PRINT, partitionData.replicaID, "Remote txn clock is NOT lower or equal")
		clkUpdsPair := PairClockUpdates{clk: &remoteTxnArgs.Timestamp, upds: remoteTxnArgs.Upds}
		//TODO: Potencial bug: it doesn't seem like txns are ever taken out of the remoteWaiting!
		partitionData.remoteWaiting[remoteTxnArgs.ReplicaID] = append(partitionData.remoteWaiting[remoteTxnArgs.ReplicaID], clkUpdsPair)
	}
	tools.FancyDebugPrint(tools.MAT_PRINT, partitionData.replicaID, "Finished handling remoteTxn")
	//if remoteTxnArgs..IsLowerOrEqualExceptFor()
	//Downstream args: {map[15352856648520921629:5777393098617126394]}
}

func handleMatClkPosUpd(request MaterializerRequest, partitionData *partitionData) {
	clkArgs := request.MatRequestArgs.(MatClkPosUpdArgs)
	//Note that updatePos keeps the maximum of the actual value and the one in the argument
	partitionData.stableVersion.UpdatePos(clkArgs.ReplicaID, clkArgs.StableTs)
}

func initializeVersionManager(crdtType proto.CRDTType, partitionData *partitionData) (newVM VersionManager) {
	//For now, all CRDTs use the same version manager
	crdt := initializeCrdt(crdtType, partitionData)
	tmpVM := (&InverseOpVM{}).Initialize(crdt)
	return &tmpVM
}

func initializeCrdt(crdtType proto.CRDTType, partitionData *partitionData) (newCrdt crdt.CRDT) {
	return crdt.InitializeCrdt(crdtType, partitionData.replicaID)
}

/*
Called by the TransactionManager or any other entity that may want to communicate with the Materializer
and doesn't yet know the appropriate channel
*/
func (mat *Materializer) SendRequest(request MaterializerRequest) {
	mat.channels[request.getChannel()] <- request
}

/*
Called by the TransactionManager or any other entity that may want to communicate with the Materializer
when they know the appropriate channel. Avoids computing an extra hash.
*/
func (mat *Materializer) SendRequestToChannel(request MaterializerRequest, channelKey uint64) {
	mat.channels[channelKey] <- request
}

func (mat *Materializer) SendRequestToChannels(request MaterializerRequest, channelsToSend ...chan MaterializerRequest) {
	for _, channel := range channelsToSend {
		//Copy the request to avoid concurrent access problems
		newReq := request
		channel <- newReq
	}
}

func (mat *Materializer) SendRequestToAllChannels(request MaterializerRequest) {
	for _, channel := range mat.channels {
		//Copy the request to avoid concurrent access problems
		newReq := request
		channel <- newReq
	}
}

func GetChannelKey(keyParams KeyParams) (key uint64) {
	hash := getHash(getCombinedKey(keyParams))
	key = hash / keyRangeSize
	//Overflow, which might happen due to rounding
	if key == nGoRoutines {
		key -= 1
	}
	return
}

func getCombinedKey(keyParams KeyParams) (combKey string) {
	combKey = keyParams.Bucket + keyParams.CrdtType.String() + keyParams.Key
	return
}

func getHash(combKey string) (hash uint64) {
	hash = hashFunc.StringSum64(combKey)
	return
}
