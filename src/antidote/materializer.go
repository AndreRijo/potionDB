package antidote

import (
	fmt "fmt"
	math "math"
	"potionDB/src/clocksi"
	"potionDB/src/crdt"
	"potionDB/src/proto"
	"potionDB/src/shared"
	"potionDB/src/tools"

	hashFunc "github.com/twmb/murmur3"
)

//TODO: I don't think remoteWaiting is being processed. This may be an issue when clients switch between replicas and use their previous txnClks.

/////*****************TYPE DEFINITIONS***********************/////
//////////************Requests**************************//////////

type MaterializerRequest struct {
	MatRequestArgs
}

type MatRequestArgs interface {
	getRequestType() (requestType MatRequestType)
	getChannel() (channelId uint64)
}

/*
//Args for latest stable version request. This won't be necessary if we remove findCommonTimestamp from transactionManager
type MatVersionArgs struct {
	ChannelId uint64
	ReplyChan chan clocksi.Timestamp
}
*/

type MatReadCommonArgs struct {
	ReadObjectParams
	clocksi.Timestamp
	ReplyChan chan crdt.State
}

//Args for read request
type MatReadArgs struct {
	MatReadCommonArgs
	TransactionId
}

//Args for update request. Note that unlike with MatReadArgs, a MatUpdateArgs represents multiple updates, but all for the same partition
type MatUpdateArgs struct {
	Updates []*UpdateObjectParams
	TransactionId
	ReplyChan chan BoolErrorPair
}

type MatStaticReadArgs struct {
	MatReadCommonArgs
}

type MatStaticUpdateArgs struct {
	Updates []*UpdateObjectParams
	TransactionId
	ReplyChan chan TimestampErrorPair
}

type MatCommitArgs struct {
	TransactionId   TransactionId
	CommitTimestamp clocksi.Timestamp
}

type MatAbortArgs struct {
	TransactionId TransactionId
}

type MatPrepareArgs struct {
	TransactionId TransactionId
	ReplyChan     chan clocksi.Timestamp
}

type MatRemoteTxnArgs struct {
	ReplicaID int16
	clocksi.Timestamp
	Upds []*UpdateObjectParams
}

//Used by the logging layer (to then pass to the replication layer) to know the most recent clock that is "safe", i.e., for which there will be no commit for sure.
type MatSafeClkArgs struct {
	ReplyChan chan clocksi.Timestamp
}

type MatClkPosUpdArgs struct {
	ReplicaID int16
	StableTs  int64
}

//Used for preparing operations generated by applying downstream remotes
type MatPrepareForRemoteArgs struct {
	Updates []*UpdateObjectParams
	TransactionId
	ReplyChan chan clocksi.Timestamp
}

//Used by the replicator's join to know the latest commited timestamp
type MatCommitedClkArgs struct {
	ReplyChan chan TimestampPartIdPair
}

//Used to get a snapshot of CRDTs, which is needed by the joining mechanism
type MatGetSnapshotArgs struct {
	clocksi.Timestamp
	Buckets   map[string]struct{}
	ReplyChan chan []*proto.ProtoCRDT
}

//Used to apply a snapshot of CRDTs send by existing replicas in the system
type MatApplySnapshotArgs struct {
	clocksi.Timestamp
	ProtoCRDTs []*proto.ProtoCRDT
}

type MatResetArgs struct {
	ReplyChan chan bool
}

type PendingReads struct {
	TransactionId
	clocksi.Timestamp
	Reads []*MatReadCommonArgs
}

//Processed separately. Used to know when can the initial TS be generated.
type MatWaitForReplicasArgs struct{}

type MatRequestType byte

//////////********************Other types************************//////////
//Struct that represents local data to each goroutine/partition
type partitionData struct {
	//db                    map[uint64]crdt.CRDT //CRDT database of this partition
	db                    map[uint64]VersionManager
	stableVersion         clocksi.Timestamp //latest commited timestamp.
	twoSmallestPendingTxn [2]*TransactionId //Contains the two transactionIds that have been prepared with the smallest timestamps.
	//Idea: avoids the issue of the txn we're veryfing being the one with the lowest proposed timestamp (in this case, check the 2nd entry)
	highestPendingTs    clocksi.Timestamp                       //Contains the highest timestamp that was prepared. Used to check if a read can be executed or not.
	pendingOps          map[TransactionId][]*UpdateObjectParams //pending transactions waiting for commit
	suggestedTimestamps map[TransactionId]clocksi.Timestamp     //map of transactionId -> timestamp suggested on first write request for transactionId
	commitedWaitToApply map[TransactionId]clocksi.Timestamp     //map of transactionId -> commit timestamp of commited transactions that couldn't be applied due to pending versions
	//TODO: Choose a better option to hold pending reads? Checking the whole map takes a long time...
	pendingReads        map[clocksi.TimestampKey]*PendingReads //pending reads that require a more recent version than stableVersion
	log                 Logger                                 //logs the operations of txns that were commited in this partition
	remoteWaiting       map[int16][]PairClockUpdates           //remote transactions that are waiting for other remote transactions to be applied. Int: replicaID
	replicaID           int16
	partitionID         int64 //Useful for debugging
	downstreamOpsCh     chan TMDownstreamRemoteMsg
	pendingOpsForRemote map[TransactionId][]*UpdateObjectParams //pending transactions that are only to be applied in remote replicas
	//nActions            int64                                   //TODO: Remove! JUST FOR DEBUGGING PURPOSES
	//lastAction          string
	//lastNActions        []string
	//currentNActions     int
}

type BoolErrorPair struct {
	bool
	error
}

type TimestampErrorPair struct {
	clocksi.Timestamp
	error
}

type TimestampPartIdPair struct {
	clocksi.Timestamp
	partID int64
}

type Materializer struct {
	//Each goroutine is responsible for a certain range of keys (with no intersection between ranges)
	//More precisely, a goroutine is responsible from its id * keyRangeSize (inclusive) to (id + 1) * keyRangeSize (exclusive)
	channels []chan MaterializerRequest
}

/////*****************CONSTANTS AND VARIABLES***********************/////

const (
	//Types of requests
	readStaticMatRequest    MatRequestType = 0
	writeStaticMatRequest   MatRequestType = 1
	readMatRequest          MatRequestType = 2
	writeMatRequest         MatRequestType = 3
	commitMatRequest        MatRequestType = 4
	abortMatRequest         MatRequestType = 5
	prepareMatRequest       MatRequestType = 6
	safeClkMatRequest       MatRequestType = 7
	remoteTxnMatRequest     MatRequestType = 8
	clkPosUpdMatRequest     MatRequestType = 9
	prepareForRemoteRequest MatRequestType = 10
	commitedClkMatRequest   MatRequestType = 11
	getSnapshotMatRequest   MatRequestType = 12
	applySnapshotMatRequest MatRequestType = 13
	resetMatRequest         MatRequestType = 14
	waitForReplicasRequest  MatRequestType = 15
	versionMatRequest       MatRequestType = 255

	//Number of goroutines in the pool to access the database. Each goroutine has a (automatically assigned) range of keys that it can access.
	//nGoRoutines               uint64 = 8
	//nGoRoutines               uint64 = 1
	//readQueueSize                    = 10 //Initial size of the read queue for pending reads (partitionData.pendingReads)
	//requestQueueSize                 = 10 //Size for the channel that handles requests
	//expectedNewDownstreamSize        = 10 //Initial size for new downstream upds generated by non-uniform CRDTs when applying remote ops

	//maxNActions = 60 //TODO: Remove
)

var (
	//Constants read from configuration files
	//Number of goroutines in the pool to access the database. Each goroutine has a (automatically assigned) range of keys that it can access.
	nGoRoutines               uint64
	readQueueSize             int //Initial size of the read queue for pending reads (partitionData.pendingReads)
	requestQueueSize          int //Size for the channel that handles requests
	expectedNewDownstreamSize int //Initial size for new downstream upds generated by non-uniform CRDTs when applying remote ops

	//uint64: result returned by the hash function
	keyRangeSize uint64 //Number of keys that each goroutine is responsible, except for the last one which might have a bit more.
)

/////*****************TYPE METHODS***********************/////

func (args MatStaticReadArgs) getRequestType() (requestType MatRequestType) {
	return readStaticMatRequest
}

func (args MatStaticReadArgs) getChannel() (channelId uint64) {
	return GetChannelKey(args.KeyParams)
}

func (args MatStaticUpdateArgs) getRequestType() (requestType MatRequestType) {
	return writeStaticMatRequest
}
func (args MatStaticUpdateArgs) getChannel() (channelId uint64) {
	return GetChannelKey(args.Updates[0].KeyParams)
}

func (args MatReadArgs) getRequestType() (requestType MatRequestType) {
	return readMatRequest
}
func (args MatReadArgs) getChannel() (channelId uint64) {
	return GetChannelKey(args.KeyParams)
}

func (args MatUpdateArgs) getRequestType() (requestType MatRequestType) {
	return writeMatRequest
}
func (args MatUpdateArgs) getChannel() (channelId uint64) {
	return GetChannelKey(args.Updates[0].KeyParams)
}

/*
func (args MatVersionArgs) getRequestType() (requestType MatRequestType) {
	return versionMatRequest
}
func (args MatVersionArgs) getChannel() (channelId uint64) {
	return args.ChannelId
}*/

func (args MatCommitArgs) getRequestType() (requestType MatRequestType) {
	return commitMatRequest
}

func (args MatCommitArgs) getChannel() (channelId uint64) {
	return 0 //When sending the commit the TM already knows the channel to send the request
}

func (args MatAbortArgs) getRequestType() (requestType MatRequestType) {
	return abortMatRequest
}

func (args MatAbortArgs) getChannel() (channelId uint64) {
	return 0 //When sending an abort the TM already knows the channel to send the request
}

func (args MatPrepareArgs) getRequestType() (requestType MatRequestType) {
	return prepareMatRequest
}

func (args MatPrepareArgs) getChannel() (channelId uint64) {
	return 0 //When sending a prepare the TM already knows the channel to send the request
}

func (args MatSafeClkArgs) getRequestType() (requestType MatRequestType) {
	return safeClkMatRequest
}

func (args MatSafeClkArgs) getChannel() (channelId uint64) {
	return 0 //When sending a safeClk request the logger sends directly to the correct materializer
}

func (args MatRemoteTxnArgs) getRequestType() (requestType MatRequestType) {
	return remoteTxnMatRequest
}

func (args MatRemoteTxnArgs) getChannel() (channelId uint64) {
	return 0 //When sending a remoteTxn request the TM sends directly to the correct materializer
}

func (args MatClkPosUpdArgs) getRequestType() (requestType MatRequestType) {
	return clkPosUpdMatRequest
}

func (args MatClkPosUpdArgs) getChannel() (channelId uint64) {
	return 0 //matClkPosUpdRequest is always sent to all partitions
}

func (args MatPrepareForRemoteArgs) getRequestType() (requestType MatRequestType) {
	return prepareForRemoteRequest
}

func (args MatPrepareForRemoteArgs) getChannel() (channelId uint64) {
	return GetChannelKey(args.Updates[0].KeyParams)
}

func (args MatCommitedClkArgs) getRequestType() (requestType MatRequestType) {
	return commitedClkMatRequest
}

func (args MatCommitedClkArgs) getChannel() (channelId uint64) {
	return 0 //commitedClkMatRequest is always sent to all partitions
}

func (args MatGetSnapshotArgs) getRequestType() (requestType MatRequestType) {
	return getSnapshotMatRequest
}

func (args MatGetSnapshotArgs) getChannel() (channelId uint64) {
	return 0 //getSnapshotArgs is always sent to all partitions
}

func (args MatApplySnapshotArgs) getRequestType() (requestType MatRequestType) {
	return applySnapshotMatRequest
}

func (args MatApplySnapshotArgs) getChannel() (channelId uint64) {
	return 0 //ApplySnapshotArgs is sent directly to the right partition
}

func (args MatResetArgs) getRequestType() (requestType MatRequestType) {
	return resetMatRequest
}

func (args MatResetArgs) getChannel() (channelId uint64) {
	return 0 //MatReset is always sent to all partitions
}

func (args MatWaitForReplicasArgs) getRequestType() (requestType MatRequestType) {
	return waitForReplicasRequest
}

func (args MatWaitForReplicasArgs) getChannel() (channelId uint64) {
	return 0 //MatWaitForReplicas is always sent to all partitions
}

/////*****************MATERIALIZER CODE***********************/////

/*
type partitionData struct {
	//db                    map[uint64]crdt.CRDT //CRDT database of this partition
	db                    map[uint64]VersionManager
	stableVersion         clocksi.Timestamp //latest commited timestamp.
	twoSmallestPendingTxn [2]*TransactionId //Contains the two transactionIds that have been prepared with the smallest timestamps.
	//Idea: avoids the issue of the txn we're veryfing being the one with the lowest proposed timestamp (in this case, check the 2nd entry)
	highestPendingTs    clocksi.Timestamp                       //Contains the highest timestamp that was prepared. Used to check if a read can be executed or not.
	pendingOps          map[TransactionId][]*UpdateObjectParams //pending transactions waiting for commit
	suggestedTimestamps map[TransactionId]clocksi.Timestamp     //map of transactionId -> timestamp suggested on first write request for transactionId
	commitedWaitToApply map[TransactionId]clocksi.Timestamp     //map of transactionId -> commit timestamp of commited transactions that couldn't be applied due to pending versions
	//TODO: Choose a better option to hold pending reads? Checking the whole map takes a long time...
	pendingReads        map[clocksi.TimestampKey]*PendingReads //pending reads that require a more recent version than stableVersion
	log                 Logger                                 //logs the operations of txns that were commited in this partition
	remoteWaiting       map[int16][]PairClockUpdates           //remote transactions that are waiting for other remote transactions to be applied. Int: replicaID
	replicaID           int16
	partitionID         int64 //Useful for debugging
	downstreamOpsCh     chan TMDownstreamRemoteMsg
	pendingOpsForRemote map[TransactionId][]*UpdateObjectParams //pending transactions that are only to be applied in remote replicas
}
*/

//Starts listening goroutines and channels. Also starts each partition's logger and returns it
func InitializeMaterializer(replicaID int16, downstreamOpsCh chan TMDownstreamRemoteMsg) (mat *Materializer,
	loggers []Logger) {

	//Loading configs
	nGoRoutines = uint64(tools.SharedConfig.GetIntConfig("nPartitions", 1))
	readQueueSize = tools.SharedConfig.GetIntConfig("readQueueSize", 10)
	requestQueueSize = tools.SharedConfig.GetIntConfig("requestChannelSize", 10)
	expectedNewDownstreamSize = tools.SharedConfig.GetIntConfig("newDownstreamSize", 10)
	keyRangeSize = math.MaxUint64 / nGoRoutines

	mat = &Materializer{
		channels: make([]chan MaterializerRequest, nGoRoutines),
	}
	loggers = make([]Logger, nGoRoutines)
	var i uint64
	for i = 0; i < nGoRoutines; i++ {
		loggers[i] = &MemLogger{}
		loggers[i].Initialize(mat, i)
		go listenForTransactionManagerRequests(i, loggers[i], replicaID, downstreamOpsCh, mat)
	}

	return
}

/*
func checkStuckMaterializer(partitionData *partitionData, materializer *Materializer) {
	lastNActions, currNActions := int64(0), int64(0)
	for {
		time.Sleep(10 * time.Second)
		currNActions = partitionData.nActions
		if currNActions == lastNActions && len(materializer.channels[partitionData.partitionID]) > 0 {
			printMaterializerInfo(partitionData, "Stuck!!!")
			break
		}
		lastNActions = currNActions
	}
}

func printMaterializerInfo(partitionData *partitionData, context string) {
	//fmt.Printf("[MAT %d]Stuck! Preparing info...\n", partitionData.partitionID)
	var sb strings.Builder
	sb.WriteString(fmt.Sprintf("[MAT %d]", partitionData.partitionID))
	sb.WriteString(context + "\n")
	sb.WriteString(fmt.Sprintf("StableVersion: %s\nPendingTxns: ", partitionData.stableVersion.ToString()))
	var pendingTxn1, pendingTxn2 clocksi.Timestamp
	if partitionData.twoSmallestPendingTxn[0] != nil {
		txnId1 := partitionData.twoSmallestPendingTxn[0]
		pendingTxn1 = partitionData.suggestedTimestamps[*txnId1]
		if pendingTxn1 == nil {
			pendingTxn1 = partitionData.commitedWaitToApply[*txnId1]
		}
		if pendingTxn1 != nil {
			sb.WriteString(pendingTxn1.ToString() + " [" + strconv.FormatInt(int64(*txnId1), 10) + "] ")
		} else {
			sb.WriteString("nil")
		}
	} else {
		sb.WriteString("nil")
	}
	if partitionData.twoSmallestPendingTxn[1] != nil {
		txnId2 := partitionData.twoSmallestPendingTxn[1]
		pendingTxn2 = partitionData.suggestedTimestamps[*txnId2]
		if pendingTxn2 == nil {
			pendingTxn2 = partitionData.commitedWaitToApply[*txnId2]
		}
		if pendingTxn2 != nil {
			sb.WriteString(pendingTxn2.ToString() + " [" + strconv.FormatInt(int64(*txnId2), 10) + "] ")
		} else {
			sb.WriteString("nil")
		}
	} else {
		sb.WriteString("nil")
	}
	sb.WriteString(fmt.Sprintf("\nHighest pending ts: %s", partitionData.highestPendingTs.ToString()))
	sb.WriteString(fmt.Sprintf("\nPending ops: %v", partitionData.pendingOps))
	sb.WriteString("\nSuggested timestamps: [")
	for id, ts := range partitionData.suggestedTimestamps {
		sb.WriteString(strconv.FormatInt(int64(id), 10) + ":" + ts.ToString())
	}
	sb.WriteString("]")
	sb.WriteString("\nCommited waiting to apply: [")
	for id, ts := range partitionData.commitedWaitToApply {
		sb.WriteString(strconv.FormatInt(int64(id), 10) + ":" + ts.ToString())
	}
	sb.WriteString("]")
	sb.WriteString(fmt.Sprintf("\nPending reads: %v", partitionData.pendingReads))
	sb.WriteString(fmt.Sprintf("\nRemote waiting: %v", partitionData.remoteWaiting))
	sb.WriteString(fmt.Sprintf("\nPending ops for remote: %v", partitionData.pendingOpsForRemote))
	sb.WriteString("\nnActions: " + strconv.FormatInt(partitionData.nActions, 10))
	sb.WriteString("\nLast action: " + partitionData.lastAction)
	sb.WriteString("\nLast 20 actions: [")
	for i := partitionData.currentNActions - maxNActions; i < partitionData.currentNActions; i++ {
		sb.WriteString(partitionData.lastNActions[i] + ", ")
	}
	sb.WriteString("]")
	fmt.Println(sb.String())
}
*/

func listenForTransactionManagerRequests(id uint64, logger Logger, replicaID int16, downstreamOpsCh chan TMDownstreamRemoteMsg, materializer *Materializer) {
	//Each goroutine is responsible for the range of keys [keyRangeSize * id, keyRangeSize * (id + 1)[
	//Where keyRangeSize = math.MaxUint64 / number of goroutines

	partitionData := partitionData{
		db: make(map[uint64]VersionManager),
		//stableVersion:       clocksi.ClockSiTimestamp{}.NewTimestamp(replicaID),
		highestPendingTs:    nil,
		pendingOps:          make(map[TransactionId][]*UpdateObjectParams),
		suggestedTimestamps: make(map[TransactionId]clocksi.Timestamp),
		commitedWaitToApply: make(map[TransactionId]clocksi.Timestamp),
		pendingReads:        make(map[clocksi.TimestampKey]*PendingReads),
		log:                 logger,
		remoteWaiting:       make(map[int16][]PairClockUpdates),
		replicaID:           replicaID,
		partitionID:         int64(id),
		downstreamOpsCh:     downstreamOpsCh,
		pendingOpsForRemote: make(map[TransactionId][]*UpdateObjectParams),
		//lastNActions:        make([]string, maxNActions*2),
	}

	//Listens to the channel and processes requests. Has a buffer since requests may be received from multiple sources.
	channel := make(chan MaterializerRequest, requestQueueSize)
	materializer.channels[id] = channel
	//laggingBehind := false

	//TODO: Remove, just for debugging purposes
	//go checkStuckMaterializer(&partitionData, materializer)
	waitForReplicas(&partitionData, channel)

	for {
		request := <-channel
		handleMatRequest(request, &partitionData)
		/*
			if !laggingBehind && len(channel) > requestQueueSize/2 {
				//Materializer starting to lag behind.
				laggingBehind = true
				fmt.Printf("[MAT%d]Failing to keep up with TM's rythm!\n", partitionData.partitionID)
			} else if laggingBehind && len(channel) == 0 {
				laggingBehind = false
				fmt.Printf("[MAT%d]All requests have been processed and partition is no longer lagging behind.\n", partitionData.partitionID)
			}
		*/
	}
}

func waitForReplicas(partitionData *partitionData, channel chan MaterializerRequest) {
	for {
		msg := <-channel
		if msg.getRequestType() == waitForReplicasRequest {
			break
		}
	}
	partitionData.stableVersion = clocksi.NewClockSiTimestamp()
}

func handleMatRequest(request MaterializerRequest, partitionData *partitionData) {
	/*
		//TODO: Remove
		defer func() {
			if panicV := recover(); panicV != nil {
				fmt.Printf("[MAT%d] materializer panic: %v\n", partitionData.partitionID, panicV)
				debug.PrintStack()
				printMaterializerInfo(partitionData, "Panic!")
				time.Sleep(10 * time.Second)
				os.Exit(0)
			}
		}()
		partitionData.nActions++
	*/
	switch request.getRequestType() {
	case readStaticMatRequest:
		handleMatStaticRead(request, partitionData)
	case readMatRequest:
		handleMatRead(request, partitionData)
	case writeStaticMatRequest:
		handleMatStaticWrite(request, partitionData)
	case writeMatRequest:
		handleMatWrite(request, partitionData)
	case commitMatRequest:
		handleMatCommit(request, partitionData)
	case abortMatRequest:
		handleMatAbort(request, partitionData)
	case prepareMatRequest:
		handleMatPrepare(request, partitionData)
	//case versionMatRequest:
	//handleMatVersion(request, partitionData)
	case safeClkMatRequest:
		handleMatSafeClk(request, partitionData)
	case remoteTxnMatRequest:
		handleMatRemoteTxn(request, partitionData)
	case clkPosUpdMatRequest:
		handleMatClkPosUpd(request, partitionData)
	case prepareForRemoteRequest:
		handleMatPrepareRemote(request, partitionData)
	case commitedClkMatRequest:
		handleMatCommitedClk(request, partitionData)
	case getSnapshotMatRequest:
		handleMatGetSnapshot(request, partitionData)
	case applySnapshotMatRequest:
		handleMatApplySnapshot(request, partitionData)
	case resetMatRequest:
		handleMatReset(request, partitionData)
	}
}

//TODO: Remove
/*
func addAction(action string, partitionData *partitionData) {
	partitionData.lastAction = action
	partitionData.lastNActions[partitionData.currentNActions] = action
	partitionData.currentNActions++
	if partitionData.currentNActions == 2*maxNActions {
		newBuf := make([]string, 2*maxNActions)
		for i := 0; i < maxNActions; i++ {
			newBuf[i] = partitionData.lastNActions[maxNActions+i]
		}
		partitionData.currentNActions = maxNActions
		partitionData.lastNActions = newBuf
	}
}
*/

func handleMatStaticRead(request MaterializerRequest, partitionData *partitionData) {
	//addAction("staticReadStart", partitionData)
	auxiliaryRead(request.MatRequestArgs.(MatStaticReadArgs).MatReadCommonArgs, math.MaxInt64, partitionData)
}

func handleMatRead(request MaterializerRequest, partitionData *partitionData) {
	//addAction("readStart", partitionData)
	matReadArgs := request.MatRequestArgs.(MatReadArgs)
	auxiliaryRead(matReadArgs.MatReadCommonArgs, matReadArgs.TransactionId, partitionData)
}

func auxiliaryRead(readArgs MatReadCommonArgs, txnId TransactionId, partitionData *partitionData) {
	if canRead, readLatest := canRead(readArgs.Timestamp, partitionData); canRead {
		applyReadAndReply(&readArgs, readLatest, readArgs.Timestamp, txnId, partitionData)
	} else {
		//Queue the request.
		//fmt.Println(partitionData.partitionID, "Warning - queuing read with key", readArgs.Timestamp.GetMapKey())
		tools.FancyDebugPrint(tools.MAT_PRINT, partitionData.replicaID, "Warning - Queuing read")
		queue, exists := partitionData.pendingReads[readArgs.Timestamp.GetMapKey()]
		if !exists {
			//fmt.Println(partitionData.partitionID, "Queue doesn't exist for key", readArgs.Timestamp.GetMapKey())
			queue = &PendingReads{
				Timestamp:     readArgs.Timestamp,
				TransactionId: txnId,
				Reads:         make([]*MatReadCommonArgs, 0, readQueueSize),
			}
			partitionData.pendingReads[readArgs.Timestamp.GetMapKey()] = queue
		} /*else {
			fmt.Println(partitionData.partitionID, "Queue exists for key", readArgs.Timestamp.GetMapKey())
		}*/
		//fmt.Println(partitionData.partitionID, "Queue size before appending for key", readArgs.Timestamp.GetMapKey(), len(queue.Reads))
		queue.Reads = append(queue.Reads, &readArgs)
		//fmt.Println(partitionData.partitionID, "Queue size after appending for key", readArgs.Timestamp.GetMapKey(),
		//len(partitionData.pendingReads[readArgs.Timestamp.GetMapKey()].Reads))
		//addAction("queueRead", partitionData)
	}
}

func canRead(readTs clocksi.Timestamp, partitionData *partitionData) (canRead bool, readLatest bool) {
	if shared.IsReadWaitingDisabled {
		canRead, readLatest = true, true
	} else {
		compResult := readTs.Compare(partitionData.stableVersion)
		if compResult == clocksi.EqualTs {
			//fmt.Printf("canRead - %s and %s are equal, thus canRead and readLatest.\n", readTs.ToString(), partitionData.stableVersion.ToString())
			canRead, readLatest = true, true
		} else if compResult == clocksi.LowerTs {
			//fmt.Printf("canRead - %s is lower than %s, thus canRead and !readLatest.\n", readTs.ToString(), partitionData.stableVersion.ToString())
			canRead, readLatest = true, false
		} else if partitionData.twoSmallestPendingTxn[0] != nil &&
			partitionData.suggestedTimestamps[*partitionData.twoSmallestPendingTxn[0]].IsLower(readTs) {
			//TODO: If it's concurrent or equal, shouldn't we also hold it?
			//fmt.Printf("%d canRead - pending ts %s is lower than read ts %s, thus !canRead and !readLatest.\n", partitionData.partitionID, partitionData.suggestedTimestamps[*partitionData.twoSmallestPendingTxn[0]].ToString(), readTs.ToString())
			//There's a commit prepared with a timestamp lower than read's
			canRead, readLatest = false, false
		} else {
			localTs := partitionData.stableVersion.NextTimestamp(partitionData.replicaID)
			//fmt.Printf("%d canRead - else case, asking for next timestamp. New ts is %s, previous read is %s, stable is %s\n", partitionData.partitionID, localTs.ToString(), readTs.ToString(), partitionData.stableVersion.ToString())
			if localTs.IsHigherOrEqual(readTs) {
				//fmt.Println(partitionData.partitionID, "canRead - generated timestamp is higher, thus canRead and readLatest")
				canRead, readLatest = true, true
			} else {
				//fmt.Printf("%d canRead - generated timestamp isn't higher, thus !canRead and !readLatest. Generated, read ts and readKey: %s %s %s\n", partitionData.partitionID, localTs.ToString(), readTs.ToString(), readTs.GetMapKey())
				canRead, readLatest = false, false
			}
		}
	}
	return
}

func applyReadAndReply(readArgs *MatReadCommonArgs, readLatest bool, readTs clocksi.Timestamp, txnId TransactionId, partitionData *partitionData) {
	//addAction("ReadApplyStart", partitionData)
	hashKey := getHash(getCombinedKey(readArgs.KeyParams))
	obj, hasKey := partitionData.db[hashKey]
	var state crdt.State
	if !hasKey {
		obj = initializeVersionManager(readArgs.CrdtType, partitionData)
	}
	pendingOps, hasPending := partitionData.pendingOps[txnId]
	var pendingObjOps []*crdt.UpdateArguments = nil
	if hasPending {
		pendingObjOps = getObjectPendingOps(readArgs.KeyParams, pendingOps)
	}
	if readLatest {
		//fmt.Println("[MAT]Reading latest version", readTs)
		state = obj.ReadLatest(readArgs.ReadArgs, pendingObjOps)
	} else {
		//fmt.Println("[MAT]Reading old version", readTs)
		state = obj.ReadOld(readArgs.ReadArgs, readTs, pendingObjOps)
	}

	//addAction("ReadAttemptReply", partitionData)
	readArgs.ReplyChan <- state
	//addAction("ReadReplySucess", partitionData)
}

func getObjectPendingOps(keyParams KeyParams, allPending []*UpdateObjectParams) (objPending []*crdt.UpdateArguments) {
	objPending = make([]*crdt.UpdateArguments, 0, len(allPending))
	for _, upd := range allPending {
		if upd.Key == keyParams.Key && upd.Bucket == keyParams.Bucket && upd.CrdtType == keyParams.CrdtType {
			objPending = append(objPending, upd.UpdateArgs)
		}
	}
	return
}

//Contains code shared between prepare and staticWrite
func auxiliaryStartTransaction(transactionId TransactionId, partitionData *partitionData) {
	var newTimestamp clocksi.Timestamp
	if partitionData.highestPendingTs == nil {
		newTimestamp = partitionData.stableVersion.NextTimestamp(partitionData.replicaID)
	} else {
		newTimestamp = partitionData.highestPendingTs.NextTimestamp(partitionData.replicaID)
	}
	partitionData.highestPendingTs = newTimestamp.Copy()
	partitionData.suggestedTimestamps[transactionId] = newTimestamp
	if partitionData.twoSmallestPendingTxn[0] == nil {
		partitionData.twoSmallestPendingTxn[0] = &transactionId
	} else if partitionData.twoSmallestPendingTxn[1] == nil {
		partitionData.twoSmallestPendingTxn[1] = &transactionId
	}
}

func handleMatStaticWrite(request MaterializerRequest, partitionData *partitionData) {
	//addAction("writeStart", partitionData)
	writeArgs := request.MatRequestArgs.(MatStaticUpdateArgs)

	ok, err := typecheckWrites(writeArgs.Updates, partitionData)
	if !ok {
		//addAction("ERRORWriteReplyAttempt", partitionData)
		writeArgs.ReplyChan <- TimestampErrorPair{
			Timestamp: nil,
			error:     err,
		}
		//addAction("ERRORWriteReplySuccess", partitionData)
	} else {
		auxiliaryStartTransaction(writeArgs.TransactionId, partitionData)
		partitionData.pendingOps[writeArgs.TransactionId] = writeArgs.Updates
		//addAction("writeReplyAttempt", partitionData)
		writeArgs.ReplyChan <- TimestampErrorPair{
			Timestamp: partitionData.suggestedTimestamps[writeArgs.TransactionId],
			error:     nil,
		}
		//addAction("writeReplySuccess", partitionData)
	}

}

func handleMatPrepareRemote(request MaterializerRequest, partitionData *partitionData) {
	//addAction("remotePrepareStart", partitionData)
	//These operations don't need to be applied locally. We still need to send them to the logger though.
	args := request.MatRequestArgs.(MatPrepareForRemoteArgs)
	auxiliaryStartTransaction(args.TransactionId, partitionData)
	partitionData.pendingOpsForRemote[args.TransactionId] = args.Updates
	//addAction("remotePrepareAttempt", partitionData)
	args.ReplyChan <- partitionData.suggestedTimestamps[args.TransactionId]
	//addAction("remotePrepareSuccess", partitionData)
}

func handleMatWrite(request MaterializerRequest, partitionData *partitionData) {
	//addAction("writeStart", partitionData)
	writeArgs := request.MatRequestArgs.(MatUpdateArgs)

	ok, err := typecheckWrites(writeArgs.Updates, partitionData)
	if ok {
		partitionData.pendingOps[writeArgs.TransactionId] = append(partitionData.pendingOps[writeArgs.TransactionId], writeArgs.Updates...)
	}
	//addAction("writeReplyAttempt", partitionData)
	writeArgs.ReplyChan <- BoolErrorPair{
		bool:  ok,
		error: err,
	}
	//addAction("writeReplySuccess", partitionData)
}

func typecheckWrites(updates []*UpdateObjectParams, partitionData *partitionData) (ok bool, err error) {
	//Typechecking
	for _, upd := range updates {
		tmpCrdt := initializeCrdt(upd.CrdtType, partitionData)
		if tmpCrdt == nil {
			return false, crdt.UnknownCrdtTypeError{CRDTType: upd.CrdtType}
		}
		ok, err = tmpCrdt.IsOperationWellTyped(*upd.UpdateArgs)
		if !ok {
			return
		}
	}

	return
}

func handleMatPrepare(request MaterializerRequest, partitionData *partitionData) {
	//addAction("prepareStart", partitionData)
	prepareArgs := request.MatRequestArgs.(MatPrepareArgs)
	auxiliaryStartTransaction(prepareArgs.TransactionId, partitionData)
	//fmt.Println(partitionData.partitionID, "Prepared txn", prepareArgs.TransactionId, "with clock", partitionData.suggestedTimestamps[prepareArgs.TransactionId].ToString())
	//addAction("prepareReplyAttempt", partitionData)
	prepareArgs.ReplyChan <- partitionData.suggestedTimestamps[prepareArgs.TransactionId]
	//addAction("prepareReplySuccess", partitionData)
}

/*
func handleMatVersion(request MaterializerRequest, partitionData *partitionData) {
	request.MatRequestArgs.(MatVersionArgs).ReplyChan <- partitionData.stableVersion.Copy()
}
*/

func handleMatAbort(request MaterializerRequest, partitionData *partitionData) {
	delete(partitionData.pendingOps, request.MatRequestArgs.(MatAbortArgs).TransactionId)
	//TODO: What about reads that were pending? We don't know which ones belong to this transaction and which belong to other transactions with the same TS...
}

func handleMatCommit(request MaterializerRequest, partitionData *partitionData) {
	//addAction("commitStart", partitionData)
	commitArgs := request.MatRequestArgs.(MatCommitArgs)

	if canCommit(commitArgs, partitionData) {
		//Safe to commit
		applyCommit(&commitArgs.TransactionId, &commitArgs.CommitTimestamp, partitionData)
	} else {
		//A transaction with smaller version is pending, thus we queue this commit
		partitionData.suggestedTimestamps[commitArgs.TransactionId] = commitArgs.CommitTimestamp
		partitionData.commitedWaitToApply[commitArgs.TransactionId] = commitArgs.CommitTimestamp
		//This might now be the highest pending timestamp
		//TODO: [Possibly revert?] It may be possible for this timestamp to be concurrent with highestPending.
		//This can happen when another partition has applied a remoteTxn of replicaA while this partition has applied a remoteTxn of replicaB, and a commit
		//comes with the updated replicaA clock but not replicaB. (Can this really happen tho?)
		/*
			if commitArgs.CommitTimestamp.IsHigher(partitionData.highestPendingTs) {
				partitionData.highestPendingTs = commitArgs.CommitTimestamp.Copy()
			}
		*/

		compResult := commitArgs.CommitTimestamp.Compare(partitionData.highestPendingTs)
		if compResult == clocksi.HigherTs || (compResult == clocksi.ConcurrentTs && partitionData.highestPendingTs.IsSmallerConcurrent(commitArgs.CommitTimestamp)) {
			partitionData.highestPendingTs = commitArgs.CommitTimestamp.Copy()
		}

		//##fmt.Println(partitionData.partitionID, "Queue commit")
		tools.FancyWarnPrint(tools.MAT_PRINT, partitionData.replicaID, "Warning - Queuing commit")

		//Update smallestPendingTxns
		//If this txn ID corresponds with twoSmallestPendingTxn[0], then it no longer is the smallest.
		//In that case we need to update both entries. Also, we might be able to apply a pending commit.
		//If this txn ID corresponds with twoSmallestPendingTxn[1], we only need to update the 2nd entry.
		//Otherwise, no update is needed
		if *partitionData.twoSmallestPendingTxn[0] == commitArgs.TransactionId {
			partitionData.twoSmallestPendingTxn[0] = partitionData.twoSmallestPendingTxn[1]
			updateSecondSmallestPendingTxn(partitionData)
			//Check if now the transaction with smaller version can be executed.
			//This may happen if the commit clock is higher than the suggested clock.
			checkAndApplyPendingCommit(partitionData)
		} else if *partitionData.twoSmallestPendingTxn[1] == commitArgs.TransactionId {
			updateSecondSmallestPendingTxn(partitionData)
			//addAction("commitQueued", partitionData)
		}

	}
}

func canCommit(commitArgs MatCommitArgs, partitionData *partitionData) (canCommit bool) {
	if commitArgs.TransactionId != *partitionData.twoSmallestPendingTxn[0] {
		//Is this ever false...?
		/*
			canCommit = commitArgs.CommitTimestamp.IsLower(partitionData.suggestedTimestamps[*partitionData.twoSmallestPendingTxn[0]])

			if !canCommit {
				if partitionData.suggestedTimestamps[*partitionData.twoSmallestPendingTxn[0]] == nil {
					fmt.Println(partitionData.partitionID, "suggestedTimestamps is null for entry twoSmallestPendingTxn[0]!")
				}
				fmt.Printf("%d Can't commit because commit timestamp %s with txnID %d is higher than smallestPendingTxn's timestamp %s.\n", partitionData.partitionID,
					commitArgs.CommitTimestamp.ToString(), commitArgs.TransactionId, partitionData.suggestedTimestamps[*partitionData.twoSmallestPendingTxn[0]].ToString())
			}
		*/
		//##canCommit = commitArgs.CommitTimestamp.IsLower(partitionData.suggestedTimestamps[*partitionData.twoSmallestPendingTxn[0]])
		//##if canCommit {
		//##fmt.Printf("%d I hope to never see this.\n", partitionData.partitionID)
		//##}
		//##fmt.Printf("%d Can't commit because commit timestamp %s with txnID %d is higher than smallestPendingTxn's timestamp %s with txnID %d.\n", partitionData.partitionID,
		//##commitArgs.CommitTimestamp.ToString(), commitArgs.TransactionId, partitionData.suggestedTimestamps[*partitionData.twoSmallestPendingTxn[0]].ToString(),
		//##*partitionData.twoSmallestPendingTxn[0])
		canCommit = false
	} else {
		//The txn we're verying is the one for which we proposed the lowest value. Check the 2nd lowest.
		//TODO: Possible bug - what if the clock is <= twoSmallestPendingTxn[1] but > than other pending txns?
		canCommit = partitionData.twoSmallestPendingTxn[1] == nil || commitArgs.CommitTimestamp.IsLower(partitionData.suggestedTimestamps[*partitionData.twoSmallestPendingTxn[1]])

		//##if !canCommit {
		//##fmt.Printf("%d Can't commit because commit timestamp %s with TxnID %d is higher than the 2nd smallestPendingTxn's timestamp %s with txnID %d.\n", partitionData.partitionID,
		//##commitArgs.CommitTimestamp.ToString(), commitArgs.TransactionId, partitionData.suggestedTimestamps[*partitionData.twoSmallestPendingTxn[1]].ToString(),
		//##*partitionData.twoSmallestPendingTxn[1])
		//##fmt.Printf("twoSmallestPendingTxnIDs: 1st - %d, 2nd - %d.\n", *partitionData.twoSmallestPendingTxn[0], *partitionData.twoSmallestPendingTxn[1])
		//##}

	}
	return
}

func applyCommit(transactionId *TransactionId, commitTimestamp *clocksi.Timestamp, partitionData *partitionData) {
	//addAction("applyCommitStart", partitionData)
	ops, has := partitionData.pendingOps[*transactionId]
	var downstreamUpds []*UpdateObjectParams
	if has {
		downstreamUpds = applyUpdates(ops, commitTimestamp, partitionData)
	} else {
		//This commit is to be applied only in remote replicas
		downstreamUpds = partitionData.pendingOpsForRemote[*transactionId]
	}
	updatePartitionDataWithCommit(transactionId, commitTimestamp, downstreamUpds, partitionData)
	//fmt.Println(partitionData.partitionID, "Finished applying commit with ID", *transactionId)
	//addAction("applyCommitEnd", partitionData)
}

func updatePartitionDataWithCommit(transactionId *TransactionId, commitTimestamp *clocksi.Timestamp, downstreamUpds []*UpdateObjectParams, partitionData *partitionData) {
	deleteTransactionMetadata(transactionId, partitionData)
	//To avoid concurrency conflicts between partitions we need to do a deep copy of the timestamp.
	//partitionData.stableVersion = (*commitTimestamp).Copy()
	partitionData.stableVersion = (*commitTimestamp).Merge(partitionData.stableVersion)

	//No need to log if there's no downstream effects (can happen with NuCRDTs and, in the future, possibly others? E.g., remove of an element that doesn't exist.)
	if len(downstreamUpds) > 0 {
		partitionData.log.SendLoggerRequest(LoggerRequest{
			LogRequestArgs: LogCommitArgs{
				TxnClk: commitTimestamp,
				Upds:   downstreamUpds,
			},
		})
	}

	//addAction("commitApplied;startingToCheckPending", partitionData)
	handlePendingCommits(partitionData)
}

func deleteTransactionMetadata(transactionId *TransactionId, partitionData *partitionData) {
	delete(partitionData.pendingOps, *transactionId)
	delete(partitionData.suggestedTimestamps, *transactionId)
	delete(partitionData.commitedWaitToApply, *transactionId)
	delete(partitionData.pendingOpsForRemote, *transactionId)
}

func handlePendingCommits(partitionData *partitionData) {
	//First step: update twoSmallestPendingTxn
	//Note that since commits are in order, the twoSmallestPendingTxn[0] was always the latest commit applied.
	partitionData.twoSmallestPendingTxn[0] = partitionData.twoSmallestPendingTxn[1]
	partitionData.twoSmallestPendingTxn[1] = nil
	//tools.FancyDebugPrint(tools.MAT_PRINT, partitionData.replicaID, "Checking for pending commits")
	//No further commits pending
	if len(partitionData.suggestedTimestamps) == 0 {
		//tools.FancyDebugPrint(tools.MAT_PRINT, partitionData.replicaID, "Nothing pending")
		partitionData.highestPendingTs = nil
	} else if len(partitionData.suggestedTimestamps) > 1 {
		//Search for the now second smallest pending timestamp
		updateSecondSmallestPendingTxn(partitionData)
	}
	//Added for, delete
	/*
		for id, clk := range partitionData.commitedWaitToApply {
			tools.FancyDebugPrint(tools.MAT_PRINT, partitionData.replicaID, "Pending id:", id, ". Pending clk:", clk.ToString())
		}
	*/
	checkAndApplyPendingCommit(partitionData)
	//Third step: apply pending reads
	if len(partitionData.pendingReads) > 0 {
		applyPendingReads(partitionData)
	}
	//addAction("finishedCheckingPending", partitionData)
	return
}

//TODO: Important (read comments below)
/*
	It seems like when we mix txns from other replicas, even though they don't get queued,
	it is possible that txns which are still being commited may be concurrent to highestPendingTs
	While that by itself may not seem like a problem (and at this point I'm not even sure that's
	the cause of the real problem), sometimes it happens that the 2nd smallest pending txn can't
	be determined, due to concurrent timestamps.
	As I haven't yet been able to root the real cause of this happening (and if it's even supposed to),
	for now I've deployed a temporary fix: if a txn can't be found for 2nd smallest pending, fix
	highestPendingTs and choose the new highestPendingTs for 2nd smallest pending.
	New problem: sometimes not even the first exists. I don't know how can this happen... for now,
	I'll deploy another temporary (band-aid) fix: if there's no smallestPendingTxn, search in smaller
	suggestedTimestamps for the two smaller (or, more precisely, two timestamps so that no other txn has
	a smaller clock than both)
*/
func updateSecondSmallestPendingTxn(partitionData *partitionData) {
	//##fmt.Println(partitionData.partitionID, "Called updateSecondSmallestPendingTxn")
	var smallestTs clocksi.Timestamp = partitionData.highestPendingTs
	if partitionData.twoSmallestPendingTxn[0] == nil {
		fmt.Printf("[MAT%d]: Applying band-aid fix\n", partitionData.partitionID)
		//Band-aid fix.
		firstID, secondID := TransactionId(math.MaxUint64), TransactionId(math.MaxUint64)
		var firstTs, secondTs clocksi.Timestamp
		for transId, ts := range partitionData.suggestedTimestamps {
			if firstID == math.MaxUint64 {
				firstID, firstTs = transId, ts
			} else if secondID == math.MaxUint64 {
				secondID, secondTs = transId, ts
				compResult := secondTs.Compare(firstTs)
				doSwap := false
				if compResult == clocksi.LowerTs {
					doSwap = true
				} else if compResult == clocksi.ConcurrentTs {
					if secondTs.IsSmallerConcurrent(firstTs) {
						doSwap = true
					}
				}
				if doSwap {
					swapID, swapTs := firstID, firstTs
					firstID, firstTs = secondID, secondTs
					secondID, secondTs = swapID, swapTs
				}
			}
			if transId != secondID && transId != firstID {
				compResult := ts.Compare(firstTs)
				if compResult == clocksi.LowerTs {
					secondID, secondTs = firstID, firstTs
					firstID, firstTs = transId, ts
				} else {
					compResultSecond := ts.Compare(secondTs)
					if compResultSecond == clocksi.LowerTs {
						secondID, secondTs = transId, ts
					} else if compResult == clocksi.ConcurrentTs {
						if ts.IsSmallerConcurrent(firstTs) {
							secondID, secondTs = firstID, firstTs
							firstID, firstTs = transId, ts
						} else if ts.IsSmallerConcurrent(secondTs) {
							secondID, secondTs = transId, ts
						}
					}
				}
			}
		}
		partitionData.twoSmallestPendingTxn[0] = &firstID
		if secondTs != nil {
			compResult := secondTs.Compare(partitionData.highestPendingTs)
			if compResult == clocksi.HigherTs || (compResult == clocksi.ConcurrentTs && partitionData.highestPendingTs.IsSmallerConcurrent(secondTs)) {
				partitionData.highestPendingTs = secondTs.Copy()
			}
			partitionData.twoSmallestPendingTxn[1] = &secondID
		} else {
			if partitionData.highestPendingTs == nil {
				partitionData.highestPendingTs = firstTs.Copy()
			}
		}
		return
		//End of band-aid fix
	}
	smallestFirst := partitionData.suggestedTimestamps[*partitionData.twoSmallestPendingTxn[0]]
	smallestFirstId := *partitionData.twoSmallestPendingTxn[0]
	var smallestId TransactionId = 0
	for transId, ts := range partitionData.suggestedTimestamps {
		if smallestFirstId != transId {
			//Actually this would be the same as <= && !=, since no ts can be smaller than smallestFirst.
			//if ts.IsLowerOrEqual(smallestTs) && ts.IsHigherOrEqual(smallestFirst) && smallestFirstId != transId {
			/*
				if ts.IsLowerOrEqual(smallestTs) {
					//##fmt.Println(partitionData.partitionID, "Updating smallestTs")
					smallestId = transId
					smallestTs = ts
				}
			*/
			compResult := ts.Compare(smallestTs)
			if compResult == clocksi.LowerTs || compResult == clocksi.EqualTs {
				//EqualTs is for the case of finding "highestPendingTs"
				smallestId, smallestTs = transId, ts
			} else if compResult == clocksi.ConcurrentTs {
				if ts.IsSmallerConcurrent(smallestTs) {
					smallestId, smallestTs = transId, ts
				} else if partitionData.highestPendingTs.IsConcurrent(ts) && partitionData.highestPendingTs.IsSmallerConcurrent(ts) {
					//The temporary fix
					//fmt.Println("[MAT]", partitionData.partitionID, "Warning: temporary fix of choosing pending txn has been applied.")
					smallestId, smallestTs, partitionData.highestPendingTs = transId, ts, ts.Copy()
				}
			}
		}
	}
	ignore(smallestFirst)

	if smallestId != 0 {
		partitionData.twoSmallestPendingTxn[1] = &smallestId
	} /*else {
		//No other txn is pending
		partitionData.twoSmallestPendingTxn[1] = nil
		//Debug
		if len(partitionData.suggestedTimestamps) > 1 {
			fmt.Println(partitionData.partitionID, "ERROR - len of suggested timestamps is > 1 yet the twoSmallestPendingTxn[1] couldn't be found! To be precise:",
				len(partitionData.suggestedTimestamps))
			fmt.Println(partitionData.partitionID, "SmallestFirstTs:", smallestFirst.ToString())
			fmt.Println(partitionData.partitionID, "Value of smallestTs:", smallestTs.ToString())
			fmt.Println(partitionData.partitionID, "HighestPendingTs:", partitionData.highestPendingTs.ToString())
			for _, ts := range partitionData.suggestedTimestamps {
				fmt.Println(partitionData.partitionID, "Found this suggestedTimestamp:", ts.ToString())
				if ts.IsLower(smallestTs) && smallestId != smallestFirstId {
					fmt.Println(partitionData.partitionID, "It is lower than the smallestTs. (oh boy)")
				} else if ts.IsHigher(smallestTs) {
					fmt.Println(partitionData.partitionID, "It is higher than the smallestTs.")
				} else if ts.IsConcurrent(smallestTs) {
					fmt.Println(partitionData.partitionID, "It is concurrent to smallestTs.")
				} else {
					fmt.Println(partitionData.partitionID, "It is equal to smallestTs.")
				}
				if ts.IsHigher(smallestFirst) {
					fmt.Println(partitionData.partitionID, "It is higher than the SmallestSuggestedTimestamp. (oh boy)")
				} else if ts.IsLower(smallestFirst) {
					fmt.Println(partitionData.partitionID, "It is smaller than the SmallestSuggestedTimestamp.")
				} else if ts.IsConcurrent(smallestFirst) {
					fmt.Println(partitionData.partitionID, "It is concurrent to SmallestSuggestedTimestamp.")
				} else {
					fmt.Println(partitionData.partitionID, "It is equal to SmallestSuggestedTimestamp.")
				}
			}
			os.Exit(1)
		}
	}
	*/
}

//Checks if the next smallest clock corresponds to a pending commit and, if it does, applies it
func checkAndApplyPendingCommit(partitionData *partitionData) {
	//Second step: check if the smallest version corresponds to a commited transaction that wasn't yet applied
	if partitionData.twoSmallestPendingTxn[0] != nil {
		nextCommitTs, isWaitingToApply := partitionData.commitedWaitToApply[*partitionData.twoSmallestPendingTxn[0]]
		if isWaitingToApply {
			//Third step: apply that transaction
			//##fmt.Println(partitionData.partitionID, "Dequeued commit")
			tools.FancyDebugPrint(tools.MAT_PRINT, partitionData.replicaID, "Dequeued commit")
			//addAction("Dequed commit", partitionData)
			applyCommit(partitionData.twoSmallestPendingTxn[0], &nextCommitTs, partitionData)
		} /*else {
			addAction("Checked pending txns, wasn't a commit.", partitionData)
		}*/
	} /*else {
		addAction("Checked pending txns, nothing pending.", partitionData)
	}*/
}

//downstreamUpds: key, CRDTType, bucket + downstream arguments of each update. This is what is sent to other replicas for them to apply the txn.
func applyUpdates(updates []*UpdateObjectParams, commitTimestamp *clocksi.Timestamp, partitionData *partitionData) (downstreamUpds []*UpdateObjectParams) {
	//addAction("startApplyUpdates", partitionData)
	tmp := make([]*UpdateObjectParams, len(updates))
	//downstreamUpds = &tmp
	//Copy of timestamp to pass to downstream, in order to avoid conflits when updating the timestamp
	copyTs := (*commitTimestamp).Copy()
	i := 0
	for _, upd := range updates {
		hashKey := getHash(getCombinedKey(upd.KeyParams))

		//If it's a reset, delete the CRDT, generate the downstream and continue to the next upd.
		//Note that reads may recreate the CRDT, but with an empty state.
		if (*upd.UpdateArgs == crdt.ResetOp{}) {
			tmp[i] = upd
			i++
			delete(partitionData.db, hashKey)
			//fmt.Println("Reset Op, continuing.")
			continue
		}
		//fmt.Printf("%s %T\n", (*upd.UpdateArgs), (*upd.UpdateArgs))

		obj, hasKey := partitionData.db[hashKey]
		if !hasKey {
			obj = initializeVersionManager(upd.CrdtType, partitionData)
			partitionData.db[hashKey] = obj
		}

		//Due to non-uniform CRDTs, the downstream args might be noop (op with no effect/doesn't need to be propagated yet)
		//Some "normal" CRDTs have also be optimized to do the same (e.g., setAWCrdt when removing element that doesn't exist)
		downArgs := obj.Update(*upd.UpdateArgs)
		if (downArgs != crdt.NoOp{}) {
			//fmt.Printf("[MAT]Applying downstream update to %v with args %T %+v\n", upd.KeyParams, *upd.UpdateArgs, *upd.UpdateArgs)
			obj.Downstream(copyTs, downArgs)
			if downArgs.MustReplicate() {
				updArgs := downArgs.(crdt.UpdateArguments)
				tmp[i] = &UpdateObjectParams{
					KeyParams:  upd.KeyParams,
					UpdateArgs: &updArgs,
				}
				i++
			}
		}
	}
	//Hides positions that are empty due to non-uniform CRDTs
	tmp = tmp[:i]
	downstreamUpds = tmp
	//addAction("finishApplyUpdates", partitionData)
	return
}

func applyPendingReads(partitionData *partitionData) {
	//addAction("startCheckingPendingReads", partitionData)
	for tsKey, pendingReads := range partitionData.pendingReads {
		if canRead, readLatest := canRead(pendingReads.Timestamp, partitionData); canRead {
			//Apply all reads of that transaction
			//fmt.Printf("%d Applying pending reads for txn with key %s. Number of reads: %d.\n", partitionData.partitionID, tsKey, len(pendingReads.Reads))
			for _, readArgs := range pendingReads.Reads {
				//fmt.Println(partitionData.partitionID, "Applying pending read.")
				applyReadAndReply(readArgs, readLatest, pendingReads.Timestamp, pendingReads.TransactionId, partitionData)
				//fmt.Println(partitionData.partitionID, "Finished applying pending read.")
			}
			//fmt.Printf("%d Finished applying all pending reads for txn with key %s with nReads %d.\n", partitionData.partitionID, tsKey, len(pendingReads.Reads))
			delete(partitionData.pendingReads, tsKey)
		}
	}
	//addAction("finishCheckingPendingReads", partitionData)
}

func handleMatSafeClk(request MaterializerRequest, partitionData *partitionData) {
	//addAction("safeClkStart", partitionData)
	replyChan := request.MatRequestArgs.(MatSafeClkArgs).ReplyChan
	if partitionData.twoSmallestPendingTxn[0] != nil {
		replyChan <- partitionData.suggestedTimestamps[*partitionData.twoSmallestPendingTxn[0]]
	} else {
		replyChan <- partitionData.stableVersion.NextTimestamp(partitionData.replicaID)
	}
	//addAction("safeClkEnd", partitionData)
}

func handleMatCommitedClk(request MaterializerRequest, partitionData *partitionData) {
	//addAction("commitedClkStart", partitionData)
	request.MatRequestArgs.(MatCommitedClkArgs).ReplyChan <- TimestampPartIdPair{
		Timestamp: partitionData.stableVersion, partID: partitionData.partitionID}
	//addAction("commitedClkEnd", partitionData)
}

func handleMatRemoteTxn(request MaterializerRequest, partitionData *partitionData) {
	//addAction("remoteTxnStart", partitionData)
	tools.FancyDebugPrint(tools.MAT_PRINT, partitionData.replicaID, "Starting to handle remoteTxn")
	remoteTxnArgs := request.MatRequestArgs.(MatRemoteTxnArgs)
	copyTs := remoteTxnArgs.Timestamp.Copy()
	if copyTs.IsLowerOrEqualExceptFor(partitionData.stableVersion, partitionData.replicaID, remoteTxnArgs.ReplicaID) {
		//Downstream upds that may be generated by non-uniform crdts
		newDownstream := make([]*UpdateObjectParams, 0, expectedNewDownstreamSize)

		tools.FancyDebugPrint(tools.MAT_PRINT, partitionData.replicaID, "Remote txn clock is lower or equal. Upds:", remoteTxnArgs.Upds)
		//addAction("remoteTxnApplying", partitionData)
		for _, upd := range remoteTxnArgs.Upds {
			tools.FancyDebugPrint(tools.MAT_PRINT, partitionData.replicaID, "Downstreaming remote op", upd, "Args:", upd.UpdateArgs)
			tools.FancyWarnPrint(tools.MAT_PRINT, partitionData.replicaID, "Remote Downstream args:", upd.UpdateArgs)
			hashKey := getHash(getCombinedKey(upd.KeyParams))

			obj, hasKey := partitionData.db[hashKey]
			if !hasKey {
				obj = initializeVersionManager(upd.CrdtType, partitionData)
				partitionData.db[hashKey] = obj
			}
			//TODO: Update this in order to avoid forced cast
			var generatedDownstream crdt.UpdateArguments = obj.Downstream(copyTs, (*upd.UpdateArgs).(crdt.DownstreamArguments))
			//non-uniform CRDTs may generate downstream updates when applying remote ops. We'll "commit" those upds with the stable clock
			if generatedDownstream != nil {
				newDownstream = append(newDownstream, &UpdateObjectParams{KeyParams: upd.KeyParams, UpdateArgs: &generatedDownstream})
			}
			tools.FancyDebugPrint(tools.MAT_PRINT, partitionData.replicaID, "Object after downstream:", obj, "hashkey:", hashKey)
		}
		tools.FancyDebugPrint(tools.MAT_PRINT, partitionData.replicaID, "Stable clk before:", partitionData.stableVersion)
		//TODO: Go Back
		//partitionData.stableVersion = partitionData.stableVersion.UpdatePos(remoteTxnArgs.ReplicaID, remoteTxnArgs.Timestamp.GetPos(remoteTxnArgs.ReplicaID))
		partitionData.stableVersion.UpdatePos(remoteTxnArgs.ReplicaID, remoteTxnArgs.Timestamp.GetPos(remoteTxnArgs.ReplicaID))
		//TODO: Not sure if this should be updated
		if partitionData.highestPendingTs != nil {
			partitionData.highestPendingTs.UpdatePos(remoteTxnArgs.ReplicaID, remoteTxnArgs.Timestamp.GetPos(remoteTxnArgs.ReplicaID))
		}
		//copyTs := partitionData.stableVersion.Copy()
		//partitionData.stableVersion = copyTs.UpdatePos(remoteTxnArgs.ReplicaID, remoteTxnArgs.Timestamp.GetPos(remoteTxnArgs.ReplicaID))

		//Need to "commit" some operations for non-uniform CRDTs
		/*
			if len(newDownstream) > 0 {
				copyClk := partitionData.stableVersion.Copy()
				partitionData.log.SendLoggerRequest(LoggerRequest{LogRequestArgs: LogCommitArgs{TxnClk: &copyClk, Upds: &newDownstream}})
			}
		*/
		//addAction("remoteTxnAttemptReply", partitionData)
		partitionData.downstreamOpsCh <- TMDownstreamNewOps{Timestamp: copyTs, partitionID: partitionData.partitionID, newOps: newDownstream}
		//addAction("remoteTxnReplySuccess", partitionData)
		tools.FancyDebugPrint(tools.MAT_PRINT, partitionData.replicaID, "Stable clk after:", partitionData.stableVersion)
	} else {
		fmt.Println("[MAT]Remote txn on hold.")
		tools.FancyDebugPrint(tools.MAT_PRINT, partitionData.replicaID, "Remote txn clock is NOT lower or equal")
		clkUpdsPair := PairClockUpdates{clk: &remoteTxnArgs.Timestamp, upds: remoteTxnArgs.Upds}
		//TODO: Potencial bug: it doesn't seem like txns are ever taken out of the remoteWaiting!
		partitionData.remoteWaiting[remoteTxnArgs.ReplicaID] = append(partitionData.remoteWaiting[remoteTxnArgs.ReplicaID], clkUpdsPair)
		//addAction("remoteTxnHold", partitionData)
	}
	tools.FancyDebugPrint(tools.MAT_PRINT, partitionData.replicaID, "Finished handling remoteTxn")
	//if remoteTxnArgs..IsLowerOrEqualExceptFor()
	//Downstream args: {map[15352856648520921629:5777393098617126394]}
}

func handleMatClkPosUpd(request MaterializerRequest, partitionData *partitionData) {
	//addAction("clkPosStart", partitionData)
	clkArgs := request.MatRequestArgs.(MatClkPosUpdArgs)
	//Note that updatePos keeps the maximum of the actual value and the one in the argument
	//TODO: Go Back
	partitionData.stableVersion.UpdatePos(clkArgs.ReplicaID, clkArgs.StableTs)
	//TODO: Not sure if this should be updated
	if partitionData.highestPendingTs != nil {
		partitionData.highestPendingTs.UpdatePos(clkArgs.ReplicaID, clkArgs.StableTs)
	}
	//copyTs := partitionData.stableVersion.Copy()
	//partitionData.stableVersion = copyTs.UpdatePos(clkArgs.ReplicaID, clkArgs.StableTs)
	applyPendingReads(partitionData)
	//TODO: CHECK PENDING COMMITS!
	//TODO: [MIX] Added this
	checkAndApplyPendingCommit(partitionData)
	//addAction("clkPosEnd", partitionData)
}

func handleMatGetSnapshot(request MaterializerRequest, partitionData *partitionData) {
	snapshotArgs := request.MatRequestArgs.(MatGetSnapshotArgs)
	//Is queuing needed?
	snapshots := make([]*proto.ProtoCRDT, len(partitionData.db))
	i := 0
	if _, has := snapshotArgs.Buckets["*"]; has {
		//No filtering, we want all crdts in the DB
		for keyHash, vm := range partitionData.db {
			snapshots[i] = crdt.CrdtToProtoCRDT(keyHash, vm.GetLatestCRDT())
			i++
		}
	} else {
		buckets := snapshotArgs.Buckets
		ignore(buckets)
		for keyHash, vm := range partitionData.db {
			//RIP.
			//TODO: Actually filter.
			snapshots[i] = crdt.CrdtToProtoCRDT(keyHash, vm.GetLatestCRDT())
			i++
		}
	}
	snapshotArgs.ReplyChan <- snapshots
}

func handleMatApplySnapshot(request MaterializerRequest, partitionData *partitionData) {
	snapshotArgs := request.MatRequestArgs.(MatApplySnapshotArgs)
	clk, protoCRDTs := snapshotArgs.Timestamp, snapshotArgs.ProtoCRDTs
	for _, protoCRDT := range protoCRDTs {
		//Assuming all these CRDTs don't exist yet.
		crdt := initializeCrdt(protoCRDT.GetType(), partitionData).(crdt.ProtoCRDT).FromProtoState(protoCRDT.GetState(),
			&clk, partitionData.replicaID)
		partitionData.db[protoCRDT.GetKeyHash()] = initializeVersionManagerWithCRDT(crdt)
		fmt.Println("Rebuilt CRDT from snapshot")
	}
	partitionData.stableVersion = clk.Copy()
}

func handleMatReset(request MaterializerRequest, partitionData *partitionData) {
	partitionData.log.Reset()
	partitionData.db = make(map[uint64]VersionManager)
	partitionData.stableVersion = clocksi.ClockSiTimestamp{}.NewTimestamp()
	partitionData.highestPendingTs = nil
	partitionData.pendingOps = make(map[TransactionId][]*UpdateObjectParams)
	partitionData.suggestedTimestamps = make(map[TransactionId]clocksi.Timestamp)
	partitionData.commitedWaitToApply = make(map[TransactionId]clocksi.Timestamp)
	partitionData.pendingReads = make(map[clocksi.TimestampKey]*PendingReads)
	partitionData.remoteWaiting = make(map[int16][]PairClockUpdates)
	partitionData.pendingOpsForRemote = make(map[TransactionId][]*UpdateObjectParams)
	//partitionData.lastNActions = make([]string, 2*maxNActions)
	//partitionData.nActions = 0
	//partitionData.lastAction = ""
	request.MatRequestArgs.(MatResetArgs).ReplyChan <- true
	fmt.Printf("[MAT %d]Reset complete.\n", partitionData.partitionID)
}

func initializeVersionManager(crdtType proto.CRDTType, partitionData *partitionData) (newVM VersionManager) {
	//For now, all CRDTs use the same version manager
	crdt := initializeCrdt(crdtType, partitionData)
	tmpVM := (&InverseOpVM{}).Initialize(crdt)
	return &tmpVM
}

func initializeVersionManagerWithCRDT(crdt crdt.CRDT) (newVM VersionManager) {
	vm := (&InverseOpVM{}).Initialize(crdt)
	return &vm
}

func initializeCrdt(crdtType proto.CRDTType, partitionData *partitionData) (newCrdt crdt.CRDT) {
	return crdt.InitializeCrdt(crdtType, partitionData.replicaID)
}

/*
Called by the TransactionManager or any other entity that may want to communicate with the Materializer
and doesn't yet know the appropriate channel
*/
func (mat *Materializer) SendRequest(request MaterializerRequest) {
	mat.channels[request.getChannel()] <- request
}

/*
Called by the TransactionManager or any other entity that may want to communicate with the Materializer
when they know the appropriate channel. Avoids computing an extra hash.
*/
func (mat *Materializer) SendRequestToChannel(request MaterializerRequest, channelKey uint64) {
	mat.channels[channelKey] <- request
}

func (mat *Materializer) SendRequestToChannels(request MaterializerRequest, channelsToSend ...chan MaterializerRequest) {
	for _, channel := range channelsToSend {
		//Copy the request to avoid concurrent access problems
		newReq := request
		channel <- newReq
	}
}

func (mat *Materializer) SendRequestToAllChannels(request MaterializerRequest) {
	for _, channel := range mat.channels {
		//Copy the request to avoid concurrent access problems
		newReq := request
		channel <- newReq
	}
}

func GetChannelKey(keyParams KeyParams) (key uint64) {
	hash := getHash(getCombinedKey(keyParams))
	key = hash / keyRangeSize
	//Overflow, which might happen due to rounding
	if key == nGoRoutines {
		key -= 1
	}
	return
}

func getCombinedKey(keyParams KeyParams) (combKey string) {
	combKey = keyParams.Bucket + keyParams.CrdtType.String() + keyParams.Key
	return
}

func getHash(combKey string) (hash uint64) {
	hash = hashFunc.StringSum64(combKey)
	return
}
